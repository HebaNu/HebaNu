---
title: "Exploring the Relationship Between Income and Living Location Preference"
subtitle: "Linear Regression VS. non_Linear Regression"
author: "Heba Nusair"
date: "2023-11-06"
categories: [news, code, analysis]
image: "image.jpg"
---

### **Introduction**

In our quest to understand how monthly income influences living location preferences, particularly among Workers From Home (WFH), we embarked on an analytical journey using linear regression. Linear regression is a statistical method that helps us understand the relationship between two variables -- in this case, income and living location preference of WFH workers.

### **The Study at a Glance**

Our dataset encapsulated responses from individuals, detailing their income levels and preferred living areas -- be it the bustling city center, the vibrant urban zones, the serene suburban areas, or the tranquil rural landscapes; this preference of **moving from urban area to rural** is represented by "**relocating index**". Our approach was straightforward:

1.  We transformed **income** into a continuous scale, assigning monetary values to income brackets.

2.  We converted **living area preferences** into a single, ordinal dependent variable.

3.  We employed linear regression to model the relationship between these two variables.

```{python}
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_excel('Income_LivingLocationPrefrences.xlsx')

# Handle NaN values
data.dropna(subset=['City_center', 'Urban_area', 'Suburban_area', 'Rural_area', 'Monthly_income'], inplace=True)

# Convert the Location preferences into a single ordinal dependent variable
area_to_number = {'City_center': 1, 'Urban_area': 2, 'Suburban_area': 3, 'Rural_area': 4}
data['living_Location_preference'] = data[['City_center', 'Urban_area', 'Suburban_area', 'Rural_area']].idxmin(axis=1).map(area_to_number)

# Convert income to a continuous scale based on the provided income brackets
income_mapping = {1.0: 625, 2.0: 2292, 3.0: 5000, 4.0: 6666}
data['continuous_income'] = data['Monthly_income'].map(income_mapping)

# Scale the income feature
scaler = StandardScaler()
data['scaled_income'] = scaler.fit_transform(data[['continuous_income']])

# Prepare the features and target variable for modeling
X = data[['scaled_income']]
y = data['living_Location_preference']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Predict on the test data
y_pred = model.predict(X_test)

# Evaluate the model's performance
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Output the performance metrics
print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')

# Debugging the sizes of arrays
print("Sizes of arrays for plotting:")
print("X_test['scaled_income']: ", len(X_test['scaled_income']))
print("y_test: ", len(y_test))
print("y_pred: ", len(y_pred))

# Plotting
plt.scatter(X_test['scaled_income'].values, y_test.values, color='black', label='Actual Data')
plt.scatter(X_test['scaled_income'].values, y_pred, color='red', label='Predicted Data', alpha=0.5)

# Optionally, create a more continuous line for predictions
sorted_order = np.argsort(X_test['scaled_income'].values)
plt.plot(X_test['scaled_income'].values[sorted_order], y_pred[sorted_order], color='blue', linewidth=2, label='Regression Line')

plt.xlabel('Monthly income')
plt.ylabel('Living Location Preference:moving from urban to rural index')
plt.title('Income vs Living Location Preference Linear Regression')
plt.legend()
plt.show()


```

### **Unveiling the Linear Regression Insights for WFH Workers**

We meticulously gathered data on WFH workers and ran a linear regression model. Here's a snapshot of our findings:

-   <div>

    -   **Mean Squared Error (MSE)**: Our model showed an MSE of 1.2658068009077692. This number tells us how close the model's predictions are to the actual data. Think of it as an accuracy gauge: the lower the number, the better. In our analysis, this number, though not extremely high, does indicate some level of discrepancy between what the model predicts and the real-world data of WFH workers.

    -   **R-squared Value**: Perhaps more tellingly, our model yielded an R-squared value of just 0.006851215790502296. This metric is crucial -- it shows how well our independent variable (income) explains the variation in our dependent variable (living location preference). The closer this value is to 1, the better. Our low R-squared value hints that our model, as it stands, might not be capturing the whole picture, especially in the context of WFH employees.

    </div>

    In our analytical journey to understand how income affects living location preferences for Workers From Home (WFH), we've encountered some intriguing results. Our linear regression model produced a Mean Squared Error (MSE) of 1.2658068009077692 and an R-squared value of just 0.006851215790502296. While the MSE suggests a reasonable level of accuracy, the low R-squared value indicates that income alone may not be a strong predictor of living area preferences among WFH employees.

    A glance at our regression plot further illustrates this point. The actual data points, represented by black dots, are scattered across the graph, deviating from the blue regression line that represents our model's predictions. The red dots, which mark our model's predicted values, also stray from many of the actual data points. This visual mismatch echoes the numerical findings, suggesting that the linear model doesn't fully capture the nuances of our data.

    The plot signals that there could be more complexity to the relationship between income and living area preferences than a simple linear model can explain. It raises the possibility that other, unconsidered factors may influence this relationship, or that the dynamics between these variables are not linear at all.

    ### **Transitioning to Non-Linear Models**

    This revelation leads us to our next analytical adventure -- exploring non-linear relationships. It's possible that the connection between income and living location preference for WFH workers bends and curves in ways that a simple straight line can't capture.

    In our upcoming analysis, we'll dive into non-linear regression models. Non-linear regression could potentially provide a better framework for capturing the intricate patterns that seem to exist between income and where WFH employees prefer to reside. We're excited to see how this approach compares with our initial linear model and what new insights we can uncover about the factors influencing where WFH employees choose

    Stay tuned as we continue to unravel the intricacies of income and living location preferences for workers from home, one model at a time!

### **Introducing the Concept of Non-Linear Relationships**

In our exploration of how income influences living area preferences, a linear model initially seemed like a straightforward choice. However, real-world data often tell a more complex story. Our linear model, with a surprisingly negative R-squared value, hinted at this complexity. It suggests that the relationship between income and living area preference might not be a straight line, but rather a curve or a more intricate pattern.

Linear regression, with its assumption of a straight-line relationship between variables, is ill-equipped to capture these nuances. This is where the idea of non-linear relationships comes into play. Such relationships are not confined to straight lines and can take various forms, like curves, which are more adept at mapping the intricate patterns often found in real-world data.

### **Implementing ordinal regression model**

To explore these potential non-linear patterns, we turn to

```{python}
import pandas as pd
import statsmodels.api as sm
from sklearn.model_selection import train_test_split

# Load the dataset
data = pd.read_excel('Income_LivingLocation_GradientBoostingRegressor.xlsx')  # Update the path to your file

# Map the income responses to the categorical income variable
data['income_category'] = data['Monthly_income'].map({
    1: '1250_or_less',
    2: '1251_to_3333',
    3: 'greater_than_3333',
    4: '6666_or_more'
})

# Use pd.get_dummies to convert 'income_category' into one-hot encoded variables
income_dummies = pd.get_dummies(data['income_category'], drop_first=True)

# Assuming that 'City_center', 'Urban_area', 'Suburban_area', 'Rural_area' are the categories for living area preference
# We can create an ordinal variable where 1 is most preferred and 4 is least preferred
data['living_area_preference'] = data[['City_center', 'Urban_area', 'Suburban_area', 'Rural_area']].idxmin(axis=1).map(area_to_number)

# Prepare the features (including the one-hot encoded income variable) and the target variable
X = pd.concat([income_dummies, data[['Education_Level', 'House_owner_or_renter', 'number of employees in household']]], axis=1)
y = data['living_area_preference']

# Add a constant to the model (intercept)
X = sm.add_constant(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit the ordinal regression model
model = sm.MNLogit(y_train, X_train)
result = model.fit()

# Output the model summary
print(result.summary())

```

### **Comparison of Linear vs. Polynomial Regression Models**

In our exploration of the relationship between income and living area preference, we analyzed two different models: Linear Regression and Polynomial Regression. The outcomes offer intriguing insights:

1.  **Linear Regression Model**:

    -   Initially, we employed a Linear Regression model, which yielded a Mean Squared Error (MSE) of 1.2229590696534407. This suggested some variance between the predicted and actual preferences.

    -   However, the R-squared value was -0.0034801764611527286, indicating a poor fit. This negative value implies that the linear model might not be suitable for capturing the relationship in our analysis.

2.  **Polynomial Regression Model**:

    -   Seeking improvement, we then applied a Polynomial Regression model. It showed a lower MSE of 0.12978886986451396, suggesting a closer fit to the data compared to the Linear Regression model.

    -   The R-squared value improved to 0.012140786799389347. While still low, it's a positive value, indicating a slight improvement in explaining the variability of the dependent variable.

### **Comparative Table**

| **Model Type**        | **Mean Squared Error (MSE)** | **R-squared Value**    |
|-----------------------|------------------------------|------------------------|
| Linear Regression     | 1.2229590696534407           | -0.0034801764611527286 |
| Polynomial Regression | 0.12978886986451396          | 0.012140786799389347   |
