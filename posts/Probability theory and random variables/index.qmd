---
title: "Probability theory and random variables"
author: "Heba Nusair"
date: "2023-11-11"
categories: [news, code, analysis]
image: "Capture.JPG"
code-fold: true
code-viewfold: true
---

**Naive Bayes classifier for multivariate Bernoulli models**

The Model is designed to classify the jobs working methods based on the industry sectors according to ....**\
1:** Fully remote (working from home or another location)

2: Fully in-person (working at a physical office or location)\
\
*The Naive Bayes classifier for multivariate Bernoulli models is a specific type of Naive Bayes classifier used in machine learning, particularly well-suited for binary (0/1) features. Here's what it is typically used for:*

1.  ***Text Classification:***

    -   *One of the most common applications is in text classification, such as spam detection in emails, sentiment analysis, and categorizing documents into different topics. In these cases, the presence or absence of certain words (represented as 1 for present and 0 for absent) is used as features.*

2.  ***Binary Feature Datasets:***

    -   *This model is ideal for datasets where features are binary. For example, in market basket analysis, where you might want to classify customer behavior based on the presence or absence of product purchases.*

3.  ***Simple Probabilistic Model:***

    -   *As a probabilistic model, it calculates the probability of each class and the conditional probability of each feature belonging to each class. Despite its simplicity, it can be quite effective, especially in text classification tasks.*

4.  ***Baseline for Comparison:***

    -   *Due to its simplicity and efficiency, it's often used as a baseline classifier. It's relatively easy to implement and fast to run, making it a good starting point for classification tasks to compare the performance of more complex algorithms.*

5.  ***Feature Independence Assumption:***

    -   *The "naive" aspect comes from its assumption that all features are independent of each other given the class. While this is a strong assumption and often not true in real-world data, the classifier can still perform surprisingly well.*

6.  ***Real-time Prediction:***

    -   *Naive Bayes classifiers are known for their efficiency, which makes them suitable for real-time prediction tasks.*

7.  ***Multi-Class Problems:***

    -   *Although the Bernoulli Naive Bayes is often used for binary classification, it can be extended to multi-class problems as well.*

*In summary, the Naive Bayes classifier for multivariate Bernoulli models is a straightforward, efficient, and often effective algorithm for classification tasks, particularly when dealing with binary features or text data. Despite the simplicity of its underlying assumptions, it can yield robust results in various applications.*

```{python}
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import OneHotEncoder
import os

# Load the dataset
df = pd.read_csv('C:/Users/NUSAI/Desktop/Machine learning/HebaNu.github.io/HebaNu.github.io/HebaNu/posts/Probability theory and random variables/updated_POST1.csv')

# Drop rows where any cell is NaN in the 'Work Method' column
df = df.dropna(subset=['Work Method'])

# One-hot encoding
encoder = OneHotEncoder(sparse=False)
X = encoder.fit_transform(df[[' industry sector']])  
# Use the correct column name

# Target variable
y = df['Work Method']

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the Bernoulli Naive Bayes model
model = BernoulliNB()
model.fit(X_train, y_train)

# Make predictions and evaluate the model
y_pred = model.predict(X_test)



# Output the classification report and accuracy
print(classification_report(y_test, y_pred, zero_division=0))
print(f'Accuracy: {accuracy_score(y_test, y_pred)}')

```

The results from your BernoulliNB model show that:

The model is predicting class 2.0 with high precision (76%) and recall (100%), which means for this class, it performs well both in terms of the accuracy of the positive predictions it makes (precision) and its ability to find all the positive instances (recall).

For class 1.0, however, the model does not predict any instances correctly, which suggests that either there's an issue with the distribution of your classes (perhaps class 1.0 is underrepresented), or that the features do not provide enough information to distinguish class 1.0 from class 2.0. The overall accuracy of the model is 76.47%, which means that it correctly predicts the class for 76.47% of the test set.

The macro avg and weighted avg for precision, recall, and f1-score provide a summary of the effectiveness of the model across the classes. The low macro avg for precision and f1-score indicates that one of the classes does not perform well, which we already know is class 1.0.

The f1-score is a harmonic mean of precision and recall and is a useful metric when you have classes that are imbalanced. In your case, the f1-score for class 1.0 is 0.00, indicating poor performance for this class.

***To create plots*** that visualize the performance of your **`BernoulliNB`** model, you would typically look at the confusion matrix, precision-recall, and possibly ROC curves. Below are examples of how you could generate each of these plots using **`matplotlib`** and **`scikit-learn`**:

1.  **Confusion Matrix**: Visualizes the correct and incorrect predictions compared to the actual values.

```{python}
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming y_test and y_pred are already defined from your BernoulliNB model
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d")
plt.title('Confusion Matrix for BernoulliNB Model')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

```

**ROC Curve**: Plots the true positive rate against the false positive rate.

```{python}
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import RocCurveDisplay
import matplotlib.pyplot as plt

# Adjust y_test to have binary labels 0 and 1
y_test_binary = y_test.replace({1: 0, 2: 1})

# Get predicted probabilities for the positive class (e.g., class 2)
y_pred_prob = model.predict_proba(X_test)[:, 1]  # Index 1 for the probability of class 2

# Calculate the ROC curve
fpr, tpr, thresholds = roc_curve(y_test_binary, y_pred_prob)

# Calculate the AUC (Area Under Curve)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
disp = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)
disp.plot()
plt.title('ROC Curve for BernoulliNB Model')
plt.show()

```

**Classification Report**: Presents precision, recall, f1-score for each class.

```{python}
from sklearn.metrics import classification_report
import pandas as pd

report = classification_report(y_test, y_pred, output_dict=True)
df_report = pd.DataFrame(report).transpose()

df_report.drop(['accuracy'], inplace=True)  # Drop accuracy as it's not a class-specific metric
df_report.plot(kind='bar', figsize=(10, 7))
plt.title('Classification Report for BernoulliNB Model')
plt.ylabel('Score')
plt.show()
```

Please note that in order to generate a ROC curve or Precision-Recall curve, you need the probability scores or decision function, not just the predicted labels. If **`y_pred`** contains only class labels, you would need to use the **`predict_proba`** or **`decision_function`** method of your classifier to obtain these scores.

These code snippets are intended to be run in your local Python environment, and you'll need to ensure that **`y_test`** and **`y_pred`** are defined in your workspace after running your **`BernoulliNB`** model. If you encounter any issues with these plots or if you require further customization, feel free to ask for additional guidance.
