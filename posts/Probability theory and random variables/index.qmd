---
title: "Probability theory and random variables"
author: "Heba Nusair"
date: "2023-11-06"
categories: [news, code, analysis]
image: "Capture.JPG"
code-fold: true
code-viewfold: true
---

## **Naive Bayes classifier for multivariate Bernoulli models**

```{python}
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import OneHotEncoder
import os

# Load the dataset
df = pd.read_csv('C:/Users/NUSAI/Desktop/Machine learning/HebaNu.github.io/HebaNu.github.io/HebaNu/posts/Probability theory and random variables/updated_POST1.csv')

# Drop rows where any cell is NaN in the 'Work Method' column
df = df.dropna(subset=['Work Method'])

# One-hot encoding
encoder = OneHotEncoder(sparse=False)
X = encoder.fit_transform(df[[' industry sector']])  
# Use the correct column name

# Target variable
y = df['Work Method']

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the Bernoulli Naive Bayes model
model = BernoulliNB()
model.fit(X_train, y_train)

# Make predictions and evaluate the model
y_pred = model.predict(X_test)



# Output the classification report and accuracy
print(classification_report(y_test, y_pred, zero_division=0))
print(f'Accuracy: {accuracy_score(y_test, y_pred)}')

```

The results from your BernoulliNB model show that:

The model is predicting class 2.0 with high precision (76%) and recall (100%), which means for this class, it performs well both in terms of the accuracy of the positive predictions it makes (precision) and its ability to find all the positive instances (recall).

For class 1.0, however, the model does not predict any instances correctly, which suggests that either there's an issue with the distribution of your classes (perhaps class 1.0 is underrepresented), or that the features do not provide enough information to distinguish class 1.0 from class 2.0. The overall accuracy of the model is 76.47%, which means that it correctly predicts the class for 76.47% of the test set.

The macro avg and weighted avg for precision, recall, and f1-score provide a summary of the effectiveness of the model across the classes. The low macro avg for precision and f1-score indicates that one of the classes does not perform well, which we already know is class 1.0.

The f1-score is a harmonic mean of precision and recall and is a useful metric when you have classes that are imbalanced. In your case, the f1-score for class 1.0 is 0.00, indicating poor performance for this class.

***To create plots*** that visualize the performance of your **`BernoulliNB`** model, you would typically look at the confusion matrix, precision-recall, and possibly ROC curves. Below are examples of how you could generate each of these plots using **`matplotlib`** and **`scikit-learn`**:

1.  **Confusion Matrix**: Visualizes the correct and incorrect predictions compared to the actual values.

```{python}
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming y_test and y_pred are already defined from your BernoulliNB model
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d")
plt.title('Confusion Matrix for BernoulliNB Model')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

```

**ROC Curve**: Plots the true positive rate against the false positive rate.

```{python}
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import RocCurveDisplay
import matplotlib.pyplot as plt

# Adjust y_test to have binary labels 0 and 1
y_test_binary = y_test.replace({1: 0, 2: 1})

# Get predicted probabilities for the positive class (e.g., class 2)
y_pred_prob = model.predict_proba(X_test)[:, 1]  # Index 1 for the probability of class 2

# Calculate the ROC curve
fpr, tpr, thresholds = roc_curve(y_test_binary, y_pred_prob)

# Calculate the AUC (Area Under Curve)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
disp = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)
disp.plot()
plt.title('ROC Curve for BernoulliNB Model')
plt.show()

```

**Classification Report**: Presents precision, recall, f1-score for each class.

```{python}
from sklearn.metrics import classification_report
import pandas as pd

report = classification_report(y_test, y_pred, output_dict=True)
df_report = pd.DataFrame(report).transpose()

df_report.drop(['accuracy'], inplace=True)  # Drop accuracy as it's not a class-specific metric
df_report.plot(kind='bar', figsize=(10, 7))
plt.title('Classification Report for BernoulliNB Model')
plt.ylabel('Score')
plt.show()
```

Please note that in order to generate a ROC curve or Precision-Recall curve, you need the probability scores or decision function, not just the predicted labels. If **`y_pred`** contains only class labels, you would need to use the **`predict_proba`** or **`decision_function`** method of your classifier to obtain these scores.

These code snippets are intended to be run in your local Python environment, and you'll need to ensure that **`y_test`** and **`y_pred`** are defined in your workspace after running your **`BernoulliNB`** model. If you encounter any issues with these plots or if you require further customization, feel free to ask for additional guidance.
