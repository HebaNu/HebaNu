if 'Education_Level' not in data.columns:
raise ValueError("'Education_Level' column is missing from the DataFrame.")
# Map the education levels to ordinal numbers
education_order = [
'High school', 'Some college', 'Trade/vocational/technical',
'Associates', 'Bachelors', 'Masters', 'Professional', 'Doctorate'
]
encoder = OrdinalEncoder(categories=[education_order])
data['Education_Level_Ordinal'] = encoder.fit_transform(data[['Education_Level']])
# Drop NaN values for all columns that will be used
data.dropna(subset=['City_center', 'Urban_area', 'Suburban_area', 'Rural_area', 'Monthly_income', 'Education_Level'], inplace=True)
# Print the columns after dropna to verify 'Education_Level_Ordinal' exists
print("Columns after dropna:", data.columns.tolist())
# Check the length of the DataFrame after dropping NaN values
print("Length of DataFrame after dropna:", len(data))
# Scale the continuous_income feature
scaler = StandardScaler()
data['scaled_income'] = scaler.fit_transform(data[['Monthly_income']])
# Define features and target
X = data[['scaled_income', 'Education_Level_Ordinal']]
y = data['City_center']  # Replace 'City_center' with your actual target column
# Check the lengths of X and y to ensure they match
print("Length of X:", len(X), "Length of y:", len(y))
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Initialize and train the GBM model with the new features
gbm_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gbm_model.fit(X_train, y_train)
# Predict on the test data and evaluate the model
y_pred_gbm = gbm_model.predict(X_test)
mse_gbm = mean_squared_error(y_test, y_pred_gbm)
r2_gbm = r2_score(y_test, y_pred_gbm)
# Output the updated performance metrics
print('GBM Mean Squared Error with Education Level:', mse_gbm)
print('GBM R-squared with Education Level:', r2_gbm)
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OrdinalEncoder
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
# Load the dataset
data_path = ''
data = pd.read_excel(Income_LivingLocation_GradientBoostingRegressor.xlsx)
# Print the columns to verify the 'Education_Level' column exists
print("Columns before any processing:", data.columns.tolist())
# Check if 'Education_Level' is in the DataFrame and raise an error if not
if 'Education_Level' not in data.columns:
raise ValueError("'Education_Level' column is missing from the DataFrame.")
# Map the education levels to ordinal numbers
education_order = [
'High school', 'Some college', 'Trade/vocational/technical',
'Associates', 'Bachelors', 'Masters', 'Professional', 'Doctorate'
]
encoder = OrdinalEncoder(categories=[education_order])
data['Education_Level_Ordinal'] = encoder.fit_transform(data[['Education_Level']])
# Drop NaN values for all columns that will be used
data.dropna(subset=['City_center', 'Urban_area', 'Suburban_area', 'Rural_area', 'Monthly_income', 'Education_Level'], inplace=True)
# Print the columns after dropna to verify 'Education_Level_Ordinal' exists
print("Columns after dropna:", data.columns.tolist())
# Check the length of the DataFrame after dropping NaN values
print("Length of DataFrame after dropna:", len(data))
# Scale the continuous_income feature
scaler = StandardScaler()
data['scaled_income'] = scaler.fit_transform(data[['Monthly_income']])
# Define features and target
X = data[['scaled_income', 'Education_Level_Ordinal']]
y = data['City_center']  # Replace 'City_center' with your actual target column
# Check the lengths of X and y to ensure they match
print("Length of X:", len(X), "Length of y:", len(y))
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Initialize and train the GBM model with the new features
gbm_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gbm_model.fit(X_train, y_train)
# Predict on the test data and evaluate the model
y_pred_gbm = gbm_model.predict(X_test)
mse_gbm = mean_squared_error(y_test, y_pred_gbm)
r2_gbm = r2_score(y_test, y_pred_gbm)
# Output the updated performance metrics
print('GBM Mean Squared Error with Education Level:', mse_gbm)
print('GBM R-squared with Education Level:', r2_gbm)
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OrdinalEncoder
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
# Load the dataset
data_path = 'Income_LivingLocation_GradientBoostingRegressor.xlsx'
data = pd.read_excel(Income_LivingLocation_GradientBoostingRegressor.xlsx)
# Print the columns to verify the 'Education_Level' column exists
print("Columns before any processing:", data.columns.tolist())
# Check if 'Education_Level' is in the DataFrame and raise an error if not
if 'Education_Level' not in data.columns:
raise ValueError("'Education_Level' column is missing from the DataFrame.")
# Map the education levels to ordinal numbers
education_order = [
'High school', 'Some college', 'Trade/vocational/technical',
'Associates', 'Bachelors', 'Masters', 'Professional', 'Doctorate'
]
encoder = OrdinalEncoder(categories=[education_order])
data['Education_Level_Ordinal'] = encoder.fit_transform(data[['Education_Level']])
# Drop NaN values for all columns that will be used
data.dropna(subset=['City_center', 'Urban_area', 'Suburban_area', 'Rural_area', 'Monthly_income', 'Education_Level'], inplace=True)
# Print the columns after dropna to verify 'Education_Level_Ordinal' exists
print("Columns after dropna:", data.columns.tolist())
# Check the length of the DataFrame after dropping NaN values
print("Length of DataFrame after dropna:", len(data))
# Scale the continuous_income feature
scaler = StandardScaler()
data['scaled_income'] = scaler.fit_transform(data[['Monthly_income']])
# Define features and target
X = data[['scaled_income', 'Education_Level_Ordinal']]
y = data['City_center']  # Replace 'City_center' with your actual target column
# Check the lengths of X and y to ensure they match
print("Length of X:", len(X), "Length of y:", len(y))
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Initialize and train the GBM model with the new features
gbm_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gbm_model.fit(X_train, y_train)
# Predict on the test data and evaluate the model
y_pred_gbm = gbm_model.predict(X_test)
mse_gbm = mean_squared_error(y_test, y_pred_gbm)
r2_gbm = r2_score(y_test, y_pred_gbm)
# Output the updated performance metrics
print('GBM Mean Squared Error with Education Level:', mse_gbm)
print('GBM R-squared with Education Level:', r2_gbm)
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OrdinalEncoder
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
# Load the dataset
data_path = 'Income_LivingLocation_GradientBoostingRegressor.xlsx'
data = pd.read_excel(data_path)
# Handle NaN values
data.dropna(subset=['City_center', 'Urban_area', 'Suburban_area', 'Rural_area', 'Monthly_income', 'Distance from home to work'], inplace=True)
# Print the columns to verify the 'Education_Level' column exists
print("Columns before any processing:", data.columns.tolist())
# Check if 'Education_Level' is in the DataFrame and raise an error if not
if 'Education_Level' not in data.columns:
raise ValueError("'Education_Level' column is missing from the DataFrame.")
# Map the education levels to ordinal numbers
education_order = [
'High school', 'Some college', 'Trade/vocational/technical',
'Associates', 'Bachelors', 'Masters', 'Professional', 'Doctorate'
]
encoder = OrdinalEncoder(categories=[education_order])
data['Education_Level_Ordinal'] = encoder.fit_transform(data[['Education_Level']])
# Drop NaN values for all columns that will be used
data.dropna(subset=['City_center', 'Urban_area', 'Suburban_area', 'Rural_area', 'Monthly_income', 'Education_Level'], inplace=True)
# Print the columns after dropna to verify 'Education_Level_Ordinal' exists
print("Columns after dropna:", data.columns.tolist())
# Check the length of the DataFrame after dropping NaN values
print("Length of DataFrame after dropna:", len(data))
# Scale the continuous_income feature
scaler = StandardScaler()
data['scaled_income'] = scaler.fit_transform(data[['Monthly_income']])
# Define features and target
X = data[['scaled_income', 'Education_Level_Ordinal']]
y = data['City_center']  # Replace 'City_center' with your actual target column
# Check the lengths of X and y to ensure they match
print("Length of X:", len(X), "Length of y:", len(y))
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Initialize and train the GBM model with the new features
gbm_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gbm_model.fit(X_train, y_train)
# Predict on the test data and evaluate the model
y_pred_gbm = gbm_model.predict(X_test)
mse_gbm = mean_squared_error(y_test, y_pred_gbm)
r2_gbm = r2_score(y_test, y_pred_gbm)
# Output the updated performance metrics
print('GBM Mean Squared Error with Education Level:', mse_gbm)
print('GBM R-squared with Education Level:', r2_gbm)
import pandas as pd
import numpy as np
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
# Load the dataset
data_path = 'Income_LivingLocation_GradientBoostingRegressor.xlsx'
data = pd.read_excel(data_path)
# Create the ordinal outcome variable
area_to_number = {
'City_center': 1,
'Urban_area': 2,
'Suburban_area': 3,
'Rural_area': 4
}
# Assuming that the preference is the minimum value across the four area columns
data['living_area_preference'] = data[['City_center', 'Urban_area', 'Suburban_area', 'Rural_area']].apply(lambda row: area_to_number[row.idxmin()], axis=1)
# Scale the continuous variables
scaler = StandardScaler()
data[['Monthly_income']] = scaler.fit_transform(data[['Monthly_income']])
# Assume 'Education_Level' is a column with categorical data
# Let's encode it as an ordinal variable
education_levels = ['High school', 'Some college', 'Trade/vocational/technical', 'Associates', 'Bachelors', 'Masters', 'Professional', 'Doctorate']
data['Education_Level_Ordinal'] = data['Education_Level'].apply(lambda x: education_levels.index(x) + 1)
# Prepare the features and target variable for modeling
X = data[['Monthly_income', 'Education_Level_Ordinal']]
y = data['living_area_preference']
# Add a constant to the model (intercept)
X = sm.add_constant(X)
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Fit the model
model = sm.MNLogit(y_train, X_train)
result = model.fit()
# Summarize the model
summary = result.summary()
print(summary)
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import statsmodels.api as sm
# Load the dataset
data_path = 'Income_LivingLocation_GradientBoostingRegressor.xlsx'
data = pd.read_excel(data_path)
# Combine the area preferences into a single ordinal variable
def get_preference_rank(row):
for col in ['City_center', 'Urban_area', 'Suburban_area', 'Rural_area']:
if row[col] == 1:
return col
data['living_area_preference'] = data.apply(get_preference_rank, axis=1)
# Map the preferences to ordinal numbers
preference_mapping = {'City_center': 1, 'Urban_area': 2, 'Suburban_area': 3, 'Rural_area': 4}
data['living_area_preference'] = data['living_area_preference'].map(preference_mapping)
# Scale the 'Monthly_income' feature
scaler = StandardScaler()
data['scaled_income'] = scaler.fit_transform(data[['Monthly_income']])
# Prepare the features and target variable for modeling
X = data[['scaled_income']]
y = data['living_area_preference']
# Add a constant to the model (intercept)
X = sm.add_constant(X)
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Fit the ordinal regression model (using MNLogit as an example)
model = sm.MNLogit(y_train, X_train)
result = model.fit()
# Output the model summary
print(result.summary())
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import statsmodels.api as sm
# Load the dataset
data_path = 'Income_LivingLocation_GradientBoostingRegressor.xlsx'
data = pd.read_excel(data_path)
# Function to reverse the ranking in the dataset since 1 indicates the most preferred area
def get_preference_rank(row):
if row['City_center'] == 1:
return 4
elif row['Urban_area'] == 1:
return 3
elif row['Suburban_area'] == 1:
return 2
elif row['Rural_area'] == 1:
return 1
# Apply the function to create the 'living_area_preference' column
data['living_area_preference'] = data.apply(get_preference_rank, axis=1)
# Scale the 'Monthly_income' feature
scaler = StandardScaler()
data['scaled_income'] = scaler.fit_transform(data[['Monthly_income']])
# One-hot encode the 'House_owner_or_renter' variable
# Assuming 'Owner' is represented by '1' and 'Renter' by '2'
data['House_owner_or_renter'] = data['House_owner_or_renter'].map({'Owner': 1, 'Renter': 2})
# Prepare the features and target variable for modeling
X = data[['scaled_income', 'Education_Level', 'House_owner_or_renter', 'number of employees in household']]
y = data['living_area_preference']
# Add a constant to the model (intercept)
X = sm.add_constant(X)
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Fit the ordinal regression model (using MNLogit as an example)
model = sm.MNLogit(y_train, X_train)
result = model.fit()
# Output the model summary
print(result.summary())
quit
library(readxl)
Income_LivingLocation_GradientBoostingRegressor <- read_excel("posts/Linear and nonlinear regression/Income_LivingLocation_GradientBoostingRegressor.xlsx")
View(Income_LivingLocation_GradientBoostingRegressor)
reticulate::repl_python()
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
# Load the dataset
data = pd.read_excel('Income_LivingLocationPrefrences.xlsx')
# Handle NaN values
data.dropna(subset=['City_center', 'Urban_area', 'Suburban_area', 'Rural_area', 'Monthly_income'], inplace=True)
# Convert the Location preferences into a single ordinal dependent variable
area_to_number = {'City_center': 1, 'Urban_area': 2, 'Suburban_area': 3, 'Rural_area': 4}
data['living_Location_preference'] = data[['City_center', 'Urban_area', 'Suburban_area', 'Rural_area']].idxmin(axis=1).map(area_to_number)
# Convert income to a continuous scale based on the provided income brackets
income_mapping = {1.0: 625, 2.0: 2292, 3.0: 5000, 4.0: 6666}
data['continuous_income'] = data['Monthly_income'].map(income_mapping)
# Scale the income feature
scaler = StandardScaler()
data['scaled_income'] = scaler.fit_transform(data[['continuous_income']])
# Prepare the features and target variable for modeling
X = data[['scaled_income']]
y = data['living_Location_preference']
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Create and train the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)
# Predict on the test data
y_pred = model.predict(X_test)
# Evaluate the model's performance
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
# Output the performance metrics
print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')
# Debugging the sizes of arrays
print("Sizes of arrays for plotting:")
print("X_test['scaled_income']: ", len(X_test['scaled_income']))
print("y_test: ", len(y_test))
print("y_pred: ", len(y_pred))
# Plotting
plt.scatter(X_test['scaled_income'].values, y_test.values, color='black', label='Actual Data')
plt.scatter(X_test['scaled_income'].values, y_pred, color='red', label='Predicted Data', alpha=0.5)
# Optionally, create a more continuous line for predictions
sorted_order = np.argsort(X_test['scaled_income'].values)
plt.plot(X_test['scaled_income'].values[sorted_order], y_pred[sorted_order], color='blue', linewidth=2, label='Regression Line')
plt.xlabel('Monthly income')
plt.ylabel('Living Location Preference:moving from urban to rural index')
plt.title('Income vs Living Location Preference Linear Regression')
plt.legend()
plt.show()
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import statsmodels.api as sm
# Load the dataset
data_path = 'Income_LivingLocation_GradientBoostingRegressor.xlsx'
data = pd.read_excel(data_path)
# Function to reverse the ranking in the dataset since 1 indicates the most preferred area
def get_preference_rank(row):
if row['City_center'] == 1:
return 4
elif row['Urban_area'] == 1:
return 3
elif row['Suburban_area'] == 1:
return 2
elif row['Rural_area'] == 1:
return 1
# Apply the function to create the 'living_area_preference' column
data['living_area_preference'] = data.apply(get_preference_rank, axis=1)
# Scale the 'Monthly_income' feature
scaler = StandardScaler()
data['scaled_income'] = scaler.fit_transform(data[['Monthly_income']])
# One-hot encode the 'House_owner_or_renter' variable
# Assuming 'Owner' is represented by '1' and 'Renter' by '2'
data['House_owner_or_renter'] = data['House_owner_or_renter'].map({'Owner': 1, 'Renter': 2})
# Prepare the features and target variable for modeling
X = data[['scaled_income', 'Education_Level', 'House_owner_or_renter', 'number of employees in household']]
y = data['living_area_preference']
# Add a constant to the model (intercept)
X = sm.add_constant(X)
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Fit the ordinal regression model (using MNLogit as an example)
model = sm.MNLogit(y_train, X_train)
result = model.fit()
# Output the model summary
print(result.summary())
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import statsmodels.api as sm
# Load the dataset
data_path = 'Income_LivingLocation_GradientBoostingRegressor.xlsx'
data = pd.read_excel(data_path)
# Function to reverse the ranking in the dataset since 1 indicates the most preferred area
def get_preference_rank(row):
if row['City_center'] == 1:
return 4
elif row['Urban_area'] == 1:
return 3
elif row['Suburban_area'] == 1:
return 2
elif row['Rural_area'] == 1:
return 1
# Apply the function to create the 'living_area_preference' column
data['living_area_preference'] = data.apply(get_preference_rank, axis=1)
# Scale the 'Monthly_income' feature
scaler = StandardScaler()
data['scaled_income'] = scaler.fit_transform(data[['Monthly_income']])
# One-hot encode the 'House_owner_or_renter' variable
# Assuming 'Owner' is represented by '1' and 'Renter' by '2'
data['House_owner_or_renter'] = data['House_owner_or_renter'].map({'Owner': 1, 'Renter': 2})
# Prepare the features and target variable for modeling
X = data[['scaled_income', 'Education_Level', 'House_owner_or_renter', 'number of employees in household']]
y = data['living_area_preference']
# Add a constant to the model (intercept)
X = sm.add_constant(X)
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Fit the ordinal regression model (using MNLogit as an example)
model = sm.MNLogit(y_train, X_train)
result = model.fit()
# Output the model summary
print(result.summary())
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import statsmodels.api as sm
# Load the dataset
data_path = 'Income_LivingLocation_GradientBoostingRegressor.xlsx'  # Replace with the actual path to your dataset
data = pd.read_excel(data_path)
# Function to reverse the ranking in the dataset since 1 indicates the most preferred area
def get_preference_rank(row):
if row['City_center'] == 1:
return 4
elif row['Urban_area'] == 1:
return 3
elif row['Suburban_area'] == 1:
return 2
elif row['Rural_area'] == 1:
return 1
# Apply the function to create the 'living_area_preference' column
data['living_area_preference'] = data.apply(get_preference_rank, axis=1)
# Scale the 'Monthly_income' feature
scaler = StandardScaler()
data['scaled_income'] = scaler.fit_transform(data[['Monthly_income']])
# One-hot encode the 'House_owner_or_renter' variable
# Assuming 'Owner' is represented by '1' and 'Renter' by '2'
data['House_owner_or_renter'] = data['House_owner_or_renter'].map({'Owner': 1, 'Renter': 2})
# Prepare the features and target variable for modeling
X = data[['scaled_income', 'Education_Level', 'House_owner_or_renter', 'number of employees in household']]
y = data['living_area_preference']
# Add a constant to the model (intercept)
X = sm.add_constant(X)
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Fit the ordinal regression model (using MNLogit as an example)
model = sm.MNLogit(y_train, X_train)
result = model.fit()
# Output the model summary
print(result.summary())
quit
library(readxl)
Income_LivingLocation_GradientBoostingRegressor <- read_excel("posts/Linear and nonlinear regression/Income_LivingLocation_GradientBoostingRegressor.xlsx")
View(Income_LivingLocation_GradientBoostingRegressor)
reticulate::repl_python()
import pandas as pd
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
# Load the dataset
data = pd.read_excel('path_to_your_file.xlsx')  # Update the path to your file
# Map the income responses to the categorical income variable
data['income_category'] = data['Monthly_income'].map({
1: '1250_or_less',
2: '1251_to_3333',
3: 'greater_than_3333',
4: '6666_or_more'
})
# Use pd.get_dummies to convert 'income_category' into one-hot encoded variables
income_dummies = pd.get_dummies(data['income_category'], drop_first=True)
# Assuming that 'City_center', 'Urban_area', 'Suburban_area', 'Rural_area' are the categories for living area preference
# We can create an ordinal variable where 1 is most preferred and 4 is least preferred
data['living_area_preference'] = data[['City_center', 'Urban_area', 'Suburban_area', 'Rural_area']].idxmin(axis=1).map(area_to_number)
# Prepare the features (including the one-hot encoded income variable) and the target variable
X = pd.concat([income_dummies, data[['Education_Level', 'House_owner_or_renter', 'number of employees in household']]], axis=1)
y = data['living_area_preference']
# Add a constant to the model (intercept)
X = sm.add_constant(X)
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Fit the ordinal regression model
model = sm.MNLogit(y_train, X_train)
result = model.fit()
# Output the model summary
print(result.summary())
import pandas as pd
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
# Load the dataset
data = pd.read_excel('Income_LivingLocation_GradientBoostingRegressor.xlsx')  # Update the path to your file
# Map the income responses to the categorical income variable
data['income_category'] = data['Monthly_income'].map({
1: '1250_or_less',
2: '1251_to_3333',
3: 'greater_than_3333',
4: '6666_or_more'
})
# Use pd.get_dummies to convert 'income_category' into one-hot encoded variables
income_dummies = pd.get_dummies(data['income_category'], drop_first=True)
# Assuming that 'City_center', 'Urban_area', 'Suburban_area', 'Rural_area' are the categories for living area preference
# We can create an ordinal variable where 1 is most preferred and 4 is least preferred
data['living_area_preference'] = data[['City_center', 'Urban_area', 'Suburban_area', 'Rural_area']].idxmin(axis=1).map(area_to_number)
# Prepare the features (including the one-hot encoded income variable) and the target variable
X = pd.concat([income_dummies, data[['Education_Level', 'House_owner_or_renter', 'number of employees in household']]], axis=1)
y = data['living_area_preference']
# Add a constant to the model (intercept)
X = sm.add_constant(X)
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Fit the ordinal regression model
model = sm.MNLogit(y_train, X_train)
result = model.fit()
# Output the model summary
print(result.summary())
