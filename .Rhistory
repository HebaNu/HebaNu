# Full code with the additional independent variable and plotting the regression curve
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
# Load the dataset
data_path = 'Income_LivingLocationPrefrences.xlsx'
data = pd.read_excel(data_path)
# Handle NaN values
data.dropna(subset=['City_center', 'Urban_area', 'Suburban_area', 'Rural_area', 'Monthly_income', 'Distance from home to work'], inplace=True)
# Create a new column 'living_area_preference' as the average of the Likert-scale responses
data['living_area_preference'] = data[['City_center', 'Urban_area', 'Suburban_area', 'Rural_area']].mean(axis=1)
# Convert income to a continuous scale based on the provided income brackets
income_mapping = {1.0: 625, 2.0: 2292, 3.0: 5000, 4.0: 6666}
data['continuous_income'] = data['Monthly_income'].map(income_mapping)
# Scale the income feature and the 'Distance from home to work' feature
scaler = StandardScaler()
data[['scaled_income', 'scaled_distance']] = scaler.fit_transform(data[['Monthly_income', 'Distance from home to work']])
# Create polynomial features including the new variable
X = data[['scaled_income', 'scaled_distance']]
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_poly, data['living_area_preference'], test_size=0.2, random_state=42)
# Polynomial Regression Model
poly_model = LinearRegression()
poly_model.fit(X_train, y_train)
y_pred_poly = poly_model.predict(X_test)
mse_poly = mean_squared_error(y_test, y_pred_poly)
r2_poly = r2_score(y_test, y_pred_poly)
# Output the results for Polynomial Regression Model
print("Polynomial Regression Model with Distance Variable:")
print(f"Mean Squared Error: {mse_poly}")
print(f"R-squared: {r2_poly}")
# Since we have two independent variables, we can't simply plot a 2D curve.
# Instead, we'll plot the actual vs predicted values for one of the variables (scaled_income).
# Scatter plot of actual vs predicted values
plt.scatter(X_test[:, 1], y_test, alpha=0.5, color='blue', label='Actual')  # Scaled income for x-axis
plt.scatter(X_test[:, 1], y_pred_poly, alpha=0.5, color='red', label='Predicted')
# Adding plot labels and title
plt.xlabel('Scaled Monthly Income')
plt.ylabel('Predicted Living Area Preference')
plt.title('Polynomial Regression with Distance Variable: Actual vs Predicted Values')
plt.legend()
# Show the plot
plt.show()
from mpl_toolkits.mplot3d import Axes3D
# Assuming 'scaled_income' is X1 and 'scaled_distance' is X2
X1 = data['scaled_income'].values.reshape(-1, 1)
X2 = data['scaled_distance'].values.reshape(-1, 1)
# Create a mesh grid for 3D plot
X1_grid, X2_grid = np.meshgrid(np.linspace(X1.min(), X1.max(), 100),
np.linspace(X2.min(), X2.max(), 100))
# Combine X1 and X2 grid for model prediction
X_grid = np.c_[X1_grid.ravel(), X2_grid.ravel()]
X_poly_grid = poly.fit_transform(X_grid)
# Predict on the grid
y_grid_pred = poly_model.predict(X_poly_grid).reshape(X1_grid.shape)
# Plotting
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
# Plot the surface
ax.plot_surface(X1_grid, X2_grid, y_grid_pred, cmap='viridis', alpha=0.7)
# Scatter plot of actual values
ax.scatter(X1, X2, y_test, color='red', label='Actual')
ax.set_xlabel('Scaled Monthly Income')
ax.set_ylabel('Distance from Home to Work')
ax.set_zlabel('Predicted Living Area Preference')
ax.legend()
plt.show()
# Since the last execution to verify the column names did not work, let's proceed with the updated code
# for polynomial regression including the additional variable "Distance from home to work" and the plot.
# I will assume the column names provided are correct and update the code accordingly.
# Full code with the additional independent variable and plotting the regression curve
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
# Load the dataset
data_path = 'Income_LivingLocationPrefrences.xlsx'
data = pd.read_excel(data_path)
# Handle NaN values
data.dropna(subset=['City_center', 'Urban_area', 'Suburban_area', 'Rural_area', 'Monthly_income', 'Distance from home to work'], inplace=True)
# Create a new column 'living_area_preference' as the average of the Likert-scale responses
data['living_area_preference'] = data[['City_center', 'Urban_area', 'Suburban_area', 'Rural_area']].mean(axis=1)
# Convert income to a continuous scale based on the provided income brackets
income_mapping = {1.0: 625, 2.0: 2292, 3.0: 5000, 4.0: 6666}
data['continuous_income'] = data['Monthly_income'].map(income_mapping)
# Scale the income feature and the 'Distance from home to work' feature
scaler = StandardScaler()
data[['scaled_income', 'scaled_distance']] = scaler.fit_transform(data[['Monthly_income', 'Distance from home to work']])
# Create polynomial features including the new variable
X = data[['scaled_income', 'scaled_distance']]
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_poly, data['living_area_preference'], test_size=0.2, random_state=42)
# Polynomial Regression Model
poly_model = LinearRegression()
poly_model.fit(X_train, y_train)
y_pred_poly = poly_model.predict(X_test)
mse_poly = mean_squared_error(y_test, y_pred_poly)
r2_poly = r2_score(y_test, y_pred_poly)
# Output the results for Polynomial Regression Model
print("Polynomial Regression Model with Distance Variable:")
print(f"Mean Squared Error: {mse_poly}")
print(f"R-squared: {r2_poly}")
# Since we have two independent variables, we can't simply plot a 2D curve.
# Instead, we'll plot the actual vs predicted values for one of the variables (scaled_income).
# Scatter plot of actual vs predicted values
plt.scatter(X_test[:, 1], y_test, alpha=0.5, color='blue', label='Actual')  # Scaled income for x-axis
plt.scatter(X_test[:, 1], y_pred_poly, alpha=0.5, color='red', label='Predicted')
# Adding plot labels and title
plt.xlabel('Scaled Monthly Income')
plt.ylabel('Predicted Living Area Preference')
plt.title('Polynomial Regression with Distance Variable: Actual vs Predicted Values')
plt.legend()
# Show the plot
plt.show()
from mpl_toolkits.mplot3d import Axes3D
# Assuming 'scaled_income' is X1 and 'scaled_distance' is X2
X1 = data['scaled_income'].values.reshape(-1, 1)
X2 = data['scaled_distance'].values.reshape(-1, 1)
# Create a mesh grid for 3D plot
X1_grid, X2_grid = np.meshgrid(np.linspace(X1.min(), X1.max(), 100),
np.linspace(X2.min(), X2.max(), 100))
# Combine X1 and X2 grid for model prediction
X_grid = np.c_[X1_grid.ravel(), X2_grid.ravel()]
X_poly_grid = poly.fit_transform(X_grid)
# Predict on the grid
y_grid_pred = poly_model.predict(X_poly_grid).reshape(X1_grid.shape)
# Plotting
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
# Plot the surface
ax.plot_surface(X1_grid, X2_grid, y_grid_pred, cmap='viridis', alpha=0.7)
# Scatter plot of actual values
ax.scatter(X1, X2, y_test, color='red', label='Actual')
ax.set_xlabel('Scaled Monthly Income')
ax.set_ylabel('Distance from Home to Work')
ax.set_zlabel('Predicted Living Area Preference')
ax.legend()
plt.show()
# Since the last execution to verify the column names did not work, let's proceed with the updated code
# for polynomial regression including the additional variable "Distance from home to work" and the plot.
# I will assume the column names provided are correct and update the code accordingly.
# Full code with the additional independent variable and plotting the regression curve
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
# Load the dataset
data_path = 'Income_LivingLocationPrefrences.xlsx'
data = pd.read_excel(data_path)
# Handle NaN values
data.dropna(subset=['City_center', 'Urban_area', 'Suburban_area', 'Rural_area', 'Monthly_income', 'Distance from home to work'], inplace=True)
# Create a new column 'living_area_preference' as the average of the Likert-scale responses
data['living_area_preference'] = data[['City_center', 'Urban_area', 'Suburban_area', 'Rural_area']].mean(axis=1)
# Convert income to a continuous scale based on the provided income brackets
income_mapping = {1.0: 625, 2.0: 2292, 3.0: 5000, 4.0: 6666}
data['continuous_income'] = data['Monthly_income'].map(income_mapping)
# Scale the income feature and the 'Distance from home to work' feature
scaler = StandardScaler()
data[['scaled_income', 'scaled_distance']] = scaler.fit_transform(data[['Monthly_income', 'Distance from home to work']])
# Create polynomial features including the new variable
X = data[['scaled_income', 'scaled_distance']]
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_poly, data['living_area_preference'], test_size=0.2, random_state=42)
# Polynomial Regression Model
poly_model = LinearRegression()
poly_model.fit(X_train, y_train)
y_pred_poly = poly_model.predict(X_test)
mse_poly = mean_squared_error(y_test, y_pred_poly)
r2_poly = r2_score(y_test, y_pred_poly)
# Output the results for Polynomial Regression Model
print("Polynomial Regression Model with Distance Variable:")
print(f"Mean Squared Error: {mse_poly}")
print(f"R-squared: {r2_poly}")
# Since we have two independent variables, we can't simply plot a 2D curve.
# Instead, we'll plot the actual vs predicted values for one of the variables (scaled_income).
# Scatter plot of actual vs predicted values
plt.scatter(X_test[:, 1], y_test, alpha=0.5, color='blue', label='Actual')  # Scaled income for x-axis
plt.scatter(X_test[:, 1], y_pred_poly, alpha=0.5, color='red', label='Predicted')
# Adding plot labels and title
plt.xlabel('Scaled Monthly Income')
plt.ylabel('Predicted Living Area Preference')
plt.title('Polynomial Regression with Distance Variable: Actual vs Predicted Values')
plt.legend()
# Show the plot
plt.show()
# Since the last execution to verify the column names did not work, let's proceed with the updated code
# for polynomial regression including the additional variable "Distance from home to work" and the plot.
# I will assume the column names provided are correct and update the code accordingly.
# Full code with the additional independent variable and plotting the regression curve
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
# Load the dataset
data_path = 'Income_LivingLocationPrefrences.xlsx'
data = pd.read_excel(data_path)
# Handle NaN values
data.dropna(subset=['City_center', 'Urban_area', 'Suburban_area', 'Rural_area', 'Monthly_income', 'Distance from home to work'], inplace=True)
# Create a new column 'living_area_preference' as the average of the Likert-scale responses
data['living_area_preference'] = data[['City_center', 'Urban_area', 'Suburban_area', 'Rural_area']].mean(axis=1)
# Convert income to a continuous scale based on the provided income brackets
income_mapping = {1.0: 625, 2.0: 2292, 3.0: 5000, 4.0: 6666}
data['continuous_income'] = data['Monthly_income'].map(income_mapping)
# Scale the income feature and the 'Distance from home to work' feature
scaler = StandardScaler()
data[['scaled_income', 'scaled_distance']] = scaler.fit_transform(data[['Monthly_income', 'Distance from home to work']])
# Create polynomial features including the new variable
X = data[['scaled_income', 'scaled_distance']]
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_poly, data['living_area_preference'], test_size=0.2, random_state=42)
# Polynomial Regression Model
poly_model = LinearRegression()
poly_model.fit(X_train, y_train)
y_pred_poly = poly_model.predict(X_test)
mse_poly = mean_squared_error(y_test, y_pred_poly)
r2_poly = r2_score(y_test, y_pred_poly)
# Output the results for Polynomial Regression Model
print("Polynomial Regression Model with Distance Variable:")
print(f"Mean Squared Error: {mse_poly}")
print(f"R-squared: {r2_poly}")
# Since we have two independent variables, we can't simply plot a 2D curve.
# Instead, we'll plot the actual vs predicted values for one of the variables (scaled_income).
# Scatter plot of actual vs predicted values
plt.scatter(X_test[:, 1], y_test, alpha=0.5, color='blue', label='Actual')  # Scaled income for x-axis
plt.scatter(X_test[:, 1], y_pred_poly, alpha=0.5, color='red', label='Predicted')
# Adding plot labels and title
plt.xlabel('Scaled Monthly Income')
plt.ylabel('Predicted Living Area Preference')
plt.title('Polynomial Regression with Distance Variable: Actual vs Predicted Values')
plt.legend()
# Show the plot
plt.show()
quit
library(readxl)
Income_LivingLocationPrefrences <- read_excel("posts/Linear and nonlinear regression/Income_LivingLocationPrefrences.xlsx")
View(Income_LivingLocationPrefrences)
reticulate::repl_python()
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import numpy as np
# Load the dataset
data = pd.read_excel('Income_LivingLocationPrefrences.xlsx')
# Drop rows with NaN values in the area preference and income columns
data.dropna(subset=['City_center', 'Urban_area', 'Suburban_area', 'Rural_area', 'Monthly_income'], inplace=True)
# Convert the area preferences into a single ordinal dependent variable
area_to_number = {'City_center': 1, 'Urban_area': 2, 'Suburban_area': 3, 'Rural_area': 4}
data['living_area_preference'] = data[['City_center', 'Urban_area', 'Suburban_area', 'Rural_area']].idxmin(axis=1).map(area_to_number)
# Convert income to a continuous scale based on the provided income brackets
income_mapping = {1.0: 625, 2.0: 2292, 3.0: 5000, 4.0: 6666}  # Adjust as needed
data['continuous_income'] = data['Monthly_income'].map(income_mapping)
# Drop the original area preference columns and the 'Monthly Income' column
data.drop(columns=['City_center', 'Urban_area', 'Suburban_area', 'Rural_area', 'Monthly_income'], inplace=True)
# Ensure no NaN values in 'continuous_income'
data.dropna(subset=['continuous_income'], inplace=True)
# Scale the income feature
scaler = StandardScaler()
data['scaled_income'] = scaler.fit_transform(data[['continuous_income']])
# Prepare the features and target variable for modeling
X = data[['scaled_income']]
y = data['living_area_preference']
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Create and train the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)
# Predict on the test data
y_pred = model.predict(X_test)
# Evaluate the model's performance
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
# Output the performance metrics
print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')
# Plotting modifications start here
# Extract the original income values corresponding to the test set
original_income_test = X_test.index.map(lambda idx: data.loc[idx, 'continuous_income'])
# Plotting with original income values
plt.scatter(original_income_test, y_test, color='black', label='Actual Data')
plt.scatter(original_income_test, y_pred, color='red', label='Predicted Data', alpha=0.5)
# Optionally, create a more continuous line for predictions (optional, for visual enhancement)
sorted_order = np.argsort(original_income_test)
plt.plot(np.array(original_income_test)[sorted_order], np.array(y_pred)[sorted_order], color='blue', linewidth=2, label='Regression Line')
plt.xlabel('Monthly income')
plt.ylabel('Moving index_ from City Center to Rural Area')
plt.title('Income vs Living Area Preference Linear Regression')
plt.legend()
plt.show()
# Since the last execution to verify the column names did not work, let's proceed with the updated code
# for polynomial regression including the additional variable "Distance from home to work" and the plot.
# I will assume the column names provided are correct and update the code accordingly.
# Full code with the additional independent variable and plotting the regression curve
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
# Load the dataset
data_path = 'Income_LivingLocationPrefrences.xlsx'
data = pd.read_excel(data_path)
# Handle NaN values
data.dropna(subset=['City_center', 'Urban_area', 'Suburban_area', 'Rural_area', 'Monthly_income', 'Distance from home to work'], inplace=True)
# Create a new column 'living_area_preference' as the average of the Likert-scale responses
data['living_area_preference'] = data[['City_center', 'Urban_area', 'Suburban_area', 'Rural_area']].mean(axis=1)
# Convert income to a continuous scale based on the provided income brackets
income_mapping = {1.0: 625, 2.0: 2292, 3.0: 5000, 4.0: 6666}
data['continuous_income'] = data['Monthly_income'].map(income_mapping)
# Scale the income feature and the 'Distance from home to work' feature
scaler = StandardScaler()
data[['scaled_income', 'scaled_distance']] = scaler.fit_transform(data[['Monthly_income', 'Distance from home to work']])
# Create polynomial features including the new variable
X = data[['scaled_income', 'scaled_distance']]
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_poly, data['living_area_preference'], test_size=0.2, random_state=42)
# Polynomial Regression Model
poly_model = LinearRegression()
poly_model.fit(X_train, y_train)
y_pred_poly = poly_model.predict(X_test)
mse_poly = mean_squared_error(y_test, y_pred_poly)
r2_poly = r2_score(y_test, y_pred_poly)
# Output the results for Polynomial Regression Model
print("Polynomial Regression Model with Distance Variable:")
print(f"Mean Squared Error: {mse_poly}")
print(f"R-squared: {r2_poly}")
# Since we have two independent variables, we can't simply plot a 2D curve.
# Instead, we'll plot the actual vs predicted values for one of the variables (scaled_income).
# Scatter plot of actual vs predicted values
plt.scatter(X_test[:, 1], y_test, alpha=0.5, color='blue', label='Actual')  # Scaled income for x-axis
plt.scatter(X_test[:, 1], y_pred_poly, alpha=0.5, color='red', label='Predicted')
# Adding plot labels and title
plt.xlabel('Scaled Monthly Income')
plt.ylabel('Predicted Living Area Preference')
plt.title('Polynomial Regression with Distance Variable: Actual vs Predicted Values')
plt.legend()
# Show the plot
plt.show()
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import numpy as np
# Load the dataset
data = pd.read_excel('Income_LivingLocationPrefrences.xlsx')
# Drop rows with NaN values in the area preference and income columns
data.dropna(subset=['City_center', 'Urban_area', 'Suburban_area', 'Rural_area', 'Monthly_income'], inplace=True)
# Convert the area preferences into a single ordinal dependent variable
area_to_number = {'City_center': 1, 'Urban_area': 2, 'Suburban_area': 3, 'Rural_area': 4}
data['living_area_preference'] = data[['City_center', 'Urban_area', 'Suburban_area', 'Rural_area']].idxmin(axis=1).map(area_to_number)
# Convert income to a continuous scale based on the provided income brackets
income_mapping = {1.0: 625, 2.0: 2292, 3.0: 5000, 4.0: 6666}  # Adjust as needed
data['continuous_income'] = data['Monthly_income'].map(income_mapping)
# Drop the original area preference columns and the 'Monthly Income' column
data.drop(columns=['City_center', 'Urban_area', 'Suburban_area', 'Rural_area', 'Monthly_income'], inplace=True)
# Ensure no NaN values in 'continuous_income'
data.dropna(subset=['continuous_income'], inplace=True)
# Scale the income feature
scaler = StandardScaler()
data['scaled_income'] = scaler.fit_transform(data[['continuous_income']])
# Prepare the features and target variable for modeling
X = data[['scaled_income']]
y = data['living_area_preference']
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Create and train the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)
# Predict on the test data
y_pred = model.predict(X_test)
# Evaluate the model's performance
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
# Output the performance metrics
print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')
# Extract the original income values corresponding to the test set
# Make sure the index aligns correctly with the test data
original_income_test = X_test['scaled_income']
# Plotting with original income values
plt.scatter(original_income_test, y_test, color='black', label='Actual Data')
plt.scatter(original_income_test, y_pred, color='red', label='Predicted Data', alpha=0.5)
# Optionally, create a more continuous line for predictions (optional, for visual enhancement)
sorted_order = np.argsort(original_income_test)
plt.plot(np.array(original_income_test)[sorted_order], np.array(y_pred)[sorted_order], color='blue', linewidth=2, label='Regression Line')
plt.xlabel('Monthly income')
plt.ylabel('Moving index_ from City Center to Rural Area')
plt.title('Income vs Living Area Preference Linear Regression')
plt.legend()
plt.show()
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
# Load the dataset
data = pd.read_excel('Income_LivingLocationPrefrences.xlsx')
# Handle NaN values
data.dropna(subset=['City_center', 'Urban_area', 'Suburban_area', 'Rural_area', 'Monthly_income'], inplace=True)
# Convert the area preferences into a single ordinal dependent variable
area_to_number = {'City_center': 1, 'Urban_area': 2, 'Suburban_area': 3, 'Rural_area': 4}
data['living_area_preference'] = data[['City_center', 'Urban_area', 'Suburban_area', 'Rural_area']].idxmin(axis=1).map(area_to_number)
# Convert income to a continuous scale based on the provided income brackets
income_mapping = {1.0: 625, 2.0: 2292, 3.0: 5000, 4.0: 6666}  # Adjust as needed
data['continuous_income'] = data['Monthly_income'].map(income_mapping)
# Scale the income feature
scaler = StandardScaler()
data['scaled_income'] = scaler.fit_transform(data[['continuous_income']])
# Prepare the features and target variable for modeling
X = data[['scaled_income']]
y = data['living_area_preference']
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Create and train the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)
# Predict on the test data
y_pred = model.predict(X_test)
# Evaluate the model's performance
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
# Output the performance metrics
print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')
# Plotting
plt.scatter(X_test['scaled_income'], y_test, color='black', label='Actual Data')
plt.scatter(X_test['scaled_income'], y_pred, color='red', label='Predicted Data', alpha=0.5)
# Optionally, create a more continuous line for predictions (for visual enhancement)
sorted_order = np.argsort(X_test['scaled_income'].values)
plt.plot(X_test['scaled_income'].values[sorted_order], y_pred[sorted_order], color='blue', linewidth=2, label='Regression Line')
plt.xlabel('Monthly income')
plt.ylabel('Living Area Preference')
plt.title('Income vs Living Area Preference Linear Regression')
plt.legend()
plt.show()
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
# Load the dataset
data = pd.read_excel('Income_LivingLocationPrefrences.xlsx')
# Handle NaN values
data.dropna(subset=['City_center', 'Urban_area', 'Suburban_area', 'Rural_area', 'Monthly_income'], inplace=True)
# Convert the area preferences into a single ordinal dependent variable
area_to_number = {'City_center': 1, 'Urban_area': 2, 'Suburban_area': 3, 'Rural_area': 4}
data['living_area_preference'] = data[['City_center', 'Urban_area', 'Suburban_area', 'Rural_area']].idxmin(axis=1).map(area_to_number)
# Convert income to a continuous scale based on the provided income brackets
income_mapping = {1.0: 625, 2.0: 2292, 3.0: 5000, 4.0: 6666}
data['continuous_income'] = data['Monthly_income'].map(income_mapping)
# Scale the income feature
scaler = StandardScaler()
data['scaled_income'] = scaler.fit_transform(data[['continuous_income']])
# Prepare the features and target variable for modeling
X = data[['scaled_income']]
y = data['living_area_preference']
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Create and train the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)
# Predict on the test data
y_pred = model.predict(X_test)
# Evaluate the model's performance
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
# Output the performance metrics
print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')
# Plotting
plt.scatter(X_test['scaled_income'].values, y_test.values, color='black', label='Actual Data')
plt.scatter(X_test['scaled_income'].values, y_pred, color='red', label='Predicted Data', alpha=0.5)
# Optionally, create a more continuous line for predictions (for visual enhancement)
sorted_order = np.argsort(X_test['scaled_income'].values)
plt.plot(X_test['scaled_income'].values[sorted_order], y_pred[sorted_order], color='blue', linewidth=2, label='Regression Line')
plt.xlabel('Monthly income')
plt.ylabel('Living Area Preference')
plt.title('Income vs Living Area Preference Linear Regression')
plt.legend()
plt.show()
