[
  {
    "objectID": "Research Overview.html",
    "href": "Research Overview.html",
    "title": "Research Overview",
    "section": "",
    "text": "Work From Home and Cityscape\nExperience the transformative shift in work dynamics worldwide. Fueled by factors like flexibility and propelled by the seismic impact of the Covid-19 pandemic, remote work, particularly Working From Home (WFH), has become a prevailing trend. This change in work culture is not merely a temporary response but a catalyst for a prolonged paradigm shift in workplaces globally.\nAs the emphasis on proximity to the workplace diminishes, individuals are opting for relocations to suburban areas, seeking affordable housing options without compromising living standards. This evolution in the work landscape intertwines with broader implications for cities—altering socioeconomic structures, reshaping urban landscapes, and influencing environmental dynamics.\n\nJoin us on this exploration of a dynamic era where work and living spaces converge, uncovering the profound effects of this transformative journey on cities, society, and the natural environment. Welcome to a new era in the world of work and living."
  },
  {
    "objectID": "posts/Probability theory and random variables/index.html",
    "href": "posts/Probability theory and random variables/index.html",
    "title": "Decoding Work Patterns, Analyzing Work From Home vs. Work On Site Trends in the 2023 US Job Market",
    "section": "",
    "text": "As we navigate through the evolving landscape of the 2023 job market, we are witnessing a significant shift in work methods. More people are embracing Work From Home (WFH) while others continue with Work On Site (WOS). This post helps to understand how these work methods vary across different jobs in the USA.\nMy Approach: For this analysis, I’ve utilized the Naive Bayes classifier for multivariate Bernoulli models. This method is excellent for sorting jobs into two main categories: WFH and WOS. It’s a practical choice for my study because it efficiently handles binary data—like choosing between remote or on-site work.\nAbout the Data: The data comes from a 2023 survey of 850 participants from various occupations. Participants were asked about their industry sector and working method. To respect data confidentiality, we’ve modified and anonymized the dataset. This ensures compliance with data privacy guidelines without affecting the overall analysis and insights.\n\n\n\nThe survey participants from workers and their working method (WFH, or WON).\n\n\nIndustry Breakdown: The study categorized 20 main category for jobs according to the North American Industry Classification System (NAICS), aligning with the 6-digit Standard Occupational Classification (SOC) system from the U.S. Bureau of Labor Statistics (BLS). Here’s a table of these industry sectors:\n\n\n\n\n\n\n\n\n#\nThe occupations categories are based on the 6-digit (SOC) system from the U.S. (BLS)\nIndustry Sector: Occupational Classification\n\n\n\n\n1\nNumber of jobs in NAICS sector 11\n(Agriculture, Forestry, Fishing and Hunting)\n\n\n2\ncns02 Number of jobs in NAICS sector 21\n(Mining, Quarrying, and Oil and Gas Extraction)\n\n\n3\ncns03 Number of jobs in NAICS sector 22\n(Utilities)\n\n\n4\ncns04 Number of jobs in NAICS sector 23\n(Construction)\n\n\n5\ncns05 Number of jobs in NAICS sector 31‐33\n(Manufacturing)\n\n\n6\ncns06 Number of jobs in NAICS sector 42\n(Wholesale Trade)\n\n\n7\ncns07 Number of jobs in NAICS sector 44‐45\n(Retail Trade)\n\n\n8\ncns08 Number of jobs in NAICS sector 48‐49\n(Transportation and Warehousing)\n\n\n9\ncns09 Number of jobs in NAICS sector 51\n(Information)\n\n\n10\ncns10 Number of jobs in NAICS sector 52\n(Finance and Insurance)\n\n\n11\ncns11 Number of jobs in NAICS sector 53\n(Real Estate and Rental and Leasing)\n\n\n12\ncns12 Number of jobs in NAICS sector 54\n(Professional, Scientific, and Technical Services)\n\n\n13\ncns13 Number of jobs in NAICS sector 55\n(Management of Companies and Enterprises)\n\n\n14\ncns14 Number of jobs in NAICS sector 56\n(Administrative and Support and Waste Management and Remediation Services)\n\n\n15\ncns15 Number of jobs in NAICS sector 61\n(Educational Services)\n\n\n16\ncns16 Number of jobs in NAICS sector 62\n(Health Care and Social Assistance)\n\n\n17\ncns17 Number of jobs in NAICS sector 71\n(Arts, Entertainment, and Recreation)\n\n\n18\ncns18 Number of jobs in NAICS sector 72\n(Accommodation and Food Services)\n\n\n19\ncns19 Number of jobs in NAICS sector 81\n(Other Services [except Public Administration])\n\n\n20\ncns20 Number of jobs in NAICS sector 92\n(Public Administration)\n\n\n\nGoal: My goal is to provide a clear and real picture of the changing work styles. I am not just analyzing numbers; I am uncovering the stories they tell about how people are adapting to new work environments, be it remotely or on-site.\nWhy I Chose the Naive Bayes Classifier for Multivariate Bernoulli Models:\nIn my exploration of work methods—remote and on-site—the Naive Bayes classifier for multivariate Bernoulli models shines with its simplicity and effectiveness. It excels at evaluating probabilities, helping determine whether a job is more likely to be remote or on-site based on its industry sector. This algorithm’s strength lies in its straightforward approach, providing clear baseline assessments that guide our analysis across different industry sectors. Dive Into the Code Behind the Analysis:\nx: The industry sector: [jobs categories From 1 to 20]\ny: The work method: 1 for WOS [Work on Site], 2 for WFH [Work From Home]\n\n\n\nCode\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.preprocessing import OneHotEncoder\nimport os\n\n# Load the dataset\ndf = pd.read_csv('C:/Users/NUSAI/Desktop/Machine learning/HebaNu.github.io/HebaNu.github.io/HebaNu/posts/Probability theory and random variables/updated_POST1.csv')\n\n# Drop rows where any cell is NaN in the 'Work Method' column\ndf = df.dropna(subset=['Work Method'])\n\n# One-hot encoding\nencoder = OneHotEncoder(sparse=False)\nX = encoder.fit_transform(df[[' industry sector']])  \n# includes 20 occupation types from NAICS for 6-digit SOC- U.S. BLS\n\n# Target variable\ny = df['Work Method']\n# 1 for WOS [Work on Site], 2 for WFH [Work From Home]\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train the Bernoulli Naive Bayes model\nmodel = BernoulliNB()\nmodel.fit(X_train, y_train)\n\n# Make predictions and evaluate the model\ny_pred = model.predict(X_test)\n\n\n\n# Output the classification report and accuracy\nprint(classification_report(y_test, y_pred, zero_division=0))\nprint(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n\n\n              precision    recall  f1-score   support\n\n         1.0       0.00      0.00      0.00        40\n         2.0       0.76      1.00      0.87       130\n\n    accuracy                           0.76       170\n   macro avg       0.38      0.50      0.43       170\nweighted avg       0.58      0.76      0.66       170\n\nAccuracy: 0.7647058823529411\n\n\nC:\\Users\\NUSAI\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning:\n\n`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n\n\n\nInsights from Our Model: Understanding the Work Trends\nThe results from our BernoulliNB model offer fascinating insights into the current work trends. The model is predicting class 2.0 for WFH [Work From Home] with high precision (76%) and recall (100%), which means for this class, it performs well both in terms of the accuracy of the positive predictions it makes (precision) and its ability to find all the positive instances (recall).\nFor class 1.0, however, the model does not predict any instances correctly, which suggests that either there’s an issue with the distribution of your classes (perhaps class 1.0 is underrepresented), or that the features do not provide enough information to distinguish class 1.0 from class 2.0. The overall accuracy of the model is 76.47%, which means that it correctly predicts the class for 76.47% of the test set.\nThe macro avg and weighted avg for precision, recall, and f1-score provide a summary of the effectiveness of the model across the classes. The low macro avg for precision and f1-score indicates that one of the classes does not perform well, which we already know is class 1.0.\nThe f1-score is a harmonic mean of precision and recall and is a useful metric when you have classes that are imbalanced. In my case, the f1-score for class 1.0 [WOS] is 0.00, indicating poor performance for this class.\nGenerally, the Naive Bayes classifier for multivariate Bernoulli models proves to be an effective and reliable tool for classification tasks. Its ability to handle binary data, like our work method categories, contributes to its robustness and applicability in various analytical scenarios.\nTo create plots that visualize the performance of your BernoulliNB model, you would typically look at the confusion matrix, precision-recall, and possibly ROC curves. Below are examples of how you could generate each of these plots using matplotlib and scikit-learn:\n\nConfusion Matrix: Visualizes the correct and incorrect predictions compared to the actual values.\n\n\n\nCode\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming y_test and y_pred are already defined from your BernoulliNB model\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.title('Confusion Matrix for BernoulliNB Model')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n\n\n\n\n\nROC Curve: Plots the true positive rate against the false positive rate.\n\n\nCode\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import RocCurveDisplay\nimport matplotlib.pyplot as plt\n\n# Adjust y_test to have binary labels 0 and 1\ny_test_binary = y_test.replace({1: 0, 2: 1})\n\n# Get predicted probabilities for the positive class (e.g., class 2)\ny_pred_prob = model.predict_proba(X_test)[:, 1]  # Index 1 for the probability of class 2\n\n# Calculate the ROC curve\nfpr, tpr, thresholds = roc_curve(y_test_binary, y_pred_prob)\n\n# Calculate the AUC (Area Under Curve)\nroc_auc = auc(fpr, tpr)\n\n# Plot the ROC curve\ndisp = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\ndisp.plot()\nplt.title('ROC Curve for BernoulliNB Model')\nplt.show()\n\n\n\n\n\nClassification Report: Presents precision, recall, f1-score for each class.\n\n\nCode\nfrom sklearn.metrics import classification_report\nimport pandas as pd\n\nreport = classification_report(y_test, y_pred, output_dict=True)\ndf_report = pd.DataFrame(report).transpose()\n\ndf_report.drop(['accuracy'], inplace=True)  # Drop accuracy as it's not a class-specific metric\ndf_report.plot(kind='bar', figsize=(10, 7))\nplt.title('Classification Report for BernoulliNB Model')\nplt.ylabel('Score')\nplt.show()\n\n\n\n\n\nPlease note that in order to generate a ROC curve or Precision-Recall curve, you need the probability scores or decision function, not just the predicted labels. If y_pred contains only class labels, you would need to use the predict_proba or decision_function method of your classifier to obtain these scores.\nThese code snippets are intended to be run in your local Python environment, and you’ll need to ensure that y_test and y_pred are defined in your workspace after running your BernoulliNB model. If you encounter any issues with these plots or if you require further customization, feel free to ask for additional guidance.\nConclusions: Through our analysis, we aim to provide a clear picture of the current job market and its evolving nature. Whether it’s adapting to remote work setups or understanding the necessity of on-site roles, our study sheds light on these important trends.\nFinal Thoughts: As the world of work continues to change, we’re here to keep you informed and help you understand these shifts. Stay tuned for more insights and analyses on the changing job landscape!"
  },
  {
    "objectID": "posts/Exploratory Data Analysis (EDA)/index.html",
    "href": "posts/Exploratory Data Analysis (EDA)/index.html",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Datasets Used in Our Analysis\nSurvey Overview:\nOur analysis is based on datasets primarily sourced from a survey conducted in 2023. This survey, designed and executed by our research team, targeted participants from a wide range of occupations across the USA. Its focus was on examining the significant impact of remote work on urban development and residential choices. The participants provided their insights through a detailed questionnaire, which covered aspects such as working methods (remote or in-person) and their effects on housing location preferences. This rich data offers insights into behavioral patterns related to working modes, thereby illuminating the interplay between human activity and urban spatial dynamics.\nData Modification for Privacy and Educational Use:\nIt’s important to note that the datasets featured in these blog posts have been modified and synthesized for educational purposes. They demonstrate various data analysis techniques but are not suitable for real-world applications due to these modifications. In compliance with data privacy guidelines, we have anonymized the dataset to maintain the confidentiality of the survey participants. However, these modifications do not detract from the overall integrity and analytical value of the data.\nPrimary Objective of the Dataset:\nThe main aim of this dataset is to explore the dynamics of the job market and their influence on urban landscapes. One of the key areas of inquiry in the survey was the industry sector and the preferred working method of the participants. By analyzing these elements, we gain a better understanding of current trends in the job market and how they shape urban environments.\nKey Elements of the Dataset:\nIn recent years, we’ve witnessed significant shifts in work methodologies. A growing number of professionals are embracing the Work From Home (WFH) approach, while a substantial segment continues to follow traditional Work On Site (WOS) practices. This study comprehensively covers 20 distinct industry sectors, focusing on four distinct categories of working methods. The details of these methods are outlined below:\n\n\n\n\n\n\n\n#\nWorking Method\n\n\n1\nFully remote (working from home or another location)\n\n\n2\nFully in-person (working at a physical office or location)\n\n\n3\nHybrid, dominated by in-person work (spending the majority of your work time at a physical office or location, with some remote work)\n\n\n4\nHybrid, dominated by remote work (spending the majority of your work time working from home or another location, with some in-person work)\n\n\n\nTwenty “industry sectors”or jobs are addressed based on the NAICS (North American Industry Classification System) to the 2-digit industry level. these jobs are as the following:\n\n\n\n\n\n\n\n\n\nThe number of industry sectors in NAICS\nDescription\n\n\n\n\n1\nsector 11\nAgriculture, Forestry, Fishing and Hunting\n\n\n2\nsector 21\nMining, Quarrying, and Oil and Gas Extraction\n\n\n3\nsector 22\nUtilities\n\n\n4\nsector 23\nConstruction\n\n\n5\nsector 31-33\nManufacturing\n\n\n6\nsector 42\nWholesale Trade\n\n\n7\nsector 44-45\nRetail Trade\n\n\n8\nsector 48-49\nTransportation and Warehousing\n\n\n9\nsector 51\nInformation\n\n\n10\nsector 52\nFinance and Insurance\n\n\n11\nsector 53\nReal Estate and Rental and Leasing\n\n\n12\nsector 54\nProfessional, Scientific, and Technical Services\n\n\n13\nsector 55\nManagement of Companies and Enterprises\n\n\n14\nsector 56\nAdministrative and Support and Waste Management and Remediation Services)\n\n\n15\nsector 61\nEducational Services\n\n\n16\nsector 62\nHealth Care and Social Assistance\n\n\n17\nsector 71\nArts, Entertainment, and Recreation\n\n\n18\nsector 72\nAccommodation and Food Services\n\n\n19\nsector 81\nOther Services (except Public Administration)\n\n\n20\nsector 92\nPublic Administration (not covered in economic census)\n\n\n\nEssentially, we want to see if the way people work is related to the industry they work in., Chi-Square Test of Independence is used to determine whether there is an association (relationship) between two categorical variables.\nIn our analysis, we’re testing two ideas:\n\nThe null hypothesis (H0): There’s no link between an employee’s industry sector and their working method. They’re independent of each other.\nThe alternative hypothesis (Ha): There is a significant link between the industry sector and the working method of employees.\n\n\n# Load necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import chi2_contingency\n\n# Load your dataset without specifying encoding\ndata = pd.read_excel('Industry sector and working method.xlsx')\n\n# Summary statistics for Industry sector\nindustry_stats = data[' Industry sector'].value_counts()\n\n# Summary statistics for Working Method\nmethod_stats = data['Working Method'].value_counts()\n\n# Custom labels for Working Method, with line breaks\nworking_method_labels = [\n    'Fully remote\\n(working from home or another location)',\n    'Fully in-person\\n(working at a physical office or location)',\n    'Hybrid, dominated by\\nin-person work',\n    'Hybrid, dominated by\\nremote work'\n]\n\n# Plotting with adjusted figure size and subplot parameters\nplt.figure(figsize=(14, 8))  # Further increase the figure size\n\n# Industry Sector plot\nplt.subplot(1, 2, 1)\nsns.countplot(data=data, x=' Industry sector')\nplt.title('Distribution of Industry Sectors')\n\n\n# Plotting with horizontal bar chart\nplt.figure(figsize=(6, 5))  # Adjust figure size for horizontal plot\n\n# Working Method plot (horizontal)\nsns.countplot(data=data, y='Working Method')\nplt.title('Distribution of Working Methods')\nplt.yticks(range(len(working_method_labels)), working_method_labels)  # Set y-axis labels\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nAlos, this plot is designed to visually explore the relationship between the industry sectors of employees and their working methods. It illustrates how the distribution of working methods varies across different industry sectors, allowing us to identify notable patterns or differences. Such visualizations are crucial for understanding the dynamics of the job market, particularly in the context of evolving work methodologies.\n\ncrosstab = pd.crosstab(data[' Industry sector'], data['Working Method'])\ncrosstab.plot(kind='bar', stacked=True)\n\n&lt;Axes: xlabel=' Industry sector'&gt;\n\n\n\n\n\nAdditionally, the dataset contains several key elements that will be crucial for our upcoming statistical analyses. The most important elements include:\n\n\n\nELement #\nDescription\nData Type\n\n\n1\nUnique identifier for each response\nNumerical\n\n\n2\nAbbreviation for country names\nCategorical\n\n\n3\nState or region within the country\nCategorical\n\n\n4\nGender identity of the respondent\nCategorical\n\n\n5\nAge range or specific age\nOrdinal\n\n\n6\nMarital status of the respondent\nCategorical\n\n\n7\nRace identification of the respondent\nCategorical\n\n\n8\nHighest level of education achieved\nOrdinal\n\n\n9\nNumber of days working in person\nNumerical\n\n\n10\nSpecific city or county of residence\nCategorical\n\n\n11\nSpecific city or county of work\nCategorical\n\n\n12\nAge of the company\nNumerical\n\n\n13\nNumber of employees or workers\nNumerical\n\n\n14\nDistance to work\nNumerical\n\n\n15\nHousing status\nCategorical\n\n\n16\nNumber of employed household members\nNumerical\n\n\n17\nHousehold size\nNumerical\n\n\n18\nDesired family size\nNumerical\n\n\n19\nIncome amount\nNumerical\n\n\n20\nSector of employment\nCategorical\n\n\n21\nPosition held at work\nTextual\n\n\n22\nWeekly working hours\nNumerical\n\n\n23\nWork mode (e.g., remote, in-person\nCategorical\n\n\n24\nImportance of physical interaction\nOrdinal\n\n\n25\nMode of transportation\nCategorical"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Anomaly-outlier detection DBSCAN labels for scatter plot/index.html",
    "href": "posts/Anomaly-outlier detection DBSCAN labels for scatter plot/index.html",
    "title": "Anomaly-outlier detection DBSCAN labels for scatter plot",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "About.html",
    "href": "About.html",
    "title": "About!",
    "section": "",
    "text": "Hi,\nThis is Heba Nusair, a Ph.D. Candidate in Landscape Architecture at Virginia Tech. I am passionate about promoting sustainability and resilience in cities, with expertise in modeling urban growth planning using Geospatial Information Science. I have a broad background in land analysis and evaluation, urban planning, and design using GIS, honed through years of extensive experience in the field.\n\nMy current doctoral dissertation, ‘Predicting the Impact of Shifting to Work-from-Home Paradigm on Urban Sprawl Using Agent-Based Model and Artificial Intelligence,’ demonstrates my commitment to developing innovative solutions that address complex urban challenges on a large and medium city scale.\nI’m here to share my findings with you through a series of engaging blog posts. In some of these, I’ve even used synthesized data sets to shed light on different facets of the outcomes. So, whether you’re passionate about urban planning or just curious about the future of work, let’s dive into the world of the changing nature of work and urban development."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my posts",
    "section": "",
    "text": "quarto render\ngit push\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nEDA 2\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2023\n\n\nHeba Nusair\n\n\n\n\n\n\n  \n\n\n\n\nDecoding Work Patterns, Analyzing Work From Home vs. Work On Site Trends in the 2023 US Job Market\n\n\nProbability theory and random variables, Naive Bayes classifier for multivariate Bernoulli models\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\nplot\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2023\n\n\nHeba Nusair\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly-outlier detection DBSCAN labels for scatter plot\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2023\n\n\nHeba Nusair\n\n\n\n\n\n\n  \n\n\n\n\nUnveiling the Tapestry of Urban Preferences for Workers On Site (WOS), A Data-Driven Glimpse into Roanoke’s Metropolitan Living Choices\n\n\nRandom Forest Classifier\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2023\n\n\nHeba Nusair\n\n\n\n\n\n\n  \n\n\n\n\nThe Dynamics of City Residency Preferences_ A deep dive into income_residential location choices\n\n\nLinear Regression VS. non_Linear Regression\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2023\n\n\nHeba Nusair\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nplot\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2023\n\n\nHeba Nusair\n\n\n\n\n\n\n  \n\n\n\n\nExploratory Data Analysis (EDA)\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2023\n\n\nHeba Nusair\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Unveiling the Tapestry of Urban Preferences for Workers On Site (WOS), A Data-Driven Glimpse into Roanoke’s Metropolitan Living Choices",
    "section": "",
    "text": "Deciding where to live for Workers on-Site (WOS) is a big decision for everyone, and for the people of Roanoke, it’s no different. We all have our own checklist when it comes to choosing our neighborhoods – some of us want to be close to work, others are looking for good schools for the kids, and some might prioritize a big backyard over everything else.\n\n\n\nFactors that might influence the residential decision-making process for Workers on_Site\n\n\nTo get to the heart of what matters most to Roanoke’s residents, we turned to data. Using a detailed survey filled with personal insights from locals. the datasets featured in these blog posts have been modified and synthesized for educational purposes. They demonstrate various data analysis techniques. We analyzed a range of factors from work life to family plans, all to answer one question: What drives people's choices about where they live?\nArmed with Python, a popular programming language, and a machine learning tool called Random Forest, we dug into the data. Think of Random Forest as a detective that examines all the evidence (or data) and identifies the usual suspects (or factors) that play a role in workers’ home-place choices.\nThe goal of using a Random Forest Classifier is to identify the factors that influence the residential preferences of Workers on-Site (WOS) in the Roanoke Metropolitan Area. Here are the specific objectives Random Forest helps to achieve:\n\nFeature Importance: Random Forest can determine the relative importance of each factor (like distance to work, income level, family size) in predicting residential preferences. This insight can inform urban planning and real estate development.\nPredictive Modeling: It can predict an individual’s preferred living area based on the features in the dataset, which could be useful for personalized recommendations or targeted marketing for real estate.\nHandling Complexity: Random Forest is robust to complex interactions between features and can handle non-linear relationships without the need for transformation, making it well-suited for diverse and complex datasets.\nReducing Overfitting: Due to its ensemble nature (combining multiple decision trees), Random Forest is less prone to overfitting compared to individual decision trees, leading to more reliable predictions.\nVersatility: It can handle both categorical and numerical data, making it versatile for datasets that contain a mix of different types of variables as is often the case in survey data.\nUnderstanding Population Segments: By examining which features are most influential, stakeholders can understand different segments of the population better and tailor their services or policies accordingly.\n\nIn essence, Random Forest acts as a powerful analytical tool that turns a complex array of data points into actionable insights about what drives people’s choices on where to live.\nAfter some thorough data cleaning to make sure we were working with accurate information, we let the Random Forest algorithm get to work. It looked at various details about people’s lives, including how far they live from work, their family size, and their income.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Load the dataset\ndata = pd.read_excel('Classification (In_person_Workers).xlsx')\n\n# Replace non-breaking spaces in column names\ndata.columns = data.columns.str.replace('\\xa0', ' ', regex=True)\n\n# Correct any potential typos in column names\n# Ensure these column names exactly match those in your DataFrame\ndata.rename(columns={'living costs ': 'living costs'}, inplace=True)\n\n# Convert the first four columns into a single ordinal dependent variable\n# Ensure these column names exactly match those in the DataFrame\narea_columns = ['City Center (Central Business District)', 'Urban area', 'Suburban area', 'Rural Area']\n# For each row, it finds the column among the specified area_columns that has the minimum value \n# (which would be equivalent to the highest preference rank, assuming 1 is the most preferred and \n# larger numbers indicate lower preferences) and stores the name of this column (i.e., the living area \n# with the highest preference for that row) as the value in the new Preferred_Living_Location column.\n\ndata['Preferred_Living_Location'] = data[area_columns].idxmin(axis=1)\ndata.drop(columns=area_columns, inplace=True)\n\n# Handle missing values (NaNs) for both features and target variable\ndata.dropna(inplace=True)\n\n# Define the list of categorical and continuous features\n# Replace these with the actual column names from the DataFrame\ncategorical_features = ['marital status', 'race', 'education level', 'firm age', 'firm size', 'owner or renter',\n                        'number of workers in household', 'people in household', 'desired family size in future',\n                        'monthly earning', 'industry sector']  # categorical features \ncontinuous_features = ['distance from work to homeplace']  # continuous features here\n\n# Preprocessing pipeline\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), continuous_features),\n        ('cat', OneHotEncoder(), categorical_features)\n    ])\n\n# Preprocess the data\nX = preprocessor.fit_transform(data.drop('Preferred_Living_Location', axis=1))\ny = data['Preferred_Living_Location'].astype(str)  # Convert to string if categorical\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train the Random Forest Classifier\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\n\n# Get feature importances\nfeature_names = continuous_features + list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features))\nimportances = rf.feature_importances_\nsorted_indices = np.argsort(importances)[::-1]\n\n# Display feature importances\nprint(\"Feature Importances:\")\nfor i in sorted_indices:\n    print(f'{feature_names[i]}: {importances[i]}')\n\nFeature Importances:\ndistance from work to homeplace: 0.09011235359551276\nfirm size_3.0: 0.02167504508628453\nfirm age_2.0: 0.021408873047786034\nmonthly earning_3.0: 0.021300374981779877\nmonthly earning_2.0: 0.021104791686565837\nfirm size_2.0: 0.02038107912688128\nfirm age_3.0: 0.020377672185918086\neducation level_2.0: 0.019561013195953273\nrace_1.0: 0.019160845985101583\nowner or renter_1.0: 0.018844688445501634\neducation level_5.0: 0.018743929585324837\nmarital status_2.0: 0.018732905100052072\nnumber of workers in household_1.0: 0.01864951171166948\ndesired family size in future_5.0: 0.018570765277333097\nmarital status_1.0: 0.0183554146397528\nowner or renter_2.0: 0.018027869381383396\nfirm age_5.0: 0.017849832995759533\ndesired family size in future_2.0: 0.017709778947114164\npeople in household_4.0: 0.017478550813315077\ndesired family size in future_4.0: 0.017108715601662005\nfirm age_4.0: 0.016789726724356288\npeople in household_5.0: 0.01632334099234769\nnumber of workers in household_2.0: 0.016315228925747823\ndesired family size in future_3.0: 0.01617330407211762\npeople in household_3.0: 0.01596151321993006\nmonthly earning_1.0: 0.015766641391166493\neducation level_6.0: 0.015469621796173305\nnumber of workers in household_3.0: 0.01444229789038982\nindustry sector_Wholesale Trade: 0.013968582336908847\npeople in household_2.0: 0.01377259278645549\nfirm size_5.0: 0.013767487384934402\nindustry sector_Retail Trade: 0.01358248572297112\ndesired family size in future_1.0: 0.013291628355221366\nfirm size_4.0: 0.013122940639644762\nindustry sector_Manufacturing: 0.013119516927665826\nindustry sector_Management of Companies and Enterprises: 0.012396793671151303\neducation level_3.0: 0.01231469892410354\nindustry sector_Mining, Quarrying, and Oil and Gas Extraction: 0.012111582920120987\nfirm size_1.0: 0.011913091014210377\nrace_3.0: 0.011609226312156712\neducation level_8.0: 0.011537026783190858\nindustry sector_Professional, Scientific, and Technical Services: 0.011199638021275337\nmonthly earning_4.0: 0.01052700759911541\nindustry sector_Educational Services: 0.010205036918991639\nrace_2.0: 0.010026805636023728\neducation level_7.0: 0.0098947679109884\nindustry sector_Accommodation and Food Services: 0.009840734458996685\nfirm age_1.0: 0.00948400494738133\npeople in household_1.0: 0.009419535216725582\nmarital status_3.0: 0.009228706207743796\nindustry sector_Construction: 0.009227194164520424\nrace_4.0: 0.009002694805664308\nindustry sector_Transportation and Warehousing: 0.008942336106934033\nrace_8.0: 0.008907848400038656\nindustry sector_Information: 0.008868402017550112\nmarital status_6.0: 0.008618361553154848\nrace_5.0: 0.008476644897984376\nindustry sector_Finance and Insurance: 0.008361883551020069\nrace_6.0: 0.008125868560967775\neducation level_4.0: 0.007667547913807791\nmarital status_5.0: 0.007660091743322922\nrace_7.0: 0.0074212789687077285\nindustry sector_Health Care and Social Assistance: 0.007080778976052004\nindustry sector_Other Services [except Public Administration: 0.006831959747994634\nmarital status_4.0: 0.006751730481612872\nindustry sector_Agriculture, Forestry, Fishing and Hunting: 0.006571556611527378\nindustry sector_Utilities: 0.006480906049060427\nindustry sector_Arts, Entertainment, and Recreation: 0.006359433082532256\neducation level_1.0: 0.006036337578909815\nindustry sector_Administrative and Support and Waste Management and Remediation Services: 0.004731402662589878\nindustry sector_Public Administration: 0.004650142781900239\nindustry sector_Real Estate and Rental and Leasing: 0.004496022245289562\n\n\n\n[General Interpretation:\n\nThe features with higher importance scores are more influential in determining the preferred living area. This doesn’t necessarily mean causation but indicates a stronger association.\nThe model suggests that practical considerations like distance to work, firm age, earnings, and household characteristics play a significant role in determining living area preferences.\n\n\nCaveats:\n\nThe Random Forest model treats the problem as a classification task and does not inherently account for the ordinal nature of the dependent variable.\nThe results should be interpreted as indicative rather than definitive. For more precise modeling of ordinal outcomes, specific ordinal regression models would be more appropriate.\n\n\nIn summary, the model’s output provides a useful guide to understanding which factors are most influential in determining living area preferences, with distance from work appearing as the most significant factor according to this model.]\n\nThe results were clear: For workers who commute to work; the distance from workplace to homeplace stood out as the number one factor. This wasn’t too surprising – after all, who wants to spend hours in traffic when they could be home relaxing? Income was also a key factor. It seems the more we earn, the more choices we have about where we live.\n\nBut it’s not all about money and travel time. The data also showed us that personal factors, like whether someone is married or planning for a big family, have a big say in the decision too.\nThis isn’t just interesting information. It’s valuable knowledge for city planners and developers. By understanding what’s important to residents, they can plan and build communities that better meet people’s needs and make Roanoke an even better place to live.\nSo next time you’re driving through Roanoke’s neighborhoods, remember that each home you see represents a decision made based on a unique mix of practical needs and personal dreams.\nA Visual Journey Through Roanoke’s Residential Decision Drivers\nIn our previous discussion, we delved into the factors that influence where WOS in Roanoke choose to live. We discovered that the daily commute and income levels are among the top considerations. But how do these factors stack up against each other? Let’s take a visual dive into the data.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Assuming 'importances', 'feature_names' are already defined as in the previous code\n# Sorting the feature importances\nsorted_indices = np.argsort(importances)[::-1]\nsorted_importances = importances[sorted_indices]\nsorted_features = [feature_names[i] for i in sorted_indices]\n\n# Selecting a number of top features to display (you can adjust this number)\nn_top_features = 20  # for example, top 20 features\n\n# Creating the plot\nplt.figure(figsize=(6.5, 6.5))\nsns.barplot(x=sorted_importances[:n_top_features], y=sorted_features[:n_top_features])\nplt.title('Significance of factors in residential decision-making process')\nplt.xlabel('Level of importance')\nplt.ylabel('Factors')\nplt.show()\n\n\n\n\nThe accompanying chart lays out the findings from our Random Forest analysis in a colorful display of bars, each representing a factor’s weight in the decision-making process. At a glance, the lengthier bars catch the eye—these are the heavy hitters of home location preferece.\nDominating the scene is ‘distance from work to homeplace,’ a beacon of red at the top, which resonates with anyone who’s ever been caught in the snarl of rush hour. This is followed closely by a cascade of oranges and yellows highlighting the importance of ‘monthly earnings’ across various brackets and the age and size of firms.\nAs we move down the chart, the colors shift to cooler greens and blues, representing factors like household size and marital status. These may not be the titans of the plot, but they still play a pivotal role in the collective narrative of where we live.\nWhat’s truly intriguing is the story this visual tells us—beyond numbers and models, it’s about real-life choices and priorities. For instance, the significance of ‘people in household_4.0’ and ‘education level_5.0’ (a bachelor’s degree) speaks to the aspirations and day-to-day realities that shape our sense of place.\nCity planners and housing developers, take note: this chart is more than just a pretty picture. It’s a roadmap to the hearts and minds of Roanoke’s residents, guiding the way to communities that not only house but also support and enrich their lives.\nAs we continue to shape Roanoke’s urban landscape, let’s keep these visual cues in mind, ensuring that every development, from high-rises to townhomes, aligns with the genuine preferences of those who will call them home. After all, understanding these preferences is the key to building not just houses but homes where life’s stories can unfold."
  },
  {
    "objectID": "posts/CopyOfExploratory Data Analysis (EDA)2/index.html",
    "href": "posts/CopyOfExploratory Data Analysis (EDA)2/index.html",
    "title": "EDA 2",
    "section": "",
    "text": "The study and the attained dataset mainly focus on exploring the dynamic of the job market and its impact on the urban cityscape. Recent years marked a notable transformation in work methodologies. Many professionals are adopting the Work From Home (WFH) model, while many still adhere to traditional Work On Site (WOS) practices. My study delves into 20 distinct industry sectors, examining four unique classes of working methods and styles, as detailed below:\n\n\n\n1\nWorking On-Site [WOS]\n\n\n2\nWork From Home [WFH]\n\n\n\nThe data comes from a 2023 survey of 850 participants from various occupations. Participants were asked about their industry sector and working method. To respect data confidentiality, we’ve modified and anonymized the dataset. This ensures compliance with data privacy guidelines without affecting the overall analysis and insights.\n\n# Load necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nStudy the relationship between nominal variables (Industry sector and Working Method)statistical analyses:\n1- Chi-Square Test of Independence: The Chi-Square test can be used to determine whether there is an association (relationship) between two categorical variables. In your case, you can use it to test if there is a relationship between Industry sector and Working Method. The null hypothesis (H0) is that the two variables are independent, and the alternative hypothesis (Ha) is that there is a significant association between them.\n\n# Load necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import chi2_contingency\n\n2-Visualization: You can create visualizations to explore the relationship visually. For example, you can create a stacked bar chart to see how the distribution of Working Method varies across different Industry sectors. This can help you identify patterns or differences."
  },
  {
    "objectID": "posts/Linear and nonlinear regression/index.html",
    "href": "posts/Linear and nonlinear regression/index.html",
    "title": "The Dynamics of City Residency Preferences_ A deep dive into income_residential location choices",
    "section": "",
    "text": "Introduction\nIn the dynamic realm of urban living choices, understanding the factors influencing our habitat preferences is crucial. We embarked on a data-driven journey to explore one such factor – income. Our goal was simple yet profound: to discern how variations in income affect preferences for living areas. For this, we turned to the ever-reliable tools of machine learning, specifically linear regression.\n\n\nThe Study at a Glance\nOur dataset encapsulated responses from individuals, detailing their income levels and preferred living areas – be it the bustling city center, the vibrant urban zones, the serene suburban areas, or the tranquil rural landscapes. Our approach was straightforward:\n\nWe transformed income into a continuous scale, assigning monetary values to income brackets.\nWe converted living area preferences into a single, ordinal dependent variable.\nWe employed linear regression to model the relationship between these two variables.\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndata = pd.read_excel('Income_LivingLocationPrefrences.xlsx')\n\n# Handle NaN values\ndata.dropna(subset=['City_center', 'Urban_area', 'Suburban_area', 'Rural_area', 'Monthly_income'], inplace=True)\n\n# Convert the area preferences into a single ordinal dependent variable\narea_to_number = {'City_center': 1, 'Urban_area': 2, 'Suburban_area': 3, 'Rural_area': 4}\ndata['living_area_preference'] = data[['City_center', 'Urban_area', 'Suburban_area', 'Rural_area']].idxmin(axis=1).map(area_to_number)\n\n# Convert income to a continuous scale based on the provided income brackets\nincome_mapping = {1.0: 625, 2.0: 2292, 3.0: 5000, 4.0: 6666}\ndata['continuous_income'] = data['Monthly_income'].map(income_mapping)\n\n# Scale the income feature\nscaler = StandardScaler()\ndata['scaled_income'] = scaler.fit_transform(data[['continuous_income']])\n\n# Prepare the features and target variable for modeling\nX = data[['scaled_income']]\ny = data['living_area_preference']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train the linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Predict on the test data\ny_pred = model.predict(X_test)\n\n# Evaluate the model's performance\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Output the performance metrics\nprint(f'Mean Squared Error: {mse}')\nprint(f'R-squared: {r2}')\n\n# Debugging the sizes of arrays\nprint(\"Sizes of arrays for plotting:\")\nprint(\"X_test['scaled_income']: \", len(X_test['scaled_income']))\nprint(\"y_test: \", len(y_test))\nprint(\"y_pred: \", len(y_pred))\n\n# Plotting\nplt.scatter(X_test['scaled_income'].values, y_test.values, color='black', label='Actual Data')\nplt.scatter(X_test['scaled_income'].values, y_pred, color='red', label='Predicted Data', alpha=0.5)\n\n# Optionally, create a more continuous line for predictions\nsorted_order = np.argsort(X_test['scaled_income'].values)\nplt.plot(X_test['scaled_income'].values[sorted_order], y_pred[sorted_order], color='blue', linewidth=2, label='Regression Line')\n\nplt.xlabel('Monthly income')\nplt.ylabel('Living Area Preference')\nplt.title('Income vs Living Area Preference Linear Regression')\nplt.legend()\nplt.show()\n\nMean Squared Error: 1.2658068009077692\nR-squared: 0.006851215790502296\nSizes of arrays for plotting:\nX_test['scaled_income']:  162\ny_test:  162\ny_pred:  162\n\n\n\n\n\n\n\nUnveiling the Results\nThe outcomes of our analysis were both intriguing and enlightening:\n\nMean Squared Error (MSE): The MSE stood at 1.2229590696534407. This figure, in isolation, hinted at some variance between the predicted and actual preferences. However, this is only one piece of the puzzle.\nR-squared Value: Here’s where the plot thickens. The R-squared value, at -0.0034801764611527286, raised eyebrows. This negative value suggested that the linear model failed to capture the essence of the relationship between income and living area preference. What does this mean? Simply put, the linear regression model we deployed might not be the right fit for this specific analysis.\n\n\n\nInterpreting the Outcome\nThis unexpected twist in our narrative compels us to introspect and question. Here are a few takeaways and considerations:\n\nNon-linear Relationships: Perhaps the bond between income and living area preference isn’t a straight line but a curve or a more complex shape. Linear regression, with its assumption of linearity, might not be adept at capturing such nuances.\nMissing Puzzle Pieces: It’s plausible that other influential factors, not included in our current model, play a significant role in shaping living area preferences.\nData Reflection: The integrity and representativeness of our data demand scrutiny. Are there outliers skewing our results? Have we translated income brackets into continuous variables accurately?\n\n\n\nMoving Forward: A Path of Exploration\n\n\nIntroducing the Concept of Non-Linear Relationships\nIn our exploration of how income influences living area preferences, a linear model initially seemed like a straightforward choice. However, real-world data often tell a more complex story. Our linear model, with a surprisingly negative R-squared value, hinted at this complexity. It suggests that the relationship between income and living area preference might not be a straight line, but rather a curve or a more intricate pattern.\nLinear regression, with its assumption of a straight-line relationship between variables, is ill-equipped to capture these nuances. This is where the idea of non-linear relationships comes into play. Such relationships are not confined to straight lines and can take various forms, like curves, which are more adept at mapping the intricate patterns often found in real-world data.\n\n\nImplementing Polynomial Regression\nTo explore these potential non-linear patterns, we turn to polynomial regression. This technique extends linear regression by adding polynomial terms (such as squared, cubed values) of the predictor variables. By doing so, it allows the model to bend and curve, fitting more complex relationships than a straight line ever could.\nImplementing polynomial regression is a straightforward extension of our current linear model. We use PolynomialFeatures from sklearn.preprocessing to transform our income data, introducing these higher-order terms. By fitting our model with these transformed features, we can capture the potential non-linear relationship between income and living area preference more effectively.\n\n# Since the last execution to verify the column names did not work, let's proceed with the updated code\n# for polynomial regression including the additional variable \"Distance from home to work\" and the plot.\n\n# I will assume the column names provided are correct and update the code accordingly.\n\n# Full code with the additional independent variable and plotting the regression curve\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndata_path = 'Income_LivingLocationPrefrences.xlsx'\ndata = pd.read_excel(data_path)\n\n# Handle NaN values\ndata.dropna(subset=['City_center', 'Urban_area', 'Suburban_area', 'Rural_area', 'Monthly_income', 'Distance from home to work'], inplace=True)\n\n# Create a new column 'living_area_preference' as the average of the Likert-scale responses\ndata['living_area_preference'] = data[['City_center', 'Urban_area', 'Suburban_area', 'Rural_area']].mean(axis=1)\n\n# Convert income to a continuous scale based on the provided income brackets\nincome_mapping = {1.0: 625, 2.0: 2292, 3.0: 5000, 4.0: 6666}\ndata['continuous_income'] = data['Monthly_income'].map(income_mapping)\n\n# Scale the income feature and the 'Distance from home to work' feature\nscaler = StandardScaler()\ndata[['scaled_income', 'scaled_distance']] = scaler.fit_transform(data[['Monthly_income', 'Distance from home to work']])\n\n# Create polynomial features including the new variable\nX = data[['scaled_income', 'scaled_distance']]\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_poly, data['living_area_preference'], test_size=0.2, random_state=42)\n\n# Polynomial Regression Model\npoly_model = LinearRegression()\npoly_model.fit(X_train, y_train)\ny_pred_poly = poly_model.predict(X_test)\nmse_poly = mean_squared_error(y_test, y_pred_poly)\nr2_poly = r2_score(y_test, y_pred_poly)\n\n# Output the results for Polynomial Regression Model\nprint(\"Polynomial Regression Model with Distance Variable:\")\nprint(f\"Mean Squared Error: {mse_poly}\")\nprint(f\"R-squared: {r2_poly}\")\n\n# Since we have two independent variables, we can't simply plot a 2D curve.\n# Instead, we'll plot the actual vs predicted values for one of the variables (scaled_income).\n\n# Scatter plot of actual vs predicted values\nplt.scatter(X_test[:, 1], y_test, alpha=0.5, color='blue', label='Actual')  # Scaled income for x-axis\nplt.scatter(X_test[:, 1], y_pred_poly, alpha=0.5, color='red', label='Predicted')\n\n# Adding plot labels and title\nplt.xlabel('Scaled Monthly Income')\nplt.ylabel('Predicted Living Area Preference')\nplt.title('Polynomial Regression with Distance Variable: Actual vs Predicted Values')\nplt.legend()\n\n# Show the plot\nplt.show()\n\nPolynomial Regression Model with Distance Variable:\nMean Squared Error: 0.2160332716401098\nR-squared: 0.01142909612703713\n\n\n\n\n\n\n\nComparison of Linear vs. Polynomial Regression Models\nIn our exploration of the relationship between income and living area preference, we analyzed two different models: Linear Regression and Polynomial Regression. The outcomes offer intriguing insights:\n\nLinear Regression Model:\n\nInitially, we employed a Linear Regression model, which yielded a Mean Squared Error (MSE) of 1.2229590696534407. This suggested some variance between the predicted and actual preferences.\nHowever, the R-squared value was -0.0034801764611527286, indicating a poor fit. This negative value implies that the linear model might not be suitable for capturing the relationship in our analysis.\n\nPolynomial Regression Model:\n\nSeeking improvement, we then applied a Polynomial Regression model. It showed a lower MSE of 0.12978886986451396, suggesting a closer fit to the data compared to the Linear Regression model.\nThe R-squared value improved to 0.012140786799389347. While still low, it’s a positive value, indicating a slight improvement in explaining the variability of the dependent variable.\n\n\n\n\nComparative Table\n\n\n\n\n\n\n\n\nModel Type\nMean Squared Error (MSE)\nR-squared Value\n\n\n\n\nLinear Regression\n1.2229590696534407\n-0.0034801764611527286\n\n\nPolynomial Regression\n0.12978886986451396\n0.012140786799389347\n\n\n\n\n\nInterpretation and Conclusion\n\nModel Fit: The Polynomial Regression model shows a lower MSE, indicating a better fit to our data compared to the Linear Regression model.\nModel’s Explanatory Power: Despite the improvement in R-squared value in the Polynomial Regression model, it remains low, suggesting that additional factors might influence living area preferences beyond what is captured by income alone.\nNext Steps: These results highlight the need to explore more complex models or additional variables that could provide a deeper understanding of the factors influencing living area preferences.\n\nThis analysis underscores the importance of choosing the right model and considering multiple factors in predictive analytics, especially in complex real-world scenarios."
  },
  {
    "objectID": "Research Insights Explorer Using Machine learning.html",
    "href": "Research Insights Explorer Using Machine learning.html",
    "title": "Research Insights Explorer Using Machine learning",
    "section": "",
    "text": "Important Note: The datasets used on these posts are modified and synthesized, so they should be used solely for educational purposes and to demonstrate data analysis techniques. Please refrain from using this data for any genuine applications.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nEDA 2\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nHeba Nusair\n\n\nNov 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecoding Work Patterns, Analyzing Work From Home vs. Work On Site Trends in the 2023 US Job Market\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\nplot\n\n\n\n\n\n\n\nHeba Nusair\n\n\nNov 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnomaly-outlier detection DBSCAN labels for scatter plot\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nHeba Nusair\n\n\nNov 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnveiling the Tapestry of Urban Preferences for Workers On Site (WOS), A Data-Driven Glimpse into Roanoke’s Metropolitan Living Choices\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nHeba Nusair\n\n\nNov 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Dynamics of City Residency Preferences_ A deep dive into income_residential location choices\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nHeba Nusair\n\n\nNov 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nClustering\n\n\n\nnews\n\n\ncode\n\n\nplot\n\n\n\n\n\n\n\nHeba Nusair\n\n\nNov 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis (EDA)\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nHeba Nusair\n\n\nNov 3, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  }
]