[
  {
    "objectID": "Research Overview.html",
    "href": "Research Overview.html",
    "title": "Research Overview",
    "section": "",
    "text": "Work From Home and Cityscape\nExperience the transformative shift in work dynamics worldwide. Fueled by factors like flexibility and propelled by the seismic impact of the Covid-19 pandemic, remote work, particularly Working From Home (WFH), has become a prevailing trend. This change in work culture is not merely a temporary response but a catalyst for a prolonged paradigm shift in workplaces globally.\nAs the emphasis on proximity to the workplace diminishes, individuals are opting for relocations to suburban areas, seeking affordable housing options without compromising living standards. This evolution in the work landscape intertwines with broader implications for cities—altering socioeconomic structures, reshaping urban landscapes, and influencing environmental dynamics.\n\nJoin us on this exploration of a dynamic era where work and living spaces converge, uncovering the profound effects of this transformative journey on cities, society, and the natural environment. Welcome to a new era in the world of work and living."
  },
  {
    "objectID": "posts/Probability theory and random variables/index.html",
    "href": "posts/Probability theory and random variables/index.html",
    "title": "Predicting Work Trends Across Industries with Naive Bayes: Remote or On-Site",
    "section": "",
    "text": "Introduction:\nAs we navigate through the evolving landscape of the 2023 job market, we are witnessing a significant shift in work methods. More people are embracing Work From Home (WFH) while others continue with Work On Site (WOS). This post helps to understand how these work methods vary across different jobs in the USA.\n\n\nMy Approach:\nFor this analysis, I’ve utilized the Naive Bayes classifier for multivariate Bernoulli models. This method is excellent for sorting jobs into two main categories: WFH and WOS. It’s a practical choice for my study because it efficiently handles binary data—like choosing between remote or on-site work.\n\n\nAbout the Data:\nThe data comes from a 2023 survey of 850 participants from various occupations. Participants were asked about their industry sector and working method. To respect data confidentiality, we’ve modified and anonymized the dataset. This ensures compliance with data privacy guidelines without affecting the overall analysis and insights.\n\n\n\nThe survey participants from workers and their working method (WFH, or WON).\n\n\n\n\nIndustry Breakdown:\nThe study categorized 20 main category for jobs according to the North American Industry Classification System (NAICS), aligning with the 6-digit Standard Occupational Classification (SOC) system from the U.S. Bureau of Labor Statistics (BLS). Here’s a table of these industry sectors:\n\n\n\n\n\n\n\n\n#\nThe occupations categories are based on the 6-digit (SOC) system from the U.S. (BLS)\nIndustry Sector: Occupational Classification\n\n\n\n\n1\nNumber of jobs in NAICS sector 11\n(Agriculture, Forestry, Fishing and Hunting)\n\n\n2\ncns02 Number of jobs in NAICS sector 21\n(Mining, Quarrying, and Oil and Gas Extraction)\n\n\n3\ncns03 Number of jobs in NAICS sector 22\n(Utilities)\n\n\n4\ncns04 Number of jobs in NAICS sector 23\n(Construction)\n\n\n5\ncns05 Number of jobs in NAICS sector 31‐33\n(Manufacturing)\n\n\n6\ncns06 Number of jobs in NAICS sector 42\n(Wholesale Trade)\n\n\n7\ncns07 Number of jobs in NAICS sector 44‐45\n(Retail Trade)\n\n\n8\ncns08 Number of jobs in NAICS sector 48‐49\n(Transportation and Warehousing)\n\n\n9\ncns09 Number of jobs in NAICS sector 51\n(Information)\n\n\n10\ncns10 Number of jobs in NAICS sector 52\n(Finance and Insurance)\n\n\n11\ncns11 Number of jobs in NAICS sector 53\n(Real Estate and Rental and Leasing)\n\n\n12\ncns12 Number of jobs in NAICS sector 54\n(Professional, Scientific, and Technical Services)\n\n\n13\ncns13 Number of jobs in NAICS sector 55\n(Management of Companies and Enterprises)\n\n\n14\ncns14 Number of jobs in NAICS sector 56\n(Administrative and Support and Waste Management and Remediation Services)\n\n\n15\ncns15 Number of jobs in NAICS sector 61\n(Educational Services)\n\n\n16\ncns16 Number of jobs in NAICS sector 62\n(Health Care and Social Assistance)\n\n\n17\ncns17 Number of jobs in NAICS sector 71\n(Arts, Entertainment, and Recreation)\n\n\n18\ncns18 Number of jobs in NAICS sector 72\n(Accommodation and Food Services)\n\n\n19\ncns19 Number of jobs in NAICS sector 81\n(Other Services [except Public Administration])\n\n\n20\ncns20 Number of jobs in NAICS sector 92\n(Public Administration)\n\n\n\nGoal: My goal is to provide a clear and real picture of the changing work styles. I am not just analyzing numbers; I am uncovering the stories they tell about how people are adapting to new work environments, be it remotely or on-site.\n\n\nWhy I Chose the Naive Bayes Classifier for Multivariate Bernoulli Models:\nIn my exploration of work methods—remote and on-site—the Naive Bayes classifier for multivariate Bernoulli models shines with its simplicity and effectiveness. It excels at evaluating probabilities, helping determine whether a job is more likely to be remote or on-site based on its industry sector. This algorithm’s strength lies in its straightforward approach, providing clear baseline assessments that guide our analysis across different industry sectors. Dive Into the Code Behind the Analysis:\nx: The industry sector: [jobs categories From 1 to 20]\ny: The work method: 1 for WOS [Work on Site], 2 for WFH [Work From Home]\n\n\n\nCode\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.preprocessing import OneHotEncoder\nimport os\n\n# Load the dataset\ndf = pd.read_csv('C:/Users/NUSAI/Desktop/Machine learning/HebaNu.github.io/HebaNu.github.io/HebaNu/posts/Probability theory and random variables/updated_POST1.csv')\n\n# Drop rows where any cell is NaN in the 'Work Method' column\ndf = df.dropna(subset=['Work Method'])\n\n# One-hot encoding\nencoder = OneHotEncoder(sparse=False)\nX = encoder.fit_transform(df[[' industry sector']])  \n# includes 20 occupation types from NAICS for 6-digit SOC- U.S. BLS\n\n# Target variable\ny = df['Work Method']\n# 1 for WOS [Work on Site], 2 for WFH [Work From Home]\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train the Bernoulli Naive Bayes model\nmodel = BernoulliNB()\nmodel.fit(X_train, y_train)\n\n# Make predictions and evaluate the model\ny_pred = model.predict(X_test)\n\n\n\n# Output the classification report and accuracy\nprint(classification_report(y_test, y_pred, zero_division=0))\nprint(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n\n\n              precision    recall  f1-score   support\n\n         1.0       0.00      0.00      0.00        40\n         2.0       0.76      1.00      0.87       130\n\n    accuracy                           0.76       170\n   macro avg       0.38      0.50      0.43       170\nweighted avg       0.58      0.76      0.66       170\n\nAccuracy: 0.7647058823529411\n\n\nC:\\Users\\NUSAI\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning:\n\n`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n\n\n\n\n\nInsights from Our Model: Understanding the Work Trends\nThe results from our BernoulliNB model offer fascinating insights into the current work trends. The model is predicting class 2.0 for WFH [Work From Home] with high precision (76%) and recall (100%), which means for this class, it performs well both in terms of the accuracy of the positive predictions it makes (precision) and its ability to find all the positive instances (recall).\nFor class 1.0, however, the model does not predict any instances correctly, which suggests that either there’s an issue with the distribution of your classes (perhaps class 1.0 is underrepresented), or that the features do not provide enough information to distinguish class 1.0 from class 2.0. The overall accuracy of the model is 76.47%, which means that it correctly predicts the class for 76.47% of the test set.\nThe macro avg and weighted avg for precision, recall, and f1-score provide a summary of the effectiveness of the model across the classes. The low macro avg for precision and f1-score indicates that one of the classes does not perform well, which we already know is class 1.0.\nThe f1-score is a harmonic mean of precision and recall and is a useful metric when you have classes that are imbalanced. In my case, the f1-score for class 1.0 [WOS] is 0.00, indicating poor performance for this class.\nGenerally, the Naive Bayes classifier for multivariate Bernoulli models proves to be an effective and reliable tool for classification tasks. Its ability to handle binary data, like our work method categories, contributes to its robustness and applicability in various analytical scenarios.\nTo create plots that visualize the performance of your BernoulliNB model, you would typically look at the confusion matrix, precision-recall, and possibly ROC curves. Below are examples of how you could generate each of these plots using matplotlib and scikit-learn:\n\nConfusion Matrix: Visualizes the correct and incorrect predictions compared to the actual values.\n\n\n\nCode\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming y_test and y_pred are already defined from BernoulliNB model\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.title('Confusion Matrix for BernoulliNB Model')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n\n\n\n\n\nROC Curve: Plots the true positive rate against the false positive rate.\n\n\nCode\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import RocCurveDisplay\nimport matplotlib.pyplot as plt\n\n# Adjust y_test to have binary labels 0 and 1\ny_test_binary = y_test.replace({1: 0, 2: 1})\n\n# Get predicted probabilities for the positive class (e.g., class 2)\ny_pred_prob = model.predict_proba(X_test)[:, 1]  # Index 1 for the probability of class 2\n\n# Calculate the ROC curve\nfpr, tpr, thresholds = roc_curve(y_test_binary, y_pred_prob)\n\n# Calculate the AUC (Area Under Curve)\nroc_auc = auc(fpr, tpr)\n\n# Plot the ROC curve\ndisp = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\ndisp.plot()\nplt.title('ROC Curve for BernoulliNB Model')\nplt.show()\n\n\n\n\n\n\n\nConclusions:\nThrough our analysis, we aim to provide a clear picture of the current job market and its evolving nature. Whether it’s adapting to remote work setups or understanding the necessity of on-site roles, our study sheds light on these important trends.\n\n\nFinal Thoughts:\nAs the world of work continues to change, we’re here to keep you informed and help you understand these shifts. Stay tuned for more insights and analyses on the changing job landscape!"
  },
  {
    "objectID": "posts/Exploratory Data Analysis (EDA)/index.html",
    "href": "posts/Exploratory Data Analysis (EDA)/index.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Datasets Used in Our Analysis\n\nSurvey Overview:\nOur analysis is based on datasets primarily sourced from a survey conducted in 2023. This survey, designed and executed by our research team, targeted participants from a wide range of occupations across the USA. Its focus was on examining the significant impact of remote work on urban development and residential choices. The participants provided their insights through a detailed questionnaire, which covered aspects such as working methods (remote or in-person) and their effects on housing location preferences. This rich data offers insights into behavioral patterns related to working modes, thereby illuminating the interplay between human activity and urban spatial dynamics.\n\n\nData Modification for Privacy and Educational Use:\nIt’s important to note that the datasets featured in these blog posts have been modified and synthesized for educational purposes. They demonstrate various data analysis techniques but are not suitable for real-world applications due to these modifications. In compliance with data privacy guidelines, we have anonymized the dataset to maintain the confidentiality of the survey participants. However, these modifications do not detract from the overall integrity and analytical value of the data.\n\n\nPrimary Objective of the Dataset:\nThe main aim of this dataset is to explore the dynamics of the job market and their influence on urban landscapes. One of the key areas of inquiry in the survey was the industry sector and the preferred working method of the participants. By analyzing these elements, we gain a better understanding of current trends in the job market and how they shape urban environments.\n\n\nKey Elements of the Dataset:\nIn recent years, we’ve witnessed significant shifts in work methodologies. A growing number of professionals are embracing the Work From Home (WFH) approach, while a substantial segment continues to follow traditional Work On Site (WOS) practices. This study comprehensively covers 20 distinct industry sectors, focusing on four distinct categories of working methods. The details of these methods are outlined below:\n\n\n\n\n\n\n\n#\nWorking Method\n\n\n1\nFully remote (working from home or another location)\n\n\n2\nFully in-person (working at a physical office or location)\n\n\n3\nHybrid, dominated by in-person work (spending the majority of your work time at a physical office or location, with some remote work)\n\n\n4\nHybrid, dominated by remote work (spending the majority of your work time working from home or another location, with some in-person work)\n\n\n\nTwenty “industry sectors”or jobs are addressed based on the NAICS (North American Industry Classification System) to the 2-digit industry level. these jobs are as the following:\n\n\n\n\n\n\n\n\n\nThe number of industry sectors in NAICS\nDescription\n\n\n\n\n1\nsector 11\nAgriculture, Forestry, Fishing and Hunting\n\n\n2\nsector 21\nMining, Quarrying, and Oil and Gas Extraction\n\n\n3\nsector 22\nUtilities\n\n\n4\nsector 23\nConstruction\n\n\n5\nsector 31-33\nManufacturing\n\n\n6\nsector 42\nWholesale Trade\n\n\n7\nsector 44-45\nRetail Trade\n\n\n8\nsector 48-49\nTransportation and Warehousing\n\n\n9\nsector 51\nInformation\n\n\n10\nsector 52\nFinance and Insurance\n\n\n11\nsector 53\nReal Estate and Rental and Leasing\n\n\n12\nsector 54\nProfessional, Scientific, and Technical Services\n\n\n13\nsector 55\nManagement of Companies and Enterprises\n\n\n14\nsector 56\nAdministrative and Support and Waste Management and Remediation Services)\n\n\n15\nsector 61\nEducational Services\n\n\n16\nsector 62\nHealth Care and Social Assistance\n\n\n17\nsector 71\nArts, Entertainment, and Recreation\n\n\n18\nsector 72\nAccommodation and Food Services\n\n\n19\nsector 81\nOther Services (except Public Administration)\n\n\n20\nsector 92\nPublic Administration (not covered in economic census)\n\n\n\nEssentially, we want to see if the way people work is related to the industry they work in., Chi-Square Test of Independence is used to determine whether there is an association (relationship) between two categorical variables.\nIn our analysis, we’re testing two ideas:\n\nThe null hypothesis (H0): There’s no link between an employee’s industry sector and their working method. They’re independent of each other.\nThe alternative hypothesis (Ha): There is a significant link between the industry sector and the working method of employees.\n\n\n\nCode\n# Load necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import chi2_contingency\n\n# Load your dataset without specifying encoding\ndata = pd.read_excel('Industry sector and working method.xlsx')\n\n# Summary statistics for Industry sector\nindustry_stats = data[' Industry sector'].value_counts()\n\n# Summary statistics for Working Method\nmethod_stats = data['Working Method'].value_counts()\n\n# Custom labels for Working Method, with line breaks\nworking_method_labels = [\n    'Fully remote\\n(working from home or another location)',\n    'Fully in-person\\n(working at a physical office or location)',\n    'Hybrid, dominated by\\nin-person work',\n    'Hybrid, dominated by\\nremote work'\n]\n\n# Plotting with adjusted figure size and subplot parameters\nplt.figure(figsize=(14, 8))  # Further increase the figure size\n\n# Industry Sector plot\nplt.subplot(1, 2, 1)\nsns.countplot(data=data, x=' Industry sector')\nplt.title('Distribution of Industry Sectors')\n\n\n# Plotting with horizontal bar chart\nplt.figure(figsize=(6, 5))  # Adjust figure size for horizontal plot\n\n# Working Method plot (horizontal)\nsns.countplot(data=data, y='Working Method')\nplt.title('Distribution of Working Methods')\nplt.yticks(range(len(working_method_labels)), working_method_labels)  # Set y-axis labels\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAlos, this plot is designed to visually explore the relationship between the industry sectors of employees and their working methods. It illustrates how the distribution of working methods varies across different industry sectors, allowing us to identify notable patterns or differences. Such visualizations are crucial for understanding the dynamics of the job market, particularly in the context of evolving work methodologies.\n\n\nCode\ncrosstab = pd.crosstab(data[' Industry sector'], data['Working Method'])\ncrosstab.plot(kind='bar', stacked=True)\n\n\n&lt;Axes: xlabel=' Industry sector'&gt;\n\n\n\n\n\nAdditionally, the dataset contains several key elements that will be crucial for our upcoming statistical analyses. The most important elements include:\n\n\n\nELement #\nDescription\nData Type\n\n\n1\nUnique identifier for each response\nNumerical\n\n\n2\nAbbreviation for country names\nCategorical\n\n\n3\nState or region within the country\nCategorical\n\n\n4\nGender identity of the respondent\nCategorical\n\n\n5\nAge range or specific age\nOrdinal\n\n\n6\nMarital status of the respondent\nCategorical\n\n\n7\nRace identification of the respondent\nCategorical\n\n\n8\nHighest level of education achieved\nOrdinal\n\n\n9\nNumber of days working in person\nNumerical\n\n\n10\nSpecific city or county of residence\nCategorical\n\n\n11\nSpecific city or county of work\nCategorical\n\n\n12\nAge of the company\nNumerical\n\n\n13\nNumber of employees or workers\nNumerical\n\n\n14\nDistance to work\nNumerical\n\n\n15\nHousing status\nCategorical\n\n\n16\nNumber of employed household members\nNumerical\n\n\n17\nHousehold size\nNumerical\n\n\n18\nDesired family size\nNumerical\n\n\n19\nIncome amount\nNumerical\n\n\n20\nSector of employment\nCategorical\n\n\n21\nPosition held at work\nTextual\n\n\n22\nWeekly working hours\nNumerical\n\n\n23\nWork mode (e.g., remote, in-person\nCategorical\n\n\n24\nImportance of physical interaction\nOrdinal\n\n\n25\nMode of transportation\nCategorical"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Urban Clustering: Decoding Roanoke and Salem’s Living Patterns",
    "section": "",
    "text": "Introduction:\nIn the bustling urban landscapes and the serene outskirts of Roanoke and Salem cities, every homeplace tells a story. Geographic clustering, a powerful analytical tool, allows us to uncover these stories by revealing the invisible patterns of human settlement and organization within these spaces. This analysis goes beyond mere numbers and dots on a map; it provides us with the insight to understand how populations distribute themselves across regions and how this distribution may affect and be affected by socio-economic factors.\n\n\n\nA Tapestry of Settlement: Over 55,000 points represent workers’ homeplaces scattered across the Roanoke and Salem cities, Painting a vivid picture of the urban landscape. Each point is a nexus of life and activity, contributing to the rich pattern of local habitation.\n\n\nIn this exploration, we dive into the spatial heart of Roanoke and Salem, employing a data-driven approach to demystify the geographic distribution and density of home locations. By utilizing the K-Means clustering technique, we aim to transcend the traditional narratives of urban planning and offer a unique lens through which we can comprehend the dynamics of these cities.\nJoin me on this cartographic journey as we navigate through the coordinates, interpret the clusters, and stitch together the fabric of these communities, one cluster at a time.\n\n\nData Description:\nThis dataset, retrieved from the web-based mapping tool OnTheMap, reveals the intricate interplay between workers’ employment locations and their residences. Carefully processed through Geographic Information Systems (GIS), the data presents a detailed residential fabric of 55,494 workers’ homeplaces in Roanoke and Salem cities. Each data point is a precise coordinate, marking the longitude (X) and latitude (Y) of individual dwellings. Together, they form an urban mosaic that not only maps out dense habitational clusters but also embodies the vibrant core of these communities, inviting us to explore the complex narrative of local settlement patterns.\n\n\nMethodology:\n\nChoosing the Right Tool: We employed K-Means clustering, a method perfect for drawing virtual boundaries around homes that are close together. It’s like sketching out neighborhoods based on where people live.\nData Standardization: Before clustering, we standardized the home coordinates. This ensures that every home, regardless of its actual geographical position, is treated equally in our analysis.\nFocused Analysis: We consciously left out job types from our study. Why? To focus solely on where people live. This allowed us to paint a clear picture of the residential layouts in Roanoke and Salem, unclouded by other factors.\n\n\n\nCode\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('Homeplaces in Roanoke and Salem Cities.csv')\n\n# Select only the geographic coordinates\ndf_geo = df[['X', 'Y']]\n\n# Standardizing the features (important for K-Means)\nscaler = StandardScaler()\ndf_geo_scaled = scaler.fit_transform(df_geo)\n\n# KMeans clustering\nkmeans = KMeans(n_clusters=5, n_init=10)\nclusters = kmeans.fit_predict(df_geo_scaled)\n\n# Adding cluster labels to dataframe\ndf['Cluster'] = clusters\n\n# Plotting the clusters\nplt.figure(figsize=(10, 6))\nplt.scatter(df['X'], df['Y'], c=df['Cluster'], cmap='viridis', marker='o')\n\nplt.title('Geographic Distribution of Homeplaces in Roanoke and Salem Cities')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.colorbar(label='Cluster Label')\nplt.show()\n\n\n\n\n\nWhat we get is a straightforward, unfiltered view of how communities in these cities are shaped – a true representation of the area’s residential dynamics.\n\n\nDissecting Roanoke and Salem’s Residential Layout\nIn the visual analysis that unfolded, we used a scatter plot to translate the K-Means clustering algorithm’s output into a vibrant map. The algorithm diligently partitioned the area into five distinct clusters, each represented by a unique color—ranging from deep purple to bright yellow. The resulting plot is a testament to the power of data-driven geographical analysis.\nUpon examining the plot, the clusters emerge as distinct groupings that correspond to different areas within Roanoke and Salem. Some clusters are tightly packed, indicating neighborhoods with high home density, while others are more spread out, suggesting less populated or more spacious living areas. The color coding not only adds visual appeal but also signifies the cluster each home belongs to, with the color bar on the right serving as a handy reference.\nA closer look at the distribution of these clusters may reveal insights into the urban planning and development patterns of the cities. For example, we might observe that certain colors (clusters) concentrate around city centers or major transport routes, suggesting a correlation between home location and accessibility. On the other hand, some clusters could be outlining the suburban and peri-urban spread, hinting at the expansion of the cities’ residential zones.\nThese observations provide a starting point for urban analysts, policymakers, and planners to dive deeper into the factors driving such settlement patterns, potentially influencing future development and infrastructure planning to better accommodate the needs of Roanoke and Salem’s growing population.\n\n\nDiscussion: Interpreting the Living Fabric of Roanoke and Salem\n\nReal-World Correspondence: The clusters delineate the urban (dense, central areas), suburban (less dense, outskirts), and rural (sparse, peripheral areas) zones, painting a picture of residential density and urban spread.\nInfluencing Factors:\n\nAmenities: Areas with more facilities, like schools and malls, could attract larger populations, leading to denser clusters.\nTransport Infrastructure: Homes near major roads or public transit lines often form noticeable clusters, suggesting a preference for connectivity.\nEconomic Dynamics: Housing affordability and job availability are likely to influence where people choose to live, affecting the cluster distribution.\n\nOverlap Insights:\n\nTransition Zones: Overlapping clusters may signal transitional areas where different urban zones meet and meld.\nGrowth Patterns: These overlaps could also highlight regions undergoing development, indicating a blend of old and new neighborhoods.\n\nPlanning Implications: Recognizing these patterns is crucial for urban development strategies, aiming to balance growth with the preservation of the cities’ unique identities.\n\n\n\nConclusion: Unveiling the Geospatial Heartbeat of Roanoke and Salem\n\n\nKey Findings: Our analysis revealed distinct clusters that reflect the nuanced interplay between Roanoke and Salem’s urban, suburban, and rural landscapes. The clustering patterns highlighted not just where people live, but also how they are likely to interact with their environment based on proximity to amenities and infrastructure.\nSignificance of Geographic Clustering: The power of geographic clustering lies in its ability to transform raw data into a narrative about population distribution and urban dynamics. It offers a bird’s-eye view of the area’s residential heartbeat, providing insights into the patterns of human settlement.\nImplications for Urban Development: This clustering analysis is more than an academic exercise; it’s a tool for change. Urban planners and policymakers can harness this knowledge to make informed decisions about where to develop next, how to allocate resources efficiently, and how to plan future infrastructure. For instance, denser clusters might need more public transit, parks, or schools, while sparser ones could be targeted for sustainable development to prevent urban sprawl.\nBroader Applications: Beyond urban planning, this analysis could inform emergency services on where to focus preparedness efforts, help businesses decide where to open new locations, and guide environmental assessments to ensure green spaces are preserved and expanded.\n\n\nIn essence, by shedding light on how our cities pulse with life, geographic clustering empowers us to craft communities that are not only vibrant and prosperous but also equitable and sustainable."
  },
  {
    "objectID": "posts/Anomaly-outlier detection DBSCAN labels for scatter plot/index.html",
    "href": "posts/Anomaly-outlier detection DBSCAN labels for scatter plot/index.html",
    "title": "Beyond the Cluster: Spotting the Outliers in Roanoke and Salem",
    "section": "",
    "text": "Introduction:\nDiscovering anomalies can reveal much about a city’s living patterns that conventional clustering might miss. In Roanoke and Salem cities, we dive into the uncommon, the outliers, the unique homeplaces that form the fabric of urban and suburban life. Using DBSCAN, an unsupervised machine learning algorithm, we identify these anomalies, offering insights into how and where people choose to live outside the expected clusters.\n\n\nThe Power of Anomaly Detection:\nAnomaly detection stands at the frontier of data analysis, challenging the status quo by highlighting data points that don’t fit in. In the context of urban landscapes, these anomalies could signify emerging neighborhoods, atypical living arrangements, or socio-economic factors influencing residential choices.\n\n\nDBSCAN: A Primer:\nDensity-Based Spatial Clustering of Applications with Noise (DBSCAN) is a robust algorithm that groups closely packed points together while labeling points that lie alone in low-density regions as outliers. Unlike K-Means, DBSCAN doesn’t require pre-specifying the number of clusters, making it ideal for real-world data that’s messy and unpredictable.\n\n\nMethodology:\nWe began with standardizing our geographical data to give each homeplace an equal footing. Next, we employed the DBSCAN algorithm, fine-tuning its parameters through a k-distance graph to ensure optimal clustering. This method distinguishes between the core points, the border points, and the noise, providing us with a granular view of residential distributions.\n\n\nCode\n```{python}\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import NearestNeighbors\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('WorkersHomeplacesNewUpdate.csv')\n\n# Select only the geographic coordinates\ndf_geo = df[['X', 'Y']]\n\n# Standardizing the features\nscaler = StandardScaler()\ndf_geo_scaled = scaler.fit_transform(df_geo)\n\n# Plot the k-distance graph\nnbrs = NearestNeighbors(n_neighbors=4).fit(df_geo_scaled)\ndistances, indices = nbrs.kneighbors(df_geo_scaled)\n\n\n# After identifying a new eps value from the graph, adjust eps and min_samples\neps_value = 0.05  # This is an example, adjust based on the k-distance graph\nmin_samples_value = 20  # This is an example, adjust based on the dataset\n\n# Perform DBSCAN clustering with the new parameters\ndbscan = DBSCAN(eps=eps_value, min_samples=min_samples_value)\nclusters = dbscan.fit_predict(df_geo_scaled)\n\n# Add cluster labels to the dataframe\ndf['Cluster'] = clusters\n\n# Plot the clusters with the new parameters\nplt.figure(figsize=(10, 6))\nscatter = plt.scatter(df['X'], df['Y'], c=df['Cluster'], cmap='viridis', marker='o')\nplt.title('Adjusted DBSCAN Clustering of Homeplaces')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.colorbar(scatter, label='Cluster Label')\nplt.show()\n```\n\n\n\n\n\n\n\n\n\n\nInsights and Patterns:\nThe DBSCAN algorithm has uncovered distinct residential patterns in Roanoke and Salem, revealing expected clusters of habitation and, more notably, outliers. These isolated data points potentially represent rural residences or unique urban spaces that challenge typical clustering.\n\n\nVisualization:\nThe map below demonstrates our findings with a scatter plot that overlaps the DBSCAN output map and the 55,000 points representing workers’ homeplaces scattered across the Roanoke and Salem cities. It color-codes the conventional neighborhoods and marks the outliers, effectively narrating the diversity in residential preferences. DBSCAN excels at detecting these patterns based on data density alone, without the need for predefined groupings. Its ability to identify clusters of any shape and its robustness in labeling outliers as noise enables us to highlight not just common residential areas but also those unique arrangements that may signal new developments or illustrate non-conforming choices. This analysis provides valuable insights into the array of living patterns present in the region.\n\n\n\nA scatter plot overlaps the DBSCAN output map and the 55,000 points representing workers’ homeplaces scattered across the Roanoke and Salem cities.\n\n\n\n\nConclusion:\nThrough this comprehensive DBSCAN analysis, we’ve uncovered the hidden layers of Roanoke and Salem’s residential tapestry. The output has illuminated the intricate tapestry of living spaces by dissecting dense cores, transitional zones, and the outliers, we’ve gained an unprecedented view of the region’s residential heartbeat. The clusters and anomalies captured here paint a picture of diversity and complexity in urban and suburban living. These insights are instrumental for urban planners and stakeholders, offering a data-driven foundation for decisions that embrace both the well-established neighborhoods and the burgeoning communities, as well as the unique outliers that give Roanoke and Salem their distinct character. Our exploration is a testament to the city's dynamic living patterns, revealing the essence of its urban and rural interplay.\nThe multifaceted nature of residency in Roanoke and Salem, highlighting the unique alongside the expected. These insights are invaluable for urban development, offering a fresh perspective that can help shape more responsive communities. In recognizing every outlier, we pave the way for a richer, more inclusive urban tapestry.\n\n\nNext Steps:\nAs we continue to explore the data, comparing the results of K-Means and DBSCAN will be our next endeavor. Stay tuned for a comprehensive analysis where we juxtapose the general trends with the unique anomalies to paint a complete picture of urban settlement patterns."
  },
  {
    "objectID": "About.html",
    "href": "About.html",
    "title": "About!",
    "section": "",
    "text": "Hi,\nThis is Heba Nusair, a Ph.D. Candidate in Landscape Architecture at Virginia Tech. I am passionate about promoting sustainability and resilience in cities, with expertise in modeling urban growth planning using Geospatial Information Science. I have a broad background in land analysis and evaluation, urban planning, and design using GIS, honed through years of extensive experience in the field.\n\nMy current doctoral dissertation, ‘Predicting the Impact of Shifting to Work-from-Home Paradigm on Urban Sprawl Using Agent-Based Model and Artificial Intelligence,’ demonstrates my commitment to developing innovative solutions that address complex urban challenges on a large and medium city scale.\nI’m here to share my findings with you through a series of engaging blog posts. In some of these, I’ve even used synthesized data sets to shed light on different facets of the outcomes. So, whether you’re passionate about urban planning or just curious about the future of work, let’s dive into the world of the changing nature of work and urban development."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my posts",
    "section": "",
    "text": "Code\nquarto render\ngit push\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Work Trends Across Industries with Naive Bayes: Remote or On-Site\n\n\nProbability theory and random variables, Naive Bayes classifier for multivariate Bernoulli models\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\nplot\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nHeba Nusair\n\n\n\n\n\n\n  \n\n\n\n\nUnveiling the Tapestry of Residential Preferences for Workers, A Data-Driven Glimpse into Roanoke’s Metropolitan Living Choices\n\n\nRandom Forest Classifier\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2023\n\n\nHeba Nusair\n\n\n\n\n\n\n  \n\n\n\n\nExploring the Relationship Between Income and Living Location Preference\n\n\nA Comparative Analysis of Linear and Non-Linear Regression Models\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nHeba Nusair\n\n\n\n\n\n\n  \n\n\n\n\nBeyond the Cluster: Spotting the Outliers in Roanoke and Salem\n\n\nUncovering Hidden Residential Gems with DBSCAN Anomaly Detection\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2023\n\n\nHeba Nusair\n\n\n\n\n\n\n  \n\n\n\n\nUrban Clustering: Decoding Roanoke and Salem’s Living Patterns\n\n\nInsights from K-Means Clustering on Residential Concentrations\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2023\n\n\nHeba Nusair\n\n\n\n\n\n\n  \n\n\n\n\nExploratory Data Analysis\n\n\nThe Future of Work and Cityscapes: An Exploratory Data Analysis\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2023\n\n\nHeba Nusair\n\n\n\n\n\n\n  \n\n\n\n\nURBAN Insights: K-Means vs. DBSCAN in Decoding Roanoke and Salem’s Residential Landscapes\n\n\nK-Means and DBSCAN Side by Side\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nHeba Nusair\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Unveiling the Tapestry of Residential Preferences for Workers, A Data-Driven Glimpse into Roanoke’s Metropolitan Living Choices",
    "section": "",
    "text": "Deciding where to live for Workers on-Site (WOS) is a big decision for everyone, and for the people of Roanoke, it’s no different. We all have our own checklist when it comes to choosing our neighborhoods – some of us want to be close to work, others are looking for good schools for the kids, and some might prioritize a big backyard over everything else.\n\n\n\nFactors that might influence the residential decision-making process for Workers on_Site\n\n\nTo get to the heart of what matters most to Roanoke’s residents, we turned to data. Using a detailed survey filled with personal insights from locals. the datasets featured in these blog posts have been modified and synthesized for educational purposes. They demonstrate various data analysis techniques. We analyzed a range of factors from work life to family plans, all to answer one question: What drives people's choices about where they live?\nArmed with Python, a popular programming language, and a machine learning tool called Random Forest, we dug into the data. Think of Random Forest as a detective that examines all the evidence (or data) and identifies the usual suspects (or factors) that play a role in workers’ home-place choices.\nThe goal of using a Random Forest Classifier is to identify the factors that influence the residential preferences of Workers on-Site (WOS) in the Roanoke Metropolitan Area. Here are the specific objectives Random Forest helps to achieve:\n\nFeature Importance: Random Forest can determine the relative importance of each factor (like distance to work, income level, family size) in predicting residential preferences. This insight can inform urban planning and real estate development.\nPredictive Modeling: It can predict an individual’s preferred living area based on the features in the dataset, which could be useful for personalized recommendations or targeted marketing for real estate.\nHandling Complexity: Random Forest is robust to complex interactions between features and can handle non-linear relationships without the need for transformation, making it well-suited for diverse and complex datasets.\nReducing Overfitting: Due to its ensemble nature (combining multiple decision trees), Random Forest is less prone to overfitting compared to individual decision trees, leading to more reliable predictions.\nVersatility: It can handle both categorical and numerical data, making it versatile for datasets that contain a mix of different types of variables as is often the case in survey data.\nUnderstanding Population Segments: By examining which features are most influential, stakeholders can understand different segments of the population better and tailor their services or policies accordingly.\n\nIn essence, Random Forest acts as a powerful analytical tool that turns a complex array of data points into actionable insights about what drives people’s choices on where to live.\nAfter some thorough data cleaning to make sure we were working with accurate information, we let the Random Forest algorithm get to work. It looked at various details about people’s lives, including how far they live from work, their family size, and their income.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Load the dataset\ndata = pd.read_excel('Classification (In_person_Workers).xlsx')\n\n# Replace non-breaking spaces in column names\ndata.columns = data.columns.str.replace('\\xa0', ' ', regex=True)\n\n# Correct any potential typos in column names\n# Ensure these column names exactly match those in your DataFrame\ndata.rename(columns={'living costs ': 'living costs'}, inplace=True)\n\n# Convert the first four columns into a single ordinal dependent variable\n# Ensure these column names exactly match those in the DataFrame\narea_columns = ['City Center (Central Business District)', 'Urban area', 'Suburban area', 'Rural Area']\n# For each row, it finds the column among the specified area_columns that has the minimum value \n# (which would be equivalent to the highest preference rank, assuming 1 is the most preferred and \n# larger numbers indicate lower preferences) and stores the name of this column (i.e., the living area \n# with the highest preference for that row) as the value in the new Preferred_Living_Location column.\n\ndata['Preferred_Living_Location'] = data[area_columns].idxmin(axis=1)\ndata.drop(columns=area_columns, inplace=True)\n\n# Handle missing values (NaNs) for both features and target variable\ndata.dropna(inplace=True)\n\n# Define the list of categorical and continuous features\n# Replace these with the actual column names from the DataFrame\ncategorical_features = ['marital status', 'race', 'education level', 'firm age', 'firm size', 'owner or renter',\n                        'number of workers in household', 'people in household', 'desired family size in future',\n                        'monthly earning', 'industry sector']  # categorical features \ncontinuous_features = ['distance from work to homeplace']  # continuous features here\n\n# Preprocessing pipeline\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), continuous_features),\n        ('cat', OneHotEncoder(), categorical_features)\n    ])\n\n# Preprocess the data\nX = preprocessor.fit_transform(data.drop('Preferred_Living_Location', axis=1))\ny = data['Preferred_Living_Location'].astype(str)  # Convert to string if categorical\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train the Random Forest Classifier\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\n\n# Get feature importances\nfeature_names = continuous_features + list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features))\nimportances = rf.feature_importances_\nsorted_indices = np.argsort(importances)[::-1]\n\n# Display feature importances\nprint(\"Feature Importances:\")\nfor i in sorted_indices:\n    print(f'{feature_names[i]}: {importances[i]}')\n\n\nFeature Importances:\ndistance from work to homeplace: 0.09208089305335408\nfirm age_3.0: 0.023416628944443307\nfirm age_2.0: 0.02240938617423798\nfirm size_3.0: 0.021960455301975785\nmonthly earning_2.0: 0.021730349477508813\nmonthly earning_3.0: 0.021169661362810812\npeople in household_3.0: 0.019542402169299766\nmarital status_2.0: 0.019248308937273233\nnumber of workers in household_1.0: 0.01901848437327954\nowner or renter_2.0: 0.01878229441224591\nfirm size_2.0: 0.018725887516167317\npeople in household_4.0: 0.0185402057096412\nfirm age_4.0: 0.018410437914987702\nrace_1.0: 0.01813395830094342\ndesired family size in future_5.0: 0.017888544166475086\nfirm age_5.0: 0.01788622507539638\neducation level_5.0: 0.01711617568391683\nmarital status_1.0: 0.01679842278918678\neducation level_2.0: 0.016711141178355583\neducation level_6.0: 0.016060106732178274\nindustry sector_Manufacturing: 0.016046061875347746\nnumber of workers in household_2.0: 0.01601492274496306\ndesired family size in future_4.0: 0.01591515949108339\nnumber of workers in household_3.0: 0.015875370860094093\ndesired family size in future_3.0: 0.015856977340919152\npeople in household_5.0: 0.01576170308621429\ndesired family size in future_2.0: 0.015504525463190443\nowner or renter_1.0: 0.015391404408841349\nmonthly earning_1.0: 0.015125757843962915\nfirm size_5.0: 0.014773976530264217\nfirm size_4.0: 0.013652021448688309\nindustry sector_Wholesale Trade: 0.013587577383640892\npeople in household_2.0: 0.013575470737134379\nrace_3.0: 0.013104668042282202\nfirm size_1.0: 0.01282010354514757\nindustry sector_Professional, Scientific, and Technical Services: 0.012720252126842255\nindustry sector_Retail Trade: 0.012428842104580999\ndesired family size in future_1.0: 0.012395010530580166\nindustry sector_Mining, Quarrying, and Oil and Gas Extraction: 0.011919447046121927\nindustry sector_Management of Companies and Enterprises: 0.011832841769346363\neducation level_3.0: 0.011818938887225811\nmonthly earning_4.0: 0.011258530167360841\nindustry sector_Transportation and Warehousing: 0.01030667345609924\neducation level_7.0: 0.010036845082616725\nindustry sector_Information: 0.009976099214426515\neducation level_8.0: 0.009838372376832468\nindustry sector_Educational Services: 0.009569272960058292\nrace_2.0: 0.009344109043648127\nindustry sector_Construction: 0.009287276911850108\npeople in household_1.0: 0.009018796450142056\nindustry sector_Accommodation and Food Services: 0.00888895012949622\nfirm age_1.0: 0.008835733007302478\nrace_4.0: 0.008802757644339156\nindustry sector_Finance and Insurance: 0.008694617400533972\nmarital status_3.0: 0.008658000399044359\nmarital status_5.0: 0.008589698442216668\nrace_8.0: 0.008410913048218364\neducation level_4.0: 0.008249845141186889\nmarital status_6.0: 0.008138947630528888\nindustry sector_Health Care and Social Assistance: 0.00811110346733394\nrace_7.0: 0.007819928549855805\nrace_5.0: 0.007624088745457004\nmarital status_4.0: 0.007124383883059234\neducation level_1.0: 0.00694676394267105\nindustry sector_Arts, Entertainment, and Recreation: 0.006868224616202844\nindustry sector_Agriculture, Forestry, Fishing and Hunting: 0.006791984946081143\nindustry sector_Utilities: 0.006626295863339866\nindustry sector_Other Services [except Public Administration: 0.006401779241334148\nrace_6.0: 0.005903030553878435\nindustry sector_Public Administration: 0.0040742536753326335\nindustry sector_Real Estate and Rental and Leasing: 0.004037109527011782\nindustry sector_Administrative and Support and Waste Management and Remediation Services: 0.004014611942391406\n\n\n\nGeneral Interpretation:\n\nThe features with higher importance scores are more influential in determining the preferred living area. This doesn't necessarily mean causation but indicates a stronger association.\nThe model suggests that practical considerations like distance to work, firm age, earnings, and household characteristics play a significant role in determining living area preferences.\n\nCaveats:\n\nThe Random Forest model treats the problem as a classification task and does not inherently account for the ordinal nature of the dependent variable.\nThe results should be interpreted as indicative rather than definitive. For more precise modeling of ordinal outcomes, specific ordinal regression models would be more appropriate.\n\nIn summary, the model's output provides a useful guide to understanding which factors are most influential in determining living area preferences, with distance from work appearing as the most significant factor according to this model.]\n\nThe results were clear: For workers who commute to work; the distance from workplace to homeplace stood out as the number one factor. This wasn’t too surprising – after all, who wants to spend hours in traffic when they could be home relaxing? Income was also a key factor. It seems the more we earn, the more choices we have about where we live.\nBut it’s not all about money and travel time. The data also showed us that personal factors, like whether someone is married or planning for a big family, have a big say in the decision too.\nThis isn’t just interesting information. It’s valuable knowledge for city planners and developers. By understanding what’s important to residents, they can plan and build communities that better meet people’s needs and make Roanoke an even better place to live.\nSo next time you’re driving through Roanoke’s neighborhoods, remember that each home you see represents a decision made based on a unique mix of practical needs and personal dreams.\n\n\nA Visual Journey Through Roanoke’s Residential Decision Drivers\nIn our previous discussion, we delved into the factors that influence where WOS in Roanoke choose to live. We discovered that the daily commute and income levels are among the top considerations. But how do these factors stack up against each other? Let’s take a visual dive into the data.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Assuming 'importances', 'feature_names' are already defined as in the previous code\n# Sorting the feature importances\nsorted_indices = np.argsort(importances)[::-1]\nsorted_importances = importances[sorted_indices]\nsorted_features = [feature_names[i] for i in sorted_indices]\n\n# Selecting a number of top features to display (you can adjust this number)\nn_top_features = 20  # for example, top 20 features\n\n# Creating the plot\nplt.figure(figsize=(6.5, 6.5))\nsns.barplot(x=sorted_importances[:n_top_features], y=sorted_features[:n_top_features])\nplt.title('Significance of factors in residential decision-making process')\nplt.xlabel('Level of importance')\nplt.ylabel('Factors')\nplt.show()\n\n\n\n\n\nThe accompanying chart lays out the findings from our Random Forest analysis in a colorful display of bars, each representing a factor’s weight in the decision-making process. At a glance, the lengthier bars catch the eye—these are the heavy hitters of home location preferece.\nDominating the scene is ‘distance from work to homeplace,’ a beacon of red at the top, which resonates with anyone who’s ever been caught in the snarl of rush hour. This is followed closely by a cascade of oranges and yellows highlighting the importance of ‘monthly earnings’ across various brackets and the age and size of firms.\nAs we move down the chart, the colors shift to cooler greens and blues, representing factors like household size and marital status. These may not be the titans of the plot, but they still play a pivotal role in the collective narrative of where we live.\nWhat’s truly intriguing is the story this visual tells us—beyond numbers and models, it’s about real-life choices and priorities. For instance, the significance of ‘people in household_4.0’ and ‘education level_5.0’ (a bachelor’s degree) speaks to the aspirations and day-to-day realities that shape our sense of place.\nCity planners and housing developers, take note: this chart is more than just a pretty picture. It’s a roadmap to the hearts and minds of Roanoke’s residents, guiding the way to communities that not only house but also support and enrich their lives.\nAs we continue to shape Roanoke’s urban landscape, let’s keep these visual cues in mind, ensuring that every development, from high-rises to townhomes, aligns with the genuine preferences of those who will call them home. After all, understanding these preferences is the key to building not just houses but homes where life’s stories can unfold."
  },
  {
    "objectID": "posts/Clustering vs. Anomaly Detection/index.html",
    "href": "posts/Clustering vs. Anomaly Detection/index.html",
    "title": "URBAN Insights: K-Means vs. DBSCAN in Decoding Roanoke and Salem’s Residential Landscapes",
    "section": "",
    "text": "Introduction:\nIn the quest to decode the living patterns of Roanoke and Salem, our exploration has evolved. We’ve delved into the fabric of these cities with a dual approach: the methodical partitioning of K-Means clustering and the intuitive scrutiny of DBSCAN anomaly detection. This blog unveils a side-by-side analysis, bridging the gap between regimented clusters and the organic sprawl of outliers to paint a comprehensive picture of urban living.\nBy juxtaposing the delineated neighborhoods identified by K-Means with the nuanced anomalies unearthed by DBSCAN, we reveal not just where residents are choosing to live, but how these patterns reflect the unique character of each city. From the densely woven centers to the individualistic fringes, this comparative study offers a multi-dimensional view of urban sprawl, giving voice to both the harmony of the collective and the distinct notes of the individual.\nJoin us as we navigate through the clustered chorus of the cities’ heartbeats and into the quiet corners of outlier enclaves, discovering a richer narrative of Roanoke and Salem’s residential landscapes.\n\n\nComparative Analysis:\nHere’s a snapshot of what each method revealed about the same dataset:\n\n\n\n\n\n\n\n\nFeature\nK-Means Clustering\nDBSCAN Anomaly Detection\n\n\n\n\n\n\n\n\n\nObjective\nTo partition the dataset into distinct, non-overlapping clusters based on proximity.\nTo identify core groups and outliers based on density.\n\n\nCluster Count\nPredefined number of clusters.\nNumber of clusters based on data density, without predefining.\n\n\nCluster Shape\nAssumes spherical clusters.\nCan detect clusters of arbitrary shapes.\n\n\nSensitivity to Outliers\nSensitive to outliers, which may skew the clustering.\nRobust to outliers, labels them as noise.\n\n\nResults Interpretation\nClusters represent common residential groupings, indicating popular areas for settlement.\nHighlights both common groupings and anomalies, providing insight into standard and atypical residential choices.\n\n\nUse-Cases\nIdeal for segmenting areas into clear, defined neighborhoods.\nSuited for identifying unique living spaces, potential for new developments, and rural or isolated homes.\n\n\n\n\n\nInsights and Patterns:\n\nK-Means Clustering provided us with a bird’s-eye view of residential densities, helping to identify the most and least populated areas.\nDBSCAN Anomaly Detection cut through the noise to spotlight the unexpected, revealing where residents break the mold of traditional urban living.\n\n\n\nVisualization:\nWe present the contrasting visuals from both analyses. The K-Means map shows clustered zones of varying sizes, while the DBSCAN plot illuminates the intricate tapestry of living spaces by dissecting dense cores, transitional zones, and the outliers, and gains an unprecedented view of the region’s residential heartbeat. DBSCAN anomaly detection could shows the diversity and complexity in urban and suburban living. These insights are instrumental for urban planners and stakeholders, offering a data-driven foundation for decisions that embrace both the well-established neighborhoods and the burgeoning communities, as well as the unique outliers that give Roanoke and Salem their distinct character.\n\n\nConclusion:\nEach method paints a part of the picture: K-Means outlines the collective rhythm of city life, while DBSCAN tunes into the soloists. Together, they compose a symphony of urban and suburban existence, a composition as diverse as the cities themselves."
  },
  {
    "objectID": "posts/Linear and nonlinear regression/index.html",
    "href": "posts/Linear and nonlinear regression/index.html",
    "title": "Exploring the Relationship Between Income and Living Location Preference",
    "section": "",
    "text": "Introduction\nLiving in Roanoke: Does Your Income Decide Your Address? We’ve dived into the world of Roanoke’s remote workers to see how much their earnings influence where they live. Using linear regression, we’ve connected the dots between income and neighborhood choices in this vibrant metropolitan area. Check out our eye-opening findings in the figure below! (See the figure).\n\n\n\nThe urban areas classification in Roanoke Metropolitan Area produced by the author.\n\n\n\n\nThe Study at a Glance\nOur dataset included responses from individuals, detailing their income levels and their preferred living areas, ranging from the bustling city center to the tranquil rural landscapes. We analyzed responses detailing income levels and preferred living areas, from busy city centers to peaceful rural settings. Our unique “relocating index” helped us turn these preferences into measurable data. So again, the variables used are processed as the following:\n\nTransforming income into a continuous scale, assigning monetary values to income brackets.\nConverting living area preferences into a single, ordinal dependent variable ‘Living Location Preference: moving from urban to rural index’.\n\n\n\nInsights from Linear Regression Analysis\nEmploying linear regression to model the relationship between the two variables: income & Relocating index.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndata = pd.read_excel('Income_LivingLocationPrefrences.xlsx')\n\n# Handle NaN values\ndata.dropna(subset=['City_center', 'Urban_area', 'Suburban_area', 'Rural_area', 'Monthly_income'], inplace=True)\n\n# Convert the Location preferences into a single ordinal dependent variable\narea_to_number = {'City_center': 1, 'Urban_area': 2, 'Suburban_area': 3, 'Rural_area': 4}\ndata['living_Location_preference'] = data[['City_center', 'Urban_area', 'Suburban_area', 'Rural_area']].idxmin(axis=1).map(area_to_number)\n\n# Convert income to a continuous scale based on the provided income brackets\nincome_mapping = {1.0: 625, 2.0: 2292, 3.0: 5000, 4.0: 6666}\ndata['continuous_income'] = data['Monthly_income'].map(income_mapping)\n\n# Scale the income feature\nscaler = StandardScaler()\ndata['scaled_income'] = scaler.fit_transform(data[['continuous_income']])\n\n# Prepare the features and target variable for modeling\nX = data[['scaled_income']]\ny = data['living_Location_preference']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train the linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Predict on the test data\ny_pred = model.predict(X_test)\n\n# Evaluate the model's performance\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Output the performance metrics\nprint(f'Mean Squared Error: {mse}')\nprint(f'R-squared: {r2}')\n\n# Debugging the sizes of arrays\nprint(\"Sizes of arrays for plotting:\")\nprint(\"X_test['scaled_income']: \", len(X_test['scaled_income']))\nprint(\"y_test: \", len(y_test))\nprint(\"y_pred: \", len(y_pred))\n\n# Plotting\nplt.scatter(X_test['scaled_income'].values, y_test.values, color='black', label='Actual Data')\nplt.scatter(X_test['scaled_income'].values, y_pred, color='red', label='Predicted Data', alpha=0.5)\n\n# Optionally, create a more continuous line for predictions\nsorted_order = np.argsort(X_test['scaled_income'].values)\nplt.plot(X_test['scaled_income'].values[sorted_order], y_pred[sorted_order], color='blue', linewidth=2, label='Regression Line')\n\nplt.xlabel('Monthly income')\nplt.ylabel('Living Location Preference:Relocating index_From urban to rural')\nplt.title('Income vs Living Location Preference Linear Regression')\nplt.legend()\nplt.show()\n\n\nMean Squared Error: 1.2658068009077692\nR-squared: 0.006851215790502296\nSizes of arrays for plotting:\nX_test['scaled_income']:  162\ny_test:  162\ny_pred:  162\n\n\n\n\n\nIn this plot, the actual data points (black dots) showed significant deviation from the blue regression line (the model’s predictions). The red dots, representing the model’s predicted values, also varied widely from many actual data points, indicating a mismatch between the model’s predictions and the real data.\n\n\nUnveiling the Linear Regression Insights for WFH Workers\nOur linear regression model yielded the following insights:\n\nMean Squared Error (MSE): The model showed an MSE of 1.2658068009077692. This number, though not extremely high, indicates some discrepancies between the model’s predictions and the actual data.\nR-squared Value: We obtained an R-squared value of 0.006851215790502296. This low value suggests that our model might not be capturing the complete picture, especially in the context of WFH employees.\n\n\n\nTransitioning to Non-Linear Models\nThe visual and numerical analysis led us to consider that the relationship between income and living area preferences might not be linear. This prompted a shift towards exploring non-linear patterns, We Chose a Multinomial Logistic Regression!!!! for these key reasons:\n\nAppropriate for Ordinal Data: Our data on living location preferences is ordinal (ranging from city centers to rural areas). Ordinal regression accurately handles such data, unlike linear regression.\nCaptures Non-Linear Patterns: This model is better suited to reveal non-linear relationships between income and living location preferences, which our initial analysis suggested.\nEnhanced Predictive Accuracy: Ordinal regression aligns more closely with our real-world data, potentially offering improved accuracy in predictions.\n\n\n\nImplementing Multinomial Logistic Regression Analysis\nIn this analysis. the location preferences into a single variable suggests that the resulting data is ordinal in nature. Here’s why:\n\nThe values assigned (1 for ‘City_center’, 2 for ‘Urban_area’, 3 for ‘Suburban_area’, and 4 for ‘Rural_area’) imply an order or ranking.\nThe order seems to represent a gradient from the most urbanized area (‘City_center’) to the least (‘Rural_area’).\n\nGiven this structure, the variable ‘preference rank’ would be more appropriate for ordinal regression analysis since it reflects a clear order or hierarchy in the data.\n\n\nCode\nimport pandas as pd\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset\ndata = pd.read_excel('Income_LivingLocation_GradientBoostingRegressor.xlsx')  \n# Update the path to your file\n\n# Combine the area preferences into a single ordinal variable\ndef get_preference_rank(row):\n    if row['City_center'] == 1:\n        return 1\n    elif row['Urban_area'] == 1:\n        return 2\n    elif row['Suburban_area'] == 1:\n        return 3\n    elif row['Rural_area'] == 1:\n        return 4\n\ndata['living_area_preference'] = data.apply(get_preference_rank, axis=1)\n\n# Prepare the features and the target variable\nX = data[['Monthly_income', 'Education_Level', 'House_owner_or_renter', 'number of employees in household']]\ny = data['living_area_preference']\n\n# Add a constant to the model (intercept)\nX = sm.add_constant(X)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Fit the ordinal regression model\nmodel = sm.MNLogit(y_train, X_train)\nresult = model.fit()\n\n# Output the model summary\nprint(result.summary())\n\n\nOptimization terminated successfully.\n         Current function value: 1.336686\n         Iterations 5\n                            MNLogit Regression Results                            \n==================================================================================\nDep. Variable:     living_area_preference   No. Observations:                  680\nModel:                            MNLogit   Df Residuals:                      665\nMethod:                               MLE   Df Model:                           12\nDate:                    Thu, 30 Nov 2023   Pseudo R-squ.:                 0.03553\nTime:                            17:25:07   Log-Likelihood:                -908.95\nconverged:                           True   LL-Null:                       -942.43\nCovariance Type:                nonrobust   LLR p-value:                 1.174e-09\n====================================================================================================\n        living_area_preference=2       coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------------------\nconst                                2.5322      0.630      4.020      0.000       1.298       3.767\nMonthly_income                      -0.6509      0.124     -5.230      0.000      -0.895      -0.407\nEducation_Level                      0.0909      0.055      1.642      0.101      -0.018       0.199\nHouse_owner_or_renter               -0.6254      0.262     -2.388      0.017      -1.139      -0.112\nnumber of employees in household    -0.2076      0.160     -1.296      0.195      -0.522       0.106\n----------------------------------------------------------------------------------------------------\n        living_area_preference=3       coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------------------\nconst                                2.8972      0.633      4.575      0.000       1.656       4.138\nMonthly_income                      -0.7957      0.126     -6.294      0.000      -1.043      -0.548\nEducation_Level                      0.0914      0.056      1.626      0.104      -0.019       0.202\nHouse_owner_or_renter               -0.6095      0.264     -2.307      0.021      -1.127      -0.092\nnumber of employees in household    -0.2320      0.163     -1.426      0.154      -0.551       0.087\n----------------------------------------------------------------------------------------------------\n        living_area_preference=4       coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------------------\nconst                                0.9399      0.628      1.496      0.135      -0.292       2.171\nMonthly_income                      -0.5330      0.124     -4.314      0.000      -0.775      -0.291\nEducation_Level                      0.1038      0.055      1.893      0.058      -0.004       0.211\nHouse_owner_or_renter               -0.0429      0.241     -0.178      0.859      -0.516       0.430\nnumber of employees in household     0.0543      0.160      0.341      0.733      -0.258       0.367\n====================================================================================================\n\n\n\n\nThe Multinomial Logistic Regression analysis revealed some interesting insights about the relationship between various factors and living area preferences:\n\nModel Convergence and Fit: The model successfully converged after 5 iterations, indicating a reliable fit to the data. The Pseudo R-squared value of 0.03553, while modest, suggests that our model has some explanatory power, though other unaccounted factors might also play a significant role.\nSignificant Predictors:\n\nMonthly Income: This was a significant predictor across all living area preferences (2, 3, and 4). The negative coefficients (-0.6509, -0.7957, and -0.5330) indicate that as monthly income increases, the likelihood of preferring urban (2) or suburban (3) areas over rural (4) areas decreases.\nHouse Ownership Status: This variable also showed significance in influencing living area preference, with negative coefficients suggesting that those who own houses or are renters are less likely to prefer urban or suburban areas compared to rural ones.\n\nOther Factors:\n\nEducation Level: While the coefficients were positive, suggesting a higher likelihood of preferring urban or suburban areas with increased education level, the significance was marginal.\nNumber of Employees in Household: This factor did not show a strong influence on living area preference, as indicated by the higher p-values.\n\nCoefficient Interpretation: The coefficients for each predictor vary for different living area preferences, reflecting the complex nature of these relationships. For instance, the impact of monthly income is more pronounced in preferring suburban areas (living_area_preference=3) than in urban or rural areas.\n\nIn conclusion, our Multinomial Logistic Regression model sheds light on how factors like income and house ownership status significantly influence living area preferences among WFH workers. The nuanced differences in coefficients across different living areas underscore the complexity of these relationships."
  },
  {
    "objectID": "Research Insights Explorer Using Machine learning.html",
    "href": "Research Insights Explorer Using Machine learning.html",
    "title": "Research Insights Explorer Using Machine learning",
    "section": "",
    "text": "Important Note: The datasets used on these posts are modified and synthesized, so they should be used solely for educational purposes and to demonstrate data analysis techniques. Please refrain from using this data for any genuine applications.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPredicting Work Trends Across Industries with Naive Bayes: Remote or On-Site\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\nplot\n\n\n\n\n\n\n\nHeba Nusair\n\n\nNov 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnveiling the Tapestry of Residential Preferences for Workers, A Data-Driven Glimpse into Roanoke’s Metropolitan Living Choices\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nHeba Nusair\n\n\nNov 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Relationship Between Income and Living Location Preference\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nHeba Nusair\n\n\nNov 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeyond the Cluster: Spotting the Outliers in Roanoke and Salem\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nHeba Nusair\n\n\nNov 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUrban Clustering: Decoding Roanoke and Salem’s Living Patterns\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nHeba Nusair\n\n\nNov 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nHeba Nusair\n\n\nNov 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nURBAN Insights: K-Means vs. DBSCAN in Decoding Roanoke and Salem’s Residential Landscapes\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nHeba Nusair\n\n\nNov 1, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  }
]