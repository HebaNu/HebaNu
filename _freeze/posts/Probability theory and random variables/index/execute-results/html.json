{
  "hash": "f19ee4cc287a64fa442b3577aa713d4f",
  "result": {
    "markdown": "---\ntitle: \"Probability theory and random variables\"\nauthor: \"Heba Nusair\"\ndate: \"2023-11-06\"\ncategories: [news, code, analysis]\nimage: \"Capture.JPG\"\ncode-viewfold: true\n---\n\n***Categorical Naive Bayes***\n\nThis project requires Python 3.7 or above:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport sys\n\nassert sys.version_info >= (3, 7)\n```\n:::\n\n\nFor the analysis I a planning, Python libraries like Pandas, Matplotlib, and Scikit-learn will be handy. Here's a high-level step-by-step guide to get you started: First I need to make Exploratory Data Analysis (EDA):\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Step 1: Load the dataset\n# Make sure to replace 'path_to_file.xlsx' with the actual path to your Excel file\ndf = pd.read_excel('CategoricalNB.xlsx')\n\n# Print the column names to check for any discrepancies\nprint(df.columns)\n\n# Step 2: Encode the 'Industry sector' using one-hot encoding\n# Notice the space before 'Industry sector' in the column name\nencoder = OneHotEncoder(sparse=False)\nX = encoder.fit_transform(df[[' Industry sector']])\n\n# The 'Working Method' feature is your target variable\ny = df['Working Method']\n\n# Step 3: Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Step 4: Train the Multinomial Naive Bayes model\nmodel = MultinomialNB()\nmodel.fit(X_train, y_train)\n\n# Step 5: Make predictions and evaluate the model\ny_pred = model.predict(X_test)\n\n# Use the classification report to handle divisions by zero\nprint(classification_report(y_test, y_pred, zero_division=0))\nprint(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIndex(['Response ID', ' Industry sector', 'Working Method',\n       'What is your job title?'],\n      dtype='object')\n              precision    recall  f1-score   support\n\n           1       0.50      0.29      0.37        24\n           2       0.47      0.91      0.62        58\n           3       0.45      0.13      0.20        38\n           4       0.00      0.00      0.00        18\n\n    accuracy                           0.47       138\n   macro avg       0.36      0.33      0.30       138\nweighted avg       0.41      0.47      0.38       138\n\nAccuracy: 0.47101449275362317\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\NUSAI\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning:\n\n`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n\n```\n:::\n:::\n\n\nThe output indicates that the code has successfully run and produced a classification report, but it shows that the Multinomial Naive Bayes classifier is not performing well for some classes (especially class 4). The FutureWarning is just to inform you about a future change in the scikit-learn library and doesn't impact the current execution of your code. It suggests that in the future version, the sparse parameter in OneHotEncoder will be removed and recommends using sparse_output instead. Since it's just a warning about future changes, you can ignore it for now.\n\nThe classification report shows the following:\n\nClass 1 has a precision of 0.50 and recall of 0.29. Class 2 has a precision of 0.47 and recall of 0.91, which is quite good. Class 3 has a precision of 0.45 and recall of 0.13, indicating the model struggles to correctly identify this class. Class 4 has both precision and recall of 0.00, which means the model fails to identify any samples of this class correctly. This poor performance for some classes may be due to class imbalance, insufficient features, or the model's inability to capture the complexities of the dataset.\n\nHere are a few recommendations to improve the model's performance:\n\nAddress Class Imbalance: Use techniques like SMOTE, ADASYN, or simply oversampling the minority class to balance the dataset. Feature Engineering: Add more features that could help the classifier distinguish between classes. Hyperparameter Tuning: Experiment with different hyperparameters for the MultinomialNB classifier. Try Different Models: Since Naive Bayes assumes feature independence and your features might be correlated, other models like Random Forest or Gradient Boosting might perform better. Evaluation Metrics: Use other evaluation metrics like AUC-ROC curve, especially when dealing with imbalanced classes. \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}