{
  "hash": "c419ccfcf75749fd1a295182315ccc53",
  "result": {
    "markdown": "---\ntitle: \"Exploring the Relationship Between Income and Living Location Preference\"\nsubtitle: \"Linear Regression VS. non_Linear Regression\"\nauthor: \"Heba Nusair\"\ndate: \"2023-11-06\"\ncategories: [news, code, analysis]\nimage: \"image.jpg\"\n---\n\n### **Introduction**\n\nIn our quest to understand how monthly income influences living location preferences, particularly among Workers From Home (WFH), we embarked on an analytical journey using linear regression. Linear regression is a statistical method that helps us understand the relationship between two variables -- in this case, income and living location preference of WFH workers.\n\n### **The Study at a Glance**\n\nOur dataset encapsulated responses from individuals, detailing their income levels and preferred living areas -- be it the bustling city center, the vibrant urban zones, the serene suburban areas, or the tranquil rural landscapes; this preference of **moving from urban area to rural** is represented by \"**relocating index**\". Our approach was straightforward:\n\n1.  We transformed **income** into a continuous scale, assigning monetary values to income brackets.\n\n2.  We converted **living area preferences** into a single, ordinal dependent variable.\n\n3.  We employed linear regression to model the relationship between these two variables.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndata = pd.read_excel('Income_LivingLocationPrefrences.xlsx')\n\n# Handle NaN values\ndata.dropna(subset=['City_center', 'Urban_area', 'Suburban_area', 'Rural_area', 'Monthly_income'], inplace=True)\n\n# Convert the Location preferences into a single ordinal dependent variable\narea_to_number = {'City_center': 1, 'Urban_area': 2, 'Suburban_area': 3, 'Rural_area': 4}\ndata['living_Location_preference'] = data[['City_center', 'Urban_area', 'Suburban_area', 'Rural_area']].idxmin(axis=1).map(area_to_number)\n\n# Convert income to a continuous scale based on the provided income brackets\nincome_mapping = {1.0: 625, 2.0: 2292, 3.0: 5000, 4.0: 6666}\ndata['continuous_income'] = data['Monthly_income'].map(income_mapping)\n\n# Scale the income feature\nscaler = StandardScaler()\ndata['scaled_income'] = scaler.fit_transform(data[['continuous_income']])\n\n# Prepare the features and target variable for modeling\nX = data[['scaled_income']]\ny = data['living_Location_preference']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train the linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Predict on the test data\ny_pred = model.predict(X_test)\n\n# Evaluate the model's performance\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Output the performance metrics\nprint(f'Mean Squared Error: {mse}')\nprint(f'R-squared: {r2}')\n\n# Debugging the sizes of arrays\nprint(\"Sizes of arrays for plotting:\")\nprint(\"X_test['scaled_income']: \", len(X_test['scaled_income']))\nprint(\"y_test: \", len(y_test))\nprint(\"y_pred: \", len(y_pred))\n\n# Plotting\nplt.scatter(X_test['scaled_income'].values, y_test.values, color='black', label='Actual Data')\nplt.scatter(X_test['scaled_income'].values, y_pred, color='red', label='Predicted Data', alpha=0.5)\n\n# Optionally, create a more continuous line for predictions\nsorted_order = np.argsort(X_test['scaled_income'].values)\nplt.plot(X_test['scaled_income'].values[sorted_order], y_pred[sorted_order], color='blue', linewidth=2, label='Regression Line')\n\nplt.xlabel('Monthly income')\nplt.ylabel('Living Location Preference:moving from urban to rural index')\nplt.title('Income vs Living Location Preference Linear Regression')\nplt.legend()\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 1.2658068009077692\nR-squared: 0.006851215790502296\nSizes of arrays for plotting:\nX_test['scaled_income']:  162\ny_test:  162\ny_pred:  162\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){width=589 height=449}\n:::\n:::\n\n\n### **Unveiling the Linear Regression Insights for WFH Workers**\n\nWe meticulously gathered data on WFH workers and ran a linear regression model. Here's a snapshot of our findings:\n\n-   <div>\n\n    -   **Mean Squared Error (MSE)**: Our model showed an MSE of 1.2658068009077692. This number tells us how close the model's predictions are to the actual data. Think of it as an accuracy gauge: the lower the number, the better. In our analysis, this number, though not extremely high, does indicate some level of discrepancy between what the model predicts and the real-world data of WFH workers.\n\n    -   **R-squared Value**: Perhaps more tellingly, our model yielded an R-squared value of just 0.006851215790502296. This metric is crucial -- it shows how well our independent variable (income) explains the variation in our dependent variable (living location preference). The closer this value is to 1, the better. Our low R-squared value hints that our model, as it stands, might not be capturing the whole picture, especially in the context of WFH employees.\n\n    </div>\n\n    In our analytical journey to understand how income affects living location preferences for Workers From Home (WFH), we've encountered some intriguing results. Our linear regression model produced a Mean Squared Error (MSE) of 1.2658068009077692 and an R-squared value of just 0.006851215790502296. While the MSE suggests a reasonable level of accuracy, the low R-squared value indicates that income alone may not be a strong predictor of living area preferences among WFH employees.\n\n    A glance at our regression plot further illustrates this point. The actual data points, represented by black dots, are scattered across the graph, deviating from the blue regression line that represents our model's predictions. The red dots, which mark our model's predicted values, also stray from many of the actual data points. This visual mismatch echoes the numerical findings, suggesting that the linear model doesn't fully capture the nuances of our data.\n\n    The plot signals that there could be more complexity to the relationship between income and living area preferences than a simple linear model can explain. It raises the possibility that other, unconsidered factors may influence this relationship, or that the dynamics between these variables are not linear at all.\n\n    ### **Transitioning to Non-Linear Models**\n\n    This revelation leads us to our next analytical adventure -- exploring non-linear relationships. It's possible that the connection between income and living location preference for WFH workers bends and curves in ways that a simple straight line can't capture.\n\n    In our upcoming analysis, we'll dive into non-linear regression models. Non-linear regression could potentially provide a better framework for capturing the intricate patterns that seem to exist between income and where WFH employees prefer to reside. We're excited to see how this approach compares with our initial linear model and what new insights we can uncover about the factors influencing where WFH employees choose\n\n    Stay tuned as we continue to unravel the intricacies of income and living location preferences for workers from home, one model at a time!\n\n### **Introducing the Concept of Non-Linear Relationships**\n\nIn our exploration of how income influences living area preferences, a linear model initially seemed like a straightforward choice. However, real-world data often tell a more complex story. Our linear model, with a surprisingly negative R-squared value, hinted at this complexity. It suggests that the relationship between income and living area preference might not be a straight line, but rather a curve or a more intricate pattern.\n\nLinear regression, with its assumption of a straight-line relationship between variables, is ill-equipped to capture these nuances. This is where the idea of non-linear relationships comes into play. Such relationships are not confined to straight lines and can take various forms, like curves, which are more adept at mapping the intricate patterns often found in real-world data.\n\n### **Implementing ordinal regression model**\n\nTo explore these potential non-linear patterns, we turn to\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset\ndata = pd.read_excel('Income_LivingLocation_GradientBoostingRegressor.xlsx')  \n# Update the path to your file\n\n# Combine the area preferences into a single ordinal variable\ndef get_preference_rank(row):\n    if row['City_center'] == 1:\n        return 1\n    elif row['Urban_area'] == 1:\n        return 2\n    elif row['Suburban_area'] == 1:\n        return 3\n    elif row['Rural_area'] == 1:\n        return 4\n\ndata['living_area_preference'] = data.apply(get_preference_rank, axis=1)\n\n# Prepare the features and the target variable\nX = data[['Monthly_income', 'Education_Level', 'House_owner_or_renter', 'number of employees in household']]\ny = data['living_area_preference']\n\n# Add a constant to the model (intercept)\nX = sm.add_constant(X)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Fit the ordinal regression model\nmodel = sm.MNLogit(y_train, X_train)\nresult = model.fit()\n\n# Output the model summary\nprint(result.summary())\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimization terminated successfully.\n         Current function value: 1.336686\n         Iterations 5\n                            MNLogit Regression Results                            \n==================================================================================\nDep. Variable:     living_area_preference   No. Observations:                  680\nModel:                            MNLogit   Df Residuals:                      665\nMethod:                               MLE   Df Model:                           12\nDate:                    Mon, 27 Nov 2023   Pseudo R-squ.:                 0.03553\nTime:                            04:23:23   Log-Likelihood:                -908.95\nconverged:                           True   LL-Null:                       -942.43\nCovariance Type:                nonrobust   LLR p-value:                 1.174e-09\n====================================================================================================\n        living_area_preference=2       coef    std err          z      P>|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------------------\nconst                                2.5322      0.630      4.020      0.000       1.298       3.767\nMonthly_income                      -0.6509      0.124     -5.230      0.000      -0.895      -0.407\nEducation_Level                      0.0909      0.055      1.642      0.101      -0.018       0.199\nHouse_owner_or_renter               -0.6254      0.262     -2.388      0.017      -1.139      -0.112\nnumber of employees in household    -0.2076      0.160     -1.296      0.195      -0.522       0.106\n----------------------------------------------------------------------------------------------------\n        living_area_preference=3       coef    std err          z      P>|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------------------\nconst                                2.8972      0.633      4.575      0.000       1.656       4.138\nMonthly_income                      -0.7957      0.126     -6.294      0.000      -1.043      -0.548\nEducation_Level                      0.0914      0.056      1.626      0.104      -0.019       0.202\nHouse_owner_or_renter               -0.6095      0.264     -2.307      0.021      -1.127      -0.092\nnumber of employees in household    -0.2320      0.163     -1.426      0.154      -0.551       0.087\n----------------------------------------------------------------------------------------------------\n        living_area_preference=4       coef    std err          z      P>|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------------------\nconst                                0.9399      0.628      1.496      0.135      -0.292       2.171\nMonthly_income                      -0.5330      0.124     -4.314      0.000      -0.775      -0.291\nEducation_Level                      0.1038      0.055      1.893      0.058      -0.004       0.211\nHouse_owner_or_renter               -0.0429      0.241     -0.178      0.859      -0.516       0.430\nnumber of employees in household     0.0543      0.160      0.341      0.733      -0.258       0.367\n====================================================================================================\n```\n:::\n:::\n\n\n### **Comparison of Linear vs. Polynomial Regression Models**\n\nIn our exploration of the relationship between income and living area preference, we analyzed two different models: Linear Regression and Polynomial Regression. The outcomes offer intriguing insights:\n\n1.  **Linear Regression Model**:\n\n    -   Initially, we employed a Linear Regression model, which yielded a Mean Squared Error (MSE) of 1.2229590696534407. This suggested some variance between the predicted and actual preferences.\n\n    -   However, the R-squared value was -0.0034801764611527286, indicating a poor fit. This negative value implies that the linear model might not be suitable for capturing the relationship in our analysis.\n\n2.  **Polynomial Regression Model**:\n\n    -   Seeking improvement, we then applied a Polynomial Regression model. It showed a lower MSE of 0.12978886986451396, suggesting a closer fit to the data compared to the Linear Regression model.\n\n    -   The R-squared value improved to 0.012140786799389347. While still low, it's a positive value, indicating a slight improvement in explaining the variability of the dependent variable.\n\n### **Comparative Table**\n\n| **Model Type**        | **Mean Squared Error (MSE)** | **R-squared Value**    |\n|----------------------|---------------------------|-----------------------|\n| Linear Regression     | 1.2229590696534407           | -0.0034801764611527286 |\n| Polynomial Regression | 0.12978886986451396          | 0.012140786799389347   |\n\n",
    "supporting": [
      "index_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}