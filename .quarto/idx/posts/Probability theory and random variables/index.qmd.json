{"title":"Probability theory and random variables","markdown":{"yaml":{"title":"Probability theory and random variables","author":"Heba Nusair","date":"2023-11-06","categories":["news","code","analysis"],"image":"Capture.JPG","code-fold":true,"code-viewfold":true},"headingText":"**Naive Bayes classifier for multivariate Bernoulli models**","containsRefs":false,"markdown":"\n\n\n```{python}\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.preprocessing import OneHotEncoder\nimport os\n\n# Load the dataset\ndf = pd.read_csv('C:/Users/NUSAI/Desktop/Machine learning/HebaNu.github.io/HebaNu.github.io/HebaNu/posts/Probability theory and random variables/updated_POST1.csv')\n\n# Drop rows where any cell is NaN in the 'Work Method' column\ndf = df.dropna(subset=['Work Method'])\n\n# One-hot encoding\nencoder = OneHotEncoder(sparse=False)\nX = encoder.fit_transform(df[[' industry sector']])  \n# Use the correct column name\n\n# Target variable\ny = df['Work Method']\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train the Bernoulli Naive Bayes model\nmodel = BernoulliNB()\nmodel.fit(X_train, y_train)\n\n# Make predictions and evaluate the model\ny_pred = model.predict(X_test)\n\n\n\n# Output the classification report and accuracy\nprint(classification_report(y_test, y_pred, zero_division=0))\nprint(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n\n```\n\nThe results from your BernoulliNB model show that:\n\nThe model is predicting class 2.0 with high precision (76%) and recall (100%), which means for this class, it performs well both in terms of the accuracy of the positive predictions it makes (precision) and its ability to find all the positive instances (recall).\n\nFor class 1.0, however, the model does not predict any instances correctly, which suggests that either there's an issue with the distribution of your classes (perhaps class 1.0 is underrepresented), or that the features do not provide enough information to distinguish class 1.0 from class 2.0. The overall accuracy of the model is 76.47%, which means that it correctly predicts the class for 76.47% of the test set.\n\nThe macro avg and weighted avg for precision, recall, and f1-score provide a summary of the effectiveness of the model across the classes. The low macro avg for precision and f1-score indicates that one of the classes does not perform well, which we already know is class 1.0.\n\nThe f1-score is a harmonic mean of precision and recall and is a useful metric when you have classes that are imbalanced. In your case, the f1-score for class 1.0 is 0.00, indicating poor performance for this class.\n\n***To create plots*** that visualize the performance of your **`BernoulliNB`** model, you would typically look at the confusion matrix, precision-recall, and possibly ROC curves. Below are examples of how you could generate each of these plots using **`matplotlib`** and **`scikit-learn`**:\n\n1.  **Confusion Matrix**: Visualizes the correct and incorrect predictions compared to the actual values.\n\n```{python}\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming y_test and y_pred are already defined from your BernoulliNB model\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.title('Confusion Matrix for BernoulliNB Model')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n\n```\n\n**ROC Curve**: Plots the true positive rate against the false positive rate.\n\n```{python}\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import RocCurveDisplay\nimport matplotlib.pyplot as plt\n\n# Adjust y_test to have binary labels 0 and 1\ny_test_binary = y_test.replace({1: 0, 2: 1})\n\n# Get predicted probabilities for the positive class (e.g., class 2)\ny_pred_prob = model.predict_proba(X_test)[:, 1]  # Index 1 for the probability of class 2\n\n# Calculate the ROC curve\nfpr, tpr, thresholds = roc_curve(y_test_binary, y_pred_prob)\n\n# Calculate the AUC (Area Under Curve)\nroc_auc = auc(fpr, tpr)\n\n# Plot the ROC curve\ndisp = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\ndisp.plot()\nplt.title('ROC Curve for BernoulliNB Model')\nplt.show()\n\n```\n\n**Classification Report**: Presents precision, recall, f1-score for each class.\n\n```{python}\nfrom sklearn.metrics import classification_report\nimport pandas as pd\n\nreport = classification_report(y_test, y_pred, output_dict=True)\ndf_report = pd.DataFrame(report).transpose()\n\ndf_report.drop(['accuracy'], inplace=True)  # Drop accuracy as it's not a class-specific metric\ndf_report.plot(kind='bar', figsize=(10, 7))\nplt.title('Classification Report for BernoulliNB Model')\nplt.ylabel('Score')\nplt.show()\n```\n\nPlease note that in order to generate a ROC curve or Precision-Recall curve, you need the probability scores or decision function, not just the predicted labels. If **`y_pred`** contains only class labels, you would need to use the **`predict_proba`** or **`decision_function`** method of your classifier to obtain these scores.\n\nThese code snippets are intended to be run in your local Python environment, and you'll need to ensure that **`y_test`** and **`y_pred`** are defined in your workspace after running your **`BernoulliNB`** model. If you encounter any issues with these plots or if you require further customization, feel free to ask for additional guidance.\n","srcMarkdownNoYaml":"\n\n## **Naive Bayes classifier for multivariate Bernoulli models**\n\n```{python}\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.preprocessing import OneHotEncoder\nimport os\n\n# Load the dataset\ndf = pd.read_csv('C:/Users/NUSAI/Desktop/Machine learning/HebaNu.github.io/HebaNu.github.io/HebaNu/posts/Probability theory and random variables/updated_POST1.csv')\n\n# Drop rows where any cell is NaN in the 'Work Method' column\ndf = df.dropna(subset=['Work Method'])\n\n# One-hot encoding\nencoder = OneHotEncoder(sparse=False)\nX = encoder.fit_transform(df[[' industry sector']])  \n# Use the correct column name\n\n# Target variable\ny = df['Work Method']\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train the Bernoulli Naive Bayes model\nmodel = BernoulliNB()\nmodel.fit(X_train, y_train)\n\n# Make predictions and evaluate the model\ny_pred = model.predict(X_test)\n\n\n\n# Output the classification report and accuracy\nprint(classification_report(y_test, y_pred, zero_division=0))\nprint(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n\n```\n\nThe results from your BernoulliNB model show that:\n\nThe model is predicting class 2.0 with high precision (76%) and recall (100%), which means for this class, it performs well both in terms of the accuracy of the positive predictions it makes (precision) and its ability to find all the positive instances (recall).\n\nFor class 1.0, however, the model does not predict any instances correctly, which suggests that either there's an issue with the distribution of your classes (perhaps class 1.0 is underrepresented), or that the features do not provide enough information to distinguish class 1.0 from class 2.0. The overall accuracy of the model is 76.47%, which means that it correctly predicts the class for 76.47% of the test set.\n\nThe macro avg and weighted avg for precision, recall, and f1-score provide a summary of the effectiveness of the model across the classes. The low macro avg for precision and f1-score indicates that one of the classes does not perform well, which we already know is class 1.0.\n\nThe f1-score is a harmonic mean of precision and recall and is a useful metric when you have classes that are imbalanced. In your case, the f1-score for class 1.0 is 0.00, indicating poor performance for this class.\n\n***To create plots*** that visualize the performance of your **`BernoulliNB`** model, you would typically look at the confusion matrix, precision-recall, and possibly ROC curves. Below are examples of how you could generate each of these plots using **`matplotlib`** and **`scikit-learn`**:\n\n1.  **Confusion Matrix**: Visualizes the correct and incorrect predictions compared to the actual values.\n\n```{python}\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming y_test and y_pred are already defined from your BernoulliNB model\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.title('Confusion Matrix for BernoulliNB Model')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n\n```\n\n**ROC Curve**: Plots the true positive rate against the false positive rate.\n\n```{python}\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import RocCurveDisplay\nimport matplotlib.pyplot as plt\n\n# Adjust y_test to have binary labels 0 and 1\ny_test_binary = y_test.replace({1: 0, 2: 1})\n\n# Get predicted probabilities for the positive class (e.g., class 2)\ny_pred_prob = model.predict_proba(X_test)[:, 1]  # Index 1 for the probability of class 2\n\n# Calculate the ROC curve\nfpr, tpr, thresholds = roc_curve(y_test_binary, y_pred_prob)\n\n# Calculate the AUC (Area Under Curve)\nroc_auc = auc(fpr, tpr)\n\n# Plot the ROC curve\ndisp = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\ndisp.plot()\nplt.title('ROC Curve for BernoulliNB Model')\nplt.show()\n\n```\n\n**Classification Report**: Presents precision, recall, f1-score for each class.\n\n```{python}\nfrom sklearn.metrics import classification_report\nimport pandas as pd\n\nreport = classification_report(y_test, y_pred, output_dict=True)\ndf_report = pd.DataFrame(report).transpose()\n\ndf_report.drop(['accuracy'], inplace=True)  # Drop accuracy as it's not a class-specific metric\ndf_report.plot(kind='bar', figsize=(10, 7))\nplt.title('Classification Report for BernoulliNB Model')\nplt.ylabel('Score')\nplt.show()\n```\n\nPlease note that in order to generate a ROC curve or Precision-Recall curve, you need the probability scores or decision function, not just the predicted labels. If **`y_pred`** contains only class labels, you would need to use the **`predict_proba`** or **`decision_function`** method of your classifier to obtain these scores.\n\nThese code snippets are intended to be run in your local Python environment, and you'll need to ensure that **`y_test`** and **`y_pred`** are defined in your workspace after running your **`BernoulliNB`** model. If you encounter any issues with these plots or if you require further customization, feel free to ask for additional guidance.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","editor":"visual","grid":{"sidebar-width":"250px","body-width":"600px","margin-width":"450px","gutter-width":"1.5em"},"theme":"Darkly","title-block-banner":true,"title":"Probability theory and random variables","author":"Heba Nusair","date":"2023-11-06","categories":["news","code","analysis"],"image":"Capture.JPG","code-viewfold":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}