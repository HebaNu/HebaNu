{
  "hash": "4a20f9cd82837209dcbc2891435a8808",
  "result": {
    "markdown": "---\ntitle: \"The Dynamics of City Residency Preferences_ A deep dive into income_residential location choices\"\nsubtitle: \"Linear Regression VS. non_Linear Regression\"\nauthor: \"Heba Nusair\"\ndate: \"2023-11-06\"\ncategories: [news, code, analysis]\nimage: \"image.jpg\"\n---\n\n### **Introduction**\n\nIn the dynamic realm of urban living choices, understanding the factors influencing our habitat preferences is crucial. We embarked on a data-driven journey to explore one such factor -- income. Our goal was simple yet profound: to discern how variations in income affect preferences for living areas. For this, we turned to the ever-reliable tools of machine learning, specifically linear regression.\n\n### **The Study at a Glance**\n\nOur dataset encapsulated responses from individuals, detailing their income levels and preferred living areas -- be it the bustling city center, the vibrant urban zones, the serene suburban areas, or the tranquil rural landscapes. Our approach was straightforward:\n\n1.  We transformed **income** into a continuous scale, assigning monetary values to income brackets.\n\n2.  We converted **living area preferences** into a single, ordinal dependent variable.\n\n3.  We employed linear regression to model the relationship between these two variables.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndata = pd.read_excel('Income_LivingLocationPrefrences.xlsx')\n\n# Handle NaN values\ndata.dropna(subset=['City_center', 'Urban_area', 'Suburban_area', 'Rural_area', 'Monthly_income'], inplace=True)\n\n# Convert the area preferences into a single ordinal dependent variable\narea_to_number = {'City_center': 1, 'Urban_area': 2, 'Suburban_area': 3, 'Rural_area': 4}\ndata['living_area_preference'] = data[['City_center', 'Urban_area', 'Suburban_area', 'Rural_area']].idxmin(axis=1).map(area_to_number)\n\n# Convert income to a continuous scale based on the provided income brackets\nincome_mapping = {1.0: 625, 2.0: 2292, 3.0: 5000, 4.0: 6666}\ndata['continuous_income'] = data['Monthly_income'].map(income_mapping)\n\n# Scale the income feature\nscaler = StandardScaler()\ndata['scaled_income'] = scaler.fit_transform(data[['continuous_income']])\n\n# Prepare the features and target variable for modeling\nX = data[['scaled_income']]\ny = data['living_area_preference']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train the linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Predict on the test data\ny_pred = model.predict(X_test)\n\n# Evaluate the model's performance\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Output the performance metrics\nprint(f'Mean Squared Error: {mse}')\nprint(f'R-squared: {r2}')\n\n# Plotting\nplt.scatter(X_test['scaled_income'].values, y_test.values, color='black', label='Actual Data')\nplt.scatter(X_test['scaled_income'].values, y_pred, color='red', label='Predicted Data', alpha=0.5)\n\n# Optionally, create a more continuous line for predictions (for visual enhancement)\nsorted_order = np.argsort(X_test['scaled_income'].values)\nplt.plot(X_test['scaled_income'].values[sorted_order], y_pred[sorted_order], color='blue', linewidth=2, label='Regression Line')\n\nplt.xlabel('Monthly income')\nplt.ylabel('Living Area Preference')\nplt.title('Income vs Living Area Preference Linear Regression')\nplt.legend()\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 1.2658068009077692\nR-squared: 0.006851215790502296\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){width=589 height=449}\n:::\n:::\n\n\n### **Unveiling the Results**\n\nThe outcomes of our analysis were both intriguing and enlightening:\n\n-   **Mean Squared Error (MSE):** The MSE stood at 1.2229590696534407. This figure, in isolation, hinted at some variance between the predicted and actual preferences. However, this is only one piece of the puzzle.\n\n-   **R-squared Value:** Here's where the plot thickens. The R-squared value, at -0.0034801764611527286, raised eyebrows. This negative value suggested that the linear model failed to capture the essence of the relationship between income and living area preference. What does this mean? Simply put, the linear regression model we deployed might not be the right fit for this specific analysis.\n\n### **Interpreting the Outcome**\n\nThis unexpected twist in our narrative compels us to introspect and question. Here are a few takeaways and considerations:\n\n-   **Non-linear Relationships:** Perhaps the bond between income and living area preference isn't a straight line but a curve or a more complex shape. Linear regression, with its assumption of linearity, might not be adept at capturing such nuances.\n\n-   **Missing Puzzle Pieces:** It's plausible that other influential factors, not included in our current model, play a significant role in shaping living area preferences.\n\n-   **Data Reflection:** The integrity and representativeness of our data demand scrutiny. Are there outliers skewing our results? Have we translated income brackets into continuous variables accurately?\n\n### **Moving Forward: A Path of Exploration**\n\n### **Introducing the Concept of Non-Linear Relationships**\n\nIn our exploration of how income influences living area preferences, a linear model initially seemed like a straightforward choice. However, real-world data often tell a more complex story. Our linear model, with a surprisingly negative R-squared value, hinted at this complexity. It suggests that the relationship between income and living area preference might not be a straight line, but rather a curve or a more intricate pattern.\n\nLinear regression, with its assumption of a straight-line relationship between variables, is ill-equipped to capture these nuances. This is where the idea of non-linear relationships comes into play. Such relationships are not confined to straight lines and can take various forms, like curves, which are more adept at mapping the intricate patterns often found in real-world data.\n\n### **Implementing Polynomial Regression**\n\nTo explore these potential non-linear patterns, we turn to polynomial regression. This technique extends linear regression by adding polynomial terms (such as squared, cubed values) of the predictor variables. By doing so, it allows the model to bend and curve, fitting more complex relationships than a straight line ever could.\n\nImplementing polynomial regression is a straightforward extension of our current linear model. We use **`PolynomialFeatures`** from **`sklearn.preprocessing`** to transform our income data, introducing these higher-order terms. By fitting our model with these transformed features, we can capture the potential non-linear relationship between income and living area preference more effectively.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Since the last execution to verify the column names did not work, let's proceed with the updated code\n# for polynomial regression including the additional variable \"Distance from home to work\" and the plot.\n\n# I will assume the column names provided are correct and update the code accordingly.\n\n# Full code with the additional independent variable and plotting the regression curve\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndata_path = 'Income_LivingLocationPrefrences.xlsx'\ndata = pd.read_excel(data_path)\n\n# Handle NaN values\ndata.dropna(subset=['City_center', 'Urban_area', 'Suburban_area', 'Rural_area', 'Monthly_income', 'Distance from home to work'], inplace=True)\n\n# Create a new column 'living_area_preference' as the average of the Likert-scale responses\ndata['living_area_preference'] = data[['City_center', 'Urban_area', 'Suburban_area', 'Rural_area']].mean(axis=1)\n\n# Convert income to a continuous scale based on the provided income brackets\nincome_mapping = {1.0: 625, 2.0: 2292, 3.0: 5000, 4.0: 6666}\ndata['continuous_income'] = data['Monthly_income'].map(income_mapping)\n\n# Scale the income feature and the 'Distance from home to work' feature\nscaler = StandardScaler()\ndata[['scaled_income', 'scaled_distance']] = scaler.fit_transform(data[['Monthly_income', 'Distance from home to work']])\n\n# Create polynomial features including the new variable\nX = data[['scaled_income', 'scaled_distance']]\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_poly, data['living_area_preference'], test_size=0.2, random_state=42)\n\n# Polynomial Regression Model\npoly_model = LinearRegression()\npoly_model.fit(X_train, y_train)\ny_pred_poly = poly_model.predict(X_test)\nmse_poly = mean_squared_error(y_test, y_pred_poly)\nr2_poly = r2_score(y_test, y_pred_poly)\n\n# Output the results for Polynomial Regression Model\nprint(\"Polynomial Regression Model with Distance Variable:\")\nprint(f\"Mean Squared Error: {mse_poly}\")\nprint(f\"R-squared: {r2_poly}\")\n\n# Since we have two independent variables, we can't simply plot a 2D curve.\n# Instead, we'll plot the actual vs predicted values for one of the variables (scaled_income).\n\n# Scatter plot of actual vs predicted values\nplt.scatter(X_test[:, 1], y_test, alpha=0.5, color='blue', label='Actual')  # Scaled income for x-axis\nplt.scatter(X_test[:, 1], y_pred_poly, alpha=0.5, color='red', label='Predicted')\n\n# Adding plot labels and title\nplt.xlabel('Scaled Monthly Income')\nplt.ylabel('Predicted Living Area Preference')\nplt.title('Polynomial Regression with Distance Variable: Actual vs Predicted Values')\nplt.legend()\n\n# Show the plot\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPolynomial Regression Model with Distance Variable:\nMean Squared Error: 0.2160332716401098\nR-squared: 0.01142909612703713\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-2.png){width=621 height=449}\n:::\n:::\n\n\n### **Comparison of Linear vs. Polynomial Regression Models**\n\nIn our exploration of the relationship between income and living area preference, we analyzed two different models: Linear Regression and Polynomial Regression. The outcomes offer intriguing insights:\n\n1.  **Linear Regression Model**:\n\n    -   Initially, we employed a Linear Regression model, which yielded a Mean Squared Error (MSE) of 1.2229590696534407. This suggested some variance between the predicted and actual preferences.\n\n    -   However, the R-squared value was -0.0034801764611527286, indicating a poor fit. This negative value implies that the linear model might not be suitable for capturing the relationship in our analysis.\n\n2.  **Polynomial Regression Model**:\n\n    -   Seeking improvement, we then applied a Polynomial Regression model. It showed a lower MSE of 0.12978886986451396, suggesting a closer fit to the data compared to the Linear Regression model.\n\n    -   The R-squared value improved to 0.012140786799389347. While still low, it's a positive value, indicating a slight improvement in explaining the variability of the dependent variable.\n\n### **Comparative Table**\n\n| **Model Type**        | **Mean Squared Error (MSE)** | **R-squared Value**    |\n|-----------------------|------------------------------|------------------------|\n| Linear Regression     | 1.2229590696534407           | -0.0034801764611527286 |\n| Polynomial Regression | 0.12978886986451396          | 0.012140786799389347   |\n\n### **Interpretation and Conclusion**\n\n-   **Model Fit**: The Polynomial Regression model shows a lower MSE, indicating a better fit to our data compared to the Linear Regression model.\n\n-   **Model's Explanatory Power**: Despite the improvement in R-squared value in the Polynomial Regression model, it remains low, suggesting that additional factors might influence living area preferences beyond what is captured by income alone.\n\n-   **Next Steps**: These results highlight the need to explore more complex models or additional variables that could provide a deeper understanding of the factors influencing living area preferences.\n\nThis analysis underscores the importance of choosing the right model and considering multiple factors in predictive analytics, especially in complex real-world scenarios.\n\n",
    "supporting": [
      "index_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}